# (PART) Data Wrangling {-}

# Introduction {-}

A frequent error in Data Science projects is thinking that they start with analysis. In fact, when a data analyst is asked where they spend most of their time, the answer remains the same: [80% in Data Wrangling](https://www.infoworld.com/article/3228245/the-80-20-data-science-dilemma.html)^[https://www.infoworld.com/article/3228245/the-80-20-data-science-dilemma.html].

Data in its natural form (Raw Data), usually contain registration errors that make exact analysis impossible. Being recorded by different systems and people, it is normal to end up with a file in which the same value is expressed in different ways (for example, a date can be recorded as June 28, or as 28/06), there may be blank records, and of course, grammatical errors.

When analyzing this data, all these records have to be pre-processed. That is, the data must be cleaned, unified, consolidated, and normalized so that it can be used to extract valuable information. Data Wrangling is the process of preparing data to be leveraged.

In the following chapters, we will see several common steps of the Data Wrangling process such as Importing data into R from files, converting data to tidy type, string processing, html processing, date and time formatting, and text mining.

In this section, we will master the essential skills for getting data into R and reshaping it for analysis. We will start by learning how to **import data** from diverse sources, including CSV files, Excel spreadsheets, and web pages. Once loaded, we will explore how to **transform** data between "wide" and "tidy" (long) formats using `pivot_longer()` and `pivot_wider()`, ensuring our data is structured correctly for visualization and modeling. We will also cover how to **combine multiple datasets** using the powerful family of join functions (`left_join()`, `inner_join()`, etc.). Finally, we will delve into specialized processing techniques, including **web scraping**, **string manipulation** with regular expressions, **date conversion** with `lubridate`, and the fundamentals of **text mining** to extract insights from unstructured text.

# Data import and consolidation

## Importing from files
For importing files, whether they are text types or spreadsheets, we need to know where we are going to import the data from.

### Working Directory
By default, when we import files, R will search in the working directory. To find out the path of our working directory we will use the `getwd()` function.

```{r eval=FALSE}
getwd()
#> [1] "c:/Documents/dparedesi/git-repository/DS"
```

This is the path where we can place our files to load them. If we want to load data from another folder we can change the working directory using `setwd()`.

```{r eval=FALSE}
setwd("c:/Documents/Projects/R Files")

getwd()
#> [1] "c:/Documents/Projects/R Files"
```

> **Note**: While `setwd()` works, it is generally discouraged in reproducible workflows because it creates absolute paths that break when code is shared or moved. Consider using **RStudio Projects** combined with the `here` package, which provides `here::here()` to construct relative paths that work across different systems.

For practical purposes, we are going to use a file already available in one of the previously installed packages, `dslabs`, when we analyzed the danger level to decide which US state to travel to. To do this, we can use the `system.file()` function and determine the path where the `dslabs` package was installed.

```{r eval=FALSE}
dslabs_path <- system.file(package="dslabs")
```

Likewise, we can list the files and folders within that path using the `list.files()` function.

```{r eval=TRUE}
dslabs_path <- system.file(package="dslabs")
list.files(dslabs_path)
```

The folder we will use is **extdata**. We can access the path of this folder if we modify the parameters of the `system.file()` function.

```{r eval=TRUE}
dslabs_path <- system.file("extdata", package="dslabs")
list.files(dslabs_path)
```

The file we will use is **murders.csv**. To build the complete path of this file we can concatenate the strings or we can also directly use the `file.path(path, file_name)` function.

```{r eval=TRUE}
csv_example_path <- file.path(dslabs_path, "murders.csv")
```

Finally, we will copy the file to our working directory with the `file.copy(source_path, destination_path)` function.

```{r eval=TRUE}
file.copy(csv_example_path, getwd())
```

We can validate the copy with the `file.exists(file_name)` function.

```{r eval=TRUE}
file.exists("murders.csv")
```

It is recommended to check the documentation of the file manipulation functions.

```{r eval=FALSE}
?files
```

### readr and readxl packages
Now that we have the file in our working directory, we will use functions within the **readr** and **readxl** packages to import files into R. Both are included in the **tidyverse** package that we previously installed.

```{r eval=TRUE}
library(tidyverse) # Here readr is included automatically
library(readxl)
```

The functions we will use the most will be `read_csv()` and `read_excel()`. The latter supports .xls and .xlsx extensions.

```{r eval=TRUE}
data_df <- read_csv("murders.csv", show_col_types = FALSE)

# Once imported we can remove the file if we wish
file.remove("murders.csv")
```

We see how by default it detects the headers in the first row and assigns them a default data type. Let's now explore our `data_df` object.

```{r eval=TRUE}
data_df
```

The first thing it indicates is that the object is of type **tibble**. This object is very similar to a data frame, but with improved features such as, for example, the number of rows and columns in the console, the data type under the header, the default report of only the first 10 records automatically, among many others that we will discover in this chapter.

The same syntax and logic would apply for importing an excel file. In this case we are importing directly from the package path and not from our working directory.

```{r eval=FALSE}
excel_example_path <- file.path(dslabs_path, "2010_bigfive_regents.xls")
data_df_from_excel <- read_excel(excel_example_path)
```

**readr** gives us 7 different types of functions for importing flat files:

The `readr` package provides a versatile suite of functions for importing flat files, each tailored to a specific delimiter or format. The most common is **`read_csv()`**, designed for comma-separated values. For tab-separated files, we use **`read_tsv()`**, while **`read_delim()`** offers a general-purpose solution for files with custom delimiters (like semicolons or pipes). Other specialized functions include **`read_fwf()`** for fixed-width files, **`read_table()`** for whitespace-separated columns, and **`read_log()`** for parsing standard web server logs.

> **Tip**: For files that use semicolons as delimiters (common in European locales), use `read_csv2()` or `read_delim()` with `delim = ";"`. Also, if you encounter import issues, use `problems(data_df)` after importing to diagnose parsing errors.

> [!TIP]
> **Clean Column Names**: Interpreted data often has column names with spaces or capital letters (e.g., "Customer ID"). We highly recommend piping your data into `janitor::clean_names()` immediately after reading it to standardize everything to `snake_case` (e.g., "customer_id").

### Importing files from the internet
We have seen how we can enter the full path to load a file directly from another source different from our working directory. In the same way, if we have a file in an internet path we can pass it directly to R since `read_csv()` and the other **readr** import functions support URL input as a parameter.

Here we see the import of grades from students of the Data Science with R course.

```{r eval=FALSE, message=FALSE, warning=FALSE}
##### Example 1:
# Historical grades data
url <- "https://dparedesi.github.io/Data-Science-with-R-book/data/student-grades.csv"
grades <- read_csv(url, show_col_types = FALSE)

```

```{r eval=FALSE}
grades <- grades |>
  mutate(total = (P1 + P2 + P3 + P4 + P5 + P6)/30*20)

grades |> 
  select(P1, P2, P3, P4, P5, total) |> 
  summary()
```

From this we could visualize a histogram:

```{r eval=FALSE}
hist(grades$total)
```

Or we could compare between genders which one has the highest median:

```{r eval=FALSE}
grades |>
  ggplot() +
  aes(gender, total) +
  geom_boxplot()
  
```

We could also extract updated Covid-19 information.

```{r eval=FALSE}
##### Example 2:
# Covid-19 Data
url <- "https://covid.ourworldindata.org/data/owid-covid-data.csv"
internet_data <- read_csv(url, show_col_types = FALSE)

internet_data |> 
  arrange(desc(date)) |> 
  head(10)
```

## Tidy data
Ordered data or _tidy data_ are those obtained from a process called _data tidying_. It is one of the important cleaning processes during processing of large data or 'big data' and is a very used step in Data Science. The main characteristics are that each different observation of that variable has to be in a different row and that each variable you measure has to be in a column [@leek2015].

As we may have noticed, we have been using _tidy data_ since the first chapters. However, not all our data comes ordered. Most of it comes in what we call wide data or _wide data_.

For example, we have previously used data from Gapminder. Let's filter the data from Germany and South Korea to remember how we had our data.

```{r eval=TRUE}
gapminder |> 
  filter(country %in% c("South Korea", "Germany")) |> 
  head(10)
```

We notice that each row is an observation and each column represents a variable. It is ordered data, _tidy data_. On the other hand, we can see what the data was like for these two countries if we access the source in the `dslabs` package.

```{r eval=TRUE}
fertility_path <- file.path(dslabs_path, "fertility-two-countries-example.csv")

wide_data <- read_csv(fertility_path, show_col_types = FALSE)

wide_data
```

We see that the original data had two rows, one per country and then each column represented a year. This is what we call wide data or _wide data_. Normally we will have _wide data_ that we first have to convert to _tidy data_ to later be able to perform our analyses.

### Transforming to tidy data
The `tidyverse` library provides two functions to reshape data between _wide_ and _long_ (tidy) formats. We use `pivot_longer()` to convert from _wide data_ to _tidy data_ and `pivot_wider()` to convert from _tidy data_ to _wide data_.

#### pivot_longer function
Let's see the utility with the `wide_data` object that we created in the previous section as a result of importing data from the csv. First apply the `pivot_longer()` function to explore the conversion that is performed by default.

```{r eval=TRUE}
tidy_data <- wide_data |>
  pivot_longer(cols = -country, names_to = "key", values_to = "value")

tidy_data
```

We see how the `pivot_longer()` function has collected the columns into two, the names column ("key") and the values column ("value"). We can change the title of these new columns, for example "year" and "fertility".

```{r eval=TRUE}
tidy_data <- wide_data |>
  pivot_longer(cols = -country, names_to = "year", values_to = "fertility")

tidy_data
```

We use `cols = -country` to exclude the country column from being pivoted. By default the column names are collected as text. To convert them to numbers we use the `names_transform` argument.

```{r eval=TRUE}
tidy_data <- wide_data |>
  pivot_longer(cols = -country, names_to = "year", values_to = "fertility",
               names_transform = list(year = as.integer))

tidy_data
```

This data would now be ready to create graphs using `ggplot()`.

```{r eval=TRUE}
tidy_data |>
  ggplot() +
  aes(year, fertility, color = country) +
  geom_point()
```

#### pivot_wider function
Sometimes, as we will see in the following section, it will be useful to go back from rows to columns. For this we will use the `pivot_wider()` function, where we specify `names_from` (the column containing the new column names) and `values_from` (the column containing the values). Additionally, we can use the `:` operator to indicate from which column to which column we want to select.

```{r eval=TRUE}
tidy_data |> 
  pivot_wider(names_from = year, values_from = fertility) |> 
  select(country, `1965`:`1970`)
```

### separate function
In the cases described above we had a situation with relatively ordered data. We only had to do a collection transformation and converted to _tidy data_. However, the data is not always stored in such an easily interpretable way. Sometimes we have data like this:

```{r eval=TRUE}
path <- file.path(dslabs_path, "life-expectancy-and-fertility-two-countries-example.csv")

data <- read_csv(path, show_col_types = FALSE)

data |> 
  select(1:5) #Report first 5 columns
```

If we apply `pivot_longer()` directly we would not have our data ordered yet. Let's see:

```{r eval=TRUE}
data |> 
  pivot_longer(cols = -country, names_to = "key_col", values_to = "value_col")
```

We will use the `separate()` function to separate a column into multiple columns using a specific separator. In this case our separator would be the character `_`. Also, we will add the attribute `extra="merge"` to indicate that if there is more than one separator character, do not separate them and keep them joined.

```{r eval=TRUE}
data |> 
  pivot_longer(cols = -country, names_to = "key_col", values_to = "value_col") |> 
  separate(key_col, c("year", "other_var"), sep="_", extra = "merge")
```

We already have the year separated, but this data is still not _tidy data_ since there is a row for fertility and a row for life expectancy for each country. We have to pass these values from row to columns. And for that we already learned that we can use the `pivot_wider()` function

```{r eval=TRUE}
data |> 
  pivot_longer(cols = -country, names_to = "key_col", values_to = "value_col") |> 
  separate(key_col, c("year", "other_var"), sep="_", extra = "merge") |> 
  pivot_wider(names_from = other_var, values_from = value_col)
```

In other cases, instead of separating a column we will want to join them. In future cases we will see how the `unite(column_1, column2)` function can also be useful.

`r if(params$hidden){"<!--"}`

## Exercises

`r ne`. Access the **Uber Peru 2010** dataset from [this link](https://dparedesi.github.io/Data-Science-with-R-book/data/uber-peru-2010.csv) and attempt to import it into an object named `uber_peru_2010`. Pay attention to the delimiters used in the file.

```{r echo=FALSE}
inc(params$hidden, ne)
```

<details>
  <summary type="button">Solution</summary>
```{r eval=FALSE}
url <- "https://dparedesi.github.io/Data-Science-with-R-book/data/uber-peru-2010.csv"

# We will use read_csv since it is separated by commas
uber_peru_2010 <- read_csv(url, show_col_types = FALSE)

# Upon importing it we realize it is separated by ";"
uber_peru_2010 |> 
  head()

# Therefore we import again using read_delim
uber_peru_2010 <- read_delim("external/uber-peru-2010.csv", 
                             delim = ";", 
                             col_types = cols(.default = "c")
                             )

uber_peru_2010 |> 
  head()

```
</details>


`r ne`. Import the **SINADEF** deaths registry from [this source](https://www.datosabiertos.gob.pe/sites/default/files/sinadef-deaths.csv) into an object called `deaths`. Ensure you handle the file encoding correctly to avoid character issues.

```{r echo=FALSE}
inc(params$hidden, ne)
```

<details>
  <summary type="button">Solution</summary>
```{r eval=FALSE}
url <- "https://www.datosabiertos.gob.pe/sites/default/files/sinadef-deaths.csv"
url <- "https://dparedesi.github.io/Data-Science-with-R-book/data/sinadef-deaths.csv"

# We will use read_delim because it is delimited by ";" and not by ","
# Also we change the encoding to avoid error in loading
deaths <- read_delim(url, ";",
                          local = locale(encoding = "latin1"))
```
</details>


`r ne`. Download the resource file from [this link](https://dparedesi.github.io/Data-Science-with-R-book/data/resources-other-idd.xlsx) to a temporary location. Validating that the file exists, load the specific sheet named "Deflators" into an object named `data`.

```{r echo=FALSE}
inc(params$hidden, ne)
```

<details>
  <summary type="button">Solution</summary>
```{r eval=FALSE}
# Store the url
url <- "https://dparedesi.github.io/Data-Science-with-R-book/data/resources-other-idd.xlsx"

# Create a temporary name & path for our file. See: ?tempfile
temp_file <- tempfile()

# Download the file to our temp
download.file(url, temp_file)

# Import the excel
dat <- read_excel(temp_file, sheet = "Deflators")

# Remove the temporary file
file.remove(temp_file)
```
</details>

For the following files run the following code so that you have access to the objects referred to in the problems:

```{r eval=FALSE}
# GDP by countries
url <- "https://dparedesi.github.io/Data-Science-with-R-book/data/gdp.csv"
gdp <- read_csv(url, show_col_types = FALSE)

# Diseases by years by countries
url <- "https://dparedesi.github.io/Data-Science-with-R-book/data/diseases-evolution.csv"
diseases_wide <- read_csv(url, show_col_types = FALSE)

# Number of female mayors
url <- "https://dparedesi.github.io/Data-Science-with-R-book/data/female-mayors.csv"
female_mayors <- read_csv(url, show_col_types = FALSE)

# Evolution of a university
url <- "https://dparedesi.github.io/Data-Science-with-R-book/data/university.csv"
university <- read_csv(url, show_col_types = FALSE)

```

`r ne`. Examine the structure of the `gdp` dataset. Transform it into a *tidy* format suitable for analysis, and then create a line plot visualizing the evolution of GDP over time for each country.

```{r echo=FALSE}
inc(params$hidden, ne)
```

<details>
  <summary type="button">Solution</summary>
```{r eval=FALSE}
# To tidy data
gdp <- gdp |> 
  pivot_longer(cols = -country, names_to = "year", values_to = "gdp",
               names_transform = list(year = as.integer))

gdp

# Visualization
gdp |> 
  ggplot() +
  aes(year, gdp, color=country) +
  geom_line()

```
</details>

`r ne`. The `diseases_wide` object contains disease counts in a wide format. Reshape this dataframe into a *tidy* structure where the specific diseases are consolidated into a single column.

```{r echo=FALSE}
inc(params$hidden, ne)
```

<details>
  <summary type="button">Solution</summary>
```{r eval=FALSE}
# Solution
diseases_1 <- diseases_wide |> 
  pivot_longer(cols = c(-country, -year, -population), names_to = "disease", values_to = "count")

diseases_1

# Alternative solution. Instead of indicating what to omit, we indicate what to take into account
diseases_2 <- diseases_wide |> 
  pivot_longer(cols = HepatitisA:Rubella, names_to = "disease", values_to = "count")

diseases_2

```
</details>

`r ne`. Convert the `female_mayors` dataset, which is currently in a long format, into a *wide* format where the variables are spread across columns.

```{r echo=FALSE}
inc(params$hidden, ne)
```

<details>
  <summary type="button">Solution</summary>
```{r eval=FALSE}
female_mayors <- female_mayors |> 
  pivot_wider(names_from = variable, values_from = total)

```
</details>

`r ne`. The `university` dataset is untidy. Reshape it by first pivoting longer to gather variables, separating the combined variable names, and then pivoting wider to achieve a final tidy structure.

```{r echo=FALSE}
inc(params$hidden, ne)
```

<details>
  <summary type="button">Solution</summary>
```{r eval=FALSE}
university <- university |> 
  pivot_longer(cols = -semester, names_to = "variable", values_to = "value") |> 
  separate(variable, c("name", "variable2"), sep="_") |> 
  pivot_wider(names_from = variable2, values_from = value)

university
```
</details>

`r if(params$hidden){"-->"}`

## Joining tables
Regularly we will have data from different sources that we then have to combine to be able to perform our analyses. For this we will learn different groups of functions that will allow us to combine multiple objects.

### Join functions
Join functions are the most used in table crossing. To use them we have to make sure we have the `dplyr` library installed.

```{r eval=FALSE}
library(dplyr)
```

This library includes a variety of functions to combine tables.

The `dplyr` package offers a family of join functions to combine tables based on common keys. The most frequently used is **`left_join()`**, which preserves all rows from the first (left) table and appends matching data from the second. Conversely, **`right_join()`** keeps all rows from the second table. **`inner_join()`** is more restrictive, retaining only the rows that have matching keys in *both* tables, effectively filtering for the intersection. **`full_join()`** does the opposite, keeping *all* rows from both tables and filling missing values with `NA`. Finally, filtering joins like **`semi_join()`** (keeps rows in the first table that match the second) and **`anti_join()`** (keeps rows in the first table that do *not* match the second) are excellent for data validation and filtering without adding new columns.

To see the join functions with examples we will use the following files:

```{r eval=TRUE, message=FALSE}
url_1 <- "https://dparedesi.github.io/Data-Science-with-R-book/data/join-card.csv"
url_2 <- "https://dparedesi.github.io/Data-Science-with-R-book/data/join-customer.csv"

card_data_1 <- read_csv(url_1, col_types = cols(id = col_character()))
customer_data_2 <- read_csv(url_2, col_types = cols(id = col_character()))

card_data_1

customer_data_2

```

#### Left join
Given two tables with the same identifier (in our case our identifier consists only of a single column: ID), the left join function maintains the information of the first table and completes it with the data that crosses in the second table

```{r eval=TRUE}
left_join(card_data_1, customer_data_2, by = c("id"))
```

As we can see, the first three columns are exactly the same as we initially had and to the right of those columns we see the columns of the other table for the values ​​that did cross the data. In this case we are facing a data inconsistency since all customers of `card_data_1` should be in `customer_data_2`. This inconsistency could lead us to have to map the data loss process, etc.

#### Right join
Given two tables with the same identifier, the right join function maintains the information of the second table and completes it with the data that crosses in the first table

```{r eval=TRUE}
right_join(card_data_1, customer_data_2, by = "id")
```

The idea is the same as in `left_join`, only this time the `NA` are in the first two columns.

#### Inner join
In this case we will only have the intersection of the tables. Only the result of the data that are in both tables will be shown.

```{r eval=TRUE}
inner_join(card_data_1, customer_data_2, by = "id")
```

#### Full join
Full join is a total crossing of both. It shows us all the data that are in both the first and the second table.

```{r eval=TRUE}
full_join(card_data_1, customer_data_2, by = "id")
```

> **Tip**: To join on multiple columns, use a vector: `by = c("col1", "col2")`. To join on columns with different names, use named vectors: `by = c("left_col" = "right_col")`.

#### Semi join
The case of the semi join is very similar to `left_join` with the difference that it only shows us the columns of the first table and eliminates the data that did not manage to cross (what in `left_join` comes out as NA). Also, none of the columns of table 2 appear. This is like doing a filter requesting the following: show me only the data from table 1 that is also in table 2.

```{r eval=TRUE}
semi_join(card_data_1, customer_data_2, by = "id")
```

#### Anti join
In the case of `anti_join` we have the opposite of `semi_join` since it shows the data from table 1 that are **not** in table 2.

```{r eval=TRUE}
anti_join(card_data_1, customer_data_2, by = "id")
```

### Joining without a common identifier
Likewise, we will have some moments when we need to combine only two objects, without using any type of intersection. For this we will use the `bind` functions. These functions allow us to put together two vectors or tables either in rows or columns.

#### Union of vectors
If we have two or more vectors of the same size we can create the union of the columns to create a table using the `bind_cols()` function. Let's see with an example:

```{r eval=TRUE}
vector_1 <- c("hello", "Have you seen", "the")
vector_2 <- c("Julian", "Carla", "Wednesday")

result <- bind_cols(greeting = vector_1, nouns = vector_2)

result
```

#### Union of tables
In the case of tables the use is the same. Likewise, we can also join the rows of two or more tables. To see its application let's first create some example tables:

```{r eval=TRUE}
table_1 <- data.frame(
  name = c("Jhasury", "Thomas", "Andres", "Josep"),
  surname = c("Campos", "Gonzales", "Santiago", "Villaverde"),
  address = c("Jr. los campos 471", "Av. Casuarinas 142", NA, "Av. Tupac Amaru 164"),
  phone = c("976567325", "956732587", "961445664", "987786453")
)

table_2 <- data.frame(
  age = c(21, 24, 19, 12),
  sign = c("Aries", "Capricorn", "Sagittarius", "Libra")
)

# Create a table from row 2 to 3 of table_1
table_3 <- table_1[2:3, ]
```

Once we have our tables let's proceed to join them. We see that they do not have a common identifier.

```{r eval=TRUE}
result <- bind_cols(table_1, table_2)
result
```

or joining by rows like this:

```{r eval=TRUE}
result <- bind_rows(table_1, table_3)
result
```

## Web Scraping
Web Scraping is the process of extracting data from a website. We will use it when we need to extract data directly from tables that are presented on websites. For this we will use the `rvest` library, included in the `tidyverse` library.

```{r eval=FALSE}
library(tidyverse)
library(rvest)
```

> **Important**: When scraping websites, always respect the site's `robots.txt` file and terms of service. Avoid making excessive requests that could overload servers. For commercial use, consider whether the data is licensed or requires permission.

The function we will use the most will be `read_html()` and as an argument we will place the url of the web from where we want to extract the data. We are not talking about a url that downloads a text file but a web page like this:

```{r, echo=FALSE, fig.alt="Wikipedia table of Hispanic countries by population"}
knitr::include_graphics(file.path(img_path, "05-wrangling", "hispanic-countries.png"))
```

Thus, we will use `read_html()` to store all the web html and then little by little access the table data in R.

```{r eval=TRUE}
html_data <- read_html("https://es.wikipedia.org/wiki/Anexo:Pa%C3%ADses_hispanos_por_poblaci%C3%B3n")

html_data
```

Now that we have the data stored in the object we have to go looking for the data, doing scraping. For this we will use the `html_nodes("table")` function to access the "table" node.

```{r eval=TRUE}
web_tables <- html_data |> 
  html_elements("table")

```

Finally, we have to go index by index looking for the table that interests us. To give it table format we will use `html_table`. In this case we will use double brackets because it is a list of lists and the `setNames()` function to change the name of the columns.

```{r eval=TRUE}
# We format as table and store in raw_table
raw_table <- web_tables[[1]] |> 
  html_table()

# Change header names
raw_table <- raw_table |> 
  setNames(
  c("N", "country", "population", "pop_prop", "avg_change", "link")
  )

# Convert to tibble
raw_table <- raw_table |> 
  as_tibble()

# Report first rows
raw_table |> 
  head(5)
```

We already have our data imported and we could already start exploring its content in detail.

`r if(params$hidden){"<!--"}`

## Exercises
For the following exercises we will use objects from the **Lahman** library, which contains US baseball player data. Run the following Script before starting to solve the exercises.

```{r eval=FALSE}
install.packages("Lahman")
library(Lahman)

# Top 10 players of the year 2016
top_players <- Batting |> 
  filter(yearID == 2016) |>
  arrange(desc(HR)) |>    # sorted by number of "Home run"
  slice(1:10)    # Take from row 1 to 10

top_players <- top_players |> as_tibble()

# List of all baseball players from recent years
master <- Master |> as_tibble()

# Awards won by players
awards <- AwardsPlayers |>
  filter(yearID == 2016) |> 
  as_tibble()
```

`r ne`. Using the `top_players` and `Master` datasets, join them to retrieve the `playerID`, first name, last name, and home runs (`HR`) for the top 10 players of 2016.

```{r echo=FALSE}
inc(params$hidden, ne)
```

<details>
  <summary type="button">Solution</summary>
```{r eval=FALSE}
top_10 <- left_join(top_players, master, by = "playerID") |> 
  select(playerID, nameFirst, nameLast, HR)

top_10
```
</details>

`r ne`. Identify the **intersection** of top players and award winners. List the ID and names of the top 10 players from 2016 who also won at least one award that year.

```{r echo=FALSE}
inc(params$hidden, ne)
```

<details>
  <summary type="button">Solution</summary>
```{r eval=FALSE}
semi_join(top_10, awards, by = "playerID")
```
</details>

`r ne`. Find the players who won awards in 2016 but did **not** make it into the top 10 list. Report their IDs and names.

```{r echo=FALSE}
inc(params$hidden, ne)
```

<details>
  <summary type="button">Solution</summary>
```{r eval=FALSE}
# First we calculate all prizes of those who are not top 10:
non_top_award_ids <- anti_join(awards, top_10, by = "playerID") |> 
  select(playerID)

# As a player could have obtained several prizes we obtain unique values
non_top_award_ids <- unique(non_top_award_ids)

# Then we cross with the master to obtain the names
other_names <- left_join(non_top_award_ids, master, by = "playerID") |> 
  select(playerID, nameFirst, nameLast)

other_names

```
</details>

`r ne`. Scrape the MLB payroll data from `http://www.stevetheump.com/Payrolls.htm`. Store the entire page html, extract the tables, and specifically isolate the fourth table (`node 4`), formatting it as a data frame.

```{r echo=FALSE}
inc(params$hidden, ne)
```

<details>
  <summary type="button">Solution</summary>
```{r eval=FALSE}
url <- "http://www.stevetheump.com/Payrolls.htm"
html <- read_html(url)

nodes <- html |> 
  html_elements("table")

nodes[[4]] |> 
  html_table()
```
</details>

`r ne`. Using the scraped tables, prepare the 2019 payroll (node 4) and 2018 payroll (node 5) data. Standardize the column names to `team`, `payroll_2019`, and `payroll_2018` respectively. Finally, perform a **full join** to combine these datasets by team name.

```{r echo=FALSE}
inc(params$hidden, ne)
```

<details>
  <summary type="button">Solution</summary>
```{r eval=FALSE}
payroll_2019 <- nodes[[4]] |> 
  html_table()

payroll_2018 <- nodes[[5]] |> 
  html_table()

####### Payroll 2019: ################
#We eliminate row 15 which is the league average:
payroll_2019 <- payroll_2019[-15, ]

#We filter the requested columns:
payroll_2019 <- payroll_2019 |> 
  select(X2, X4) |> 
  rename(team = X2, payroll_2019 = X4)

# We eliminate row 1 since it is the source header
payroll_2019 <- payroll_2019[-1,]

####### Payroll 2018: ################
# We select the two columns that interest us and 
#change name to headers
payroll_2018 <- payroll_2018 |> 
  select(Team, Payroll) |> 
  rename(team = Team, payroll_2018 = Payroll)

####### Full join: ################
full_join(payroll_2018, payroll_2019, by = "team")
```
</details>

`r if(params$hidden){"-->"}`