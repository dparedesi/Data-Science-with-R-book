# String processing and text mining

## Basic functions
We have already learned how to import data and consolidate it. However, we cannot yet work with this data. We have to validate through string processing and ensure a minimum quality to be able to perform our analyses.

For example, in the previous chapter we imported data from Wikipedia, however we did not focus on whether we could already perform operations or visualizations with our data.

```{r eval=FALSE}
library(rvest)
url <- "https://es.wikipedia.org/wiki/Anexo:Pa%C3%ADses_hispanos_por_poblaci%C3%B3n"
#url <- "https://es.wikipedia.org/wiki/Distribuci%C3%B3n_geogr%C3%A1fica_del_idioma_espa%C3%B1ol" #as a back up URL
html_data <- read_html(url)

web_tables <- html_data |>
  html_nodes("body") |>
  html_nodes("table")

raw_table <- web_tables[[2]] |>
  html_table()

raw_table <- raw_table |> 
  setNames(c("N", "country", "population", "prop_population", "avg_change", "link")) 

raw_table <- raw_table |>
  as_tibble()

raw_table |> head(5)

```

We may not have noticed, but we can observe columns with spaces or commas where there should be numbers. We can validate this not only by analyzing the class of the column, but also if we try to calculate the average of that variable.

```{r eval=FALSE}
class(raw_table$population)

mean(raw_table$population)
```

We cannot do a direct conversion to number either because white spaces and commas are characters.

```{r eval=FALSE}
as.numeric(raw_table$population)
```

There are so frequent and so many possible use cases that there are already multiple functions for processing strings included in the tidyverse library. Likewise, there is more than one way to process strings. It will always depend on how the raw data is found.

### Replacing characters
One of the basic functions that we will use the most will be replacing characters. We apply this function when we are sure that this change will not compromise the rest of the data. We have spaces and we have commas. So we could start by replacing one of the two to normalize them using the `str_replace_all(string, pattern, replacement)` function. In the pattern attribute we will use `\\s`, which comes from `space`. We are going to learn first to modify the data stored in a vector and then we will replicate it to our entire table.

```{r eval=FALSE}
library(tidyverse)
library(stringr)

population_vector <- raw_table$population

population_vector <- str_replace_all(population_vector, "\\s", ",")

population_vector
```

We have purposely taken all the values to be separated by commas because now we can easily use the `parse_number(vector)` function which not only replaces the commas with empty strings, but also removes any non-numeric value before the first number, which facilitates us if we had monetary values, and also converts the value from character type to numeric type.

```{r eval=FALSE}
population_vector <- parse_number(population_vector)

# Additional example in case we had a monetary value:
parse_number("$345,153")
```

This vector now allows us to perform mathematical operations or visualization of the distribution.
```{r eval=FALSE}
# Convert to millions
population_vector <- population_vector/10^6

# We remove the last value which is the world population:
length_val <- length(population_vector)
population_vector <- population_vector[-length_val]

# Visualization
boxplot(population_vector)
```

We already know which functions to use to transform the fields of our case. However, we have applied them to vectors. To mutate the columns of our table in raw form we will use the function `mutate(across(columns, function))` using the pipeline operator `|>`. Let's apply the first change of spaces by commas and not only to column 3, population, but also to column 5, average change.

```{r eval=FALSE}
raw_table |> 
  mutate(across(c(3,5), ~str_replace_all(., "\\s", ",")))

```

We have removed from the `str_replace_all` function the `string` attribute and replaced it with a dot `.`. And that dot `.` indicates that it will evaluate for each column `c(3,5)` of our table.

Now, let's apply the parse_number function that we applied previously.

```{r eval=FALSE}
raw_table |> 
  mutate(across(c(3,5), ~str_replace_all(., "\\s", ","))) |> 
  mutate(across(c(3,5), ~parse_number(.)))
```

## Regular expressions
A [regular expression](https://stringr.tidyverse.org/articles/regular-expressions.html)^[https://stringr.tidyverse.org/articles/regular-expressions.html] (or regex as it is known in English) is a pattern that describes a set of strings. We have already used regex in the previous section using only the pattern `\\s`. However, usually we will have many more use cases that will require a pattern that can convert a wider range of cases.

Although we could analyze all possible use cases available in the documentation, we learn faster by use cases. Let's analyze a case that will allow us to learn some patterns little by little.

In the `dslabs` library we found and used previously the height data, `heights`, of students from a university expressed in inches.

```{r eval=TRUE}
library(dslabs)
library(tidyverse)
data(heights)

heights |> 
  head(10)
```

These data were ready to be analyzed. However, that was not how it came from the source. The students had to fill out a survey and even when they were asked for their height in inches, they completed their height in inches, feet, centimeters, writing numbers, letters, etc. We can see the initial data from the form in the `reported_heights` data frame.

```{r eval=TRUE}
reported_heights |> 
  head(10)
```

Although we might think that they entered the data correctly, we do not have to trust and it is always better to validate the quality of our data. There are multiple ways to validate, as we can see below:

```{r eval=TRUE}
heights <- reported_heights$height

# Validation option 1: Random sample
sample(heights, 100)

# Validation option 2: convert to numbers and count if there are NAs
x <- as.numeric(heights)
sum(is.na(x))

# Validation option 3: add column of those that cannot be converted to number:
reported_heights |> 
  mutate(numeric_height = as.numeric(height)) |> 
  filter(is.na(numeric_height)) |> 
  head(10)
```

We might want to choose to eliminate these NA data as they are not significant with respect to the total of 1,095 data points. However, there are several of these data points that follow a determined pattern and instead of being discarded could be converted to the scale we have in the rest of the data. For example, there are people who entered their height as 5'7", which, for those who remember the conversion, can be converted because 1 foot is 12 inches. So $5*12+7=67$. And so, like that case, we can detect patterns, but we have, again, to be careful in detecting the exact pattern and not a very generic one that can change other use cases. If everyone followed the same pattern $x'y''$ or $x'y$ it would be much easier to convert it to inches by calculating $x*12+y$.

Let's start by extracting our column to a single character vector with all the values that do not convert automatically to number or were entered in inches. We detect this if they measure more than 5 and up to 7 feet (from 1.5m to 2.1 meters). After that we will create the transformations little by little.

```{r eval=TRUE, message=FALSE, warning=FALSE}
problematic_heights <- reported_heights |> 
  filter(is.na(as.numeric(height)) | # Does not convert to number
         (!is.na(as.numeric(height)) & as.numeric(height) >= 5 &
            as.numeric(height) <= 7 ) # or entered in feet and not inches
        ) |> 
  pull(height)

length(problematic_heights)
```

Adding the condition of having entered in feet we have 168 errors. We cannot ignore 15.3% of errors.

We will use `str_view()` to visualize matches. This function is extremely helpful when debugging regular expressions as it highlights exactly what is matching your pattern.

```{r eval=TRUE}
# Let's visualize entries containing "feet"
str_view(problematic_heights, "feet", match=TRUE)
```

We can also use `str_detect(string, pattern)` to get a logical value (`TRUE`/`FALSE`) to filter our vector.

```{r eval=TRUE}
index <- str_detect(problematic_heights, "feet")

problematic_heights[index] # Match the pattern
problematic_heights[!index] |> # Do not match the pattern
  head(40) 
```

### Alternation
`|` is the alternation operator that will choose between one or more possible values. In our case, we have indicated to detect if there is the word "feet", but we also have "ft" and "foot" to refer to the same thing in our data. Thus, we can create the pattern "feet" or "ft" or "foot".

```{r eval=TRUE}
# Visualize the matches
str_view(problematic_heights, "feet|ft|foot", match=TRUE)
```

```{r eval=TRUE}
index <- str_detect(problematic_heights, "feet|ft|foot")
problematic_heights[index] # Match
```

In the same way we can find the variations for inches and other symbols that we can remove:

```{r eval=TRUE}
index <- str_detect(problematic_heights, "inches|in|''|\"|cm|and")
problematic_heights[index] # Match
```

In this case we have entered `''` to detect those who entered that symbol to denote inches and `\"` in case they used double quotes. In this latter case we have used `\` so that it does not generate an error when interpreting as closing the string.

We could already start replacing based on the detected patterns:

```{r eval=TRUE}
problematic_heights <- str_replace_all(problematic_heights, "feet|ft|foot", "'")
problematic_heights <- str_replace_all(problematic_heights, "inches|in|''|\"|cm|and", "")

problematic_heights |> 
  head(30)
```

As an additional effort, we could also look to solve that some people have written words instead of numbers. For this we create a function that replaces each word with a number and apply it to the vector:

```{r eval=TRUE}
words_to_number <- function(s){
  str_to_lower(s) |>  
    str_replace_all("zero", "0") |>
    str_replace_all("one", "1") |>
    str_replace_all("two", "2") |>
    str_replace_all("three", "3") |>
    str_replace_all("four", "4") |>
    str_replace_all("five", "5") |>
    str_replace_all("six", "6") |>
    str_replace_all("seven", "7") |>
    str_replace_all("eight", "8") |>
    str_replace_all("nine", "9") |>
    str_replace_all("ten", "10") |>
    str_replace_all("eleven", "11")
}

problematic_heights <- words_to_number(problematic_heights)
problematic_heights |> 
  head(30)

```

### Anchoring
Now that it is more standardized we can start with regex with more generic characteristics. For example, there is a person who has entered `6'`. It would be convenient to have everything in the form feet plus inches. With which we should have `6'0`. To achieve this we have to create a regex according to this generic situation. We will use the symbol `^` to anchor our validation to "start with" and the symbol `$` to match with the end of the string. Before replacing, let's first see who matches.

```{r eval=TRUE}
str_view(problematic_heights, "^6'$", match=TRUE)
```

This regex indicates that it starts with `6'` and that the expression ends there. We could still make it more generic to address those who, in the future, write 5 inches (1.52m) or 6 inches (1.82m). For this we will use brackets and inside them we will put all the values that we will accept.

```{r eval=TRUE}
index <- str_detect(problematic_heights, "^[56]'$")
problematic_heights[index] # Match
```

There is still only one result, but our regex is more generic now and we can already use it to replace. Before replacing in our vector we are going to do a test to learn how to create what we need from a pattern.

```{r eval=TRUE}
test_vec <- c("5'", "6'")

str_replace_all(test_vec, "^([56])'$", "\\1'0")

```

We have placed between parentheses to indicate that what is inside is our first value and we use `\\1` to refer to that first value. So we are indicating to write the first value, then a quote `'`, and then a zero `0`.

Now we are ready to apply to our entire vector. We are going to make the change to consider not only 5 and 6, but up to the value of 7 inches (2.1m). Likewise, we are going to take the cases in which there is only a number without the foot symbol `'`.

```{r eval=TRUE}
problematic_heights <- str_replace_all(problematic_heights, "^([5-7])'$", "\\1'0")
problematic_heights <- str_replace_all(problematic_heights, "^([5-7])$", "\\1'0")

problematic_heights |> 
  head(30)
```

### Repetitions
We can control how many times a pattern matches using repetition operators:

We can control how many times a pattern matches using repetition operators. The question mark **`?`** indicates that the preceding element matches **0 or 1** time (making it optional). The plus sign **`+`** requires **1 or more** matches, ensuring the element is present at least once. The asterisk **`*`** allows for **0 or more** matches, meaning the element can be absent or repeated indefinitely.

For example, to find all cases where instead of using the foot symbol `'` they entered a comma, a period, or a space we will use the following pattern:

```{r eval=TRUE}
pattern <- "^([4-7])\\s*[,\\.]\\s*(\\d*)$"
```

Let's read the pattern:

1. The string starts with a digit ranging from 4 to 7.
2. `\\s` means that it is followed by a white space, but we use `*` to indicate that this character appears 0 or more times.
3. After that space we will look for any of the following characters: `,`, a period `\\.` (to which we put double backslash because the period alone in a pattern means "any value").
4. We use `\\s*` again to look for zero or more white spaces.
5. Finally we indicate that the string ends there with a digit, to denote that look for any digit we use `\\d`, d for digit. And we add asterisk so that it keeps one or more digits that it finds.

In summary: it starts with a number, then symbols and then a digit. Between the symbols there could be white spaces. That is our pattern.

```{r eval=TRUE}
str_view(problematic_heights, pattern, match=TRUE)
```

We already found the values that match the pattern, so we are ready to replace.

```{r eval=TRUE}
problematic_heights <- str_replace_all(
                        problematic_heights, 
                        "^([4-7])\\s*[,\\.]\\s*(\\d*)$", "\\1.\\2'0"
                   )

problematic_heights |> 
  head(30)
```

Another pattern we see now is when before or after the foot symbol `'` there is a white space. Let's make the change with what we learned and include cases where there are decimals:

```{r eval=TRUE}
index <- str_detect(problematic_heights, 
                     "^([4-7]\\.?\\d*)\\s*'\\s*(\\d+\\.?\\d*)\\s*$")

problematic_heights[index] |> # Match
  head(30)

problematic_heights <- str_replace_all(
                      problematic_heights, 
                      "^([4-7]\\.?\\d*)\\s*'\\s*(\\d+\\.?\\d*)\\s*$",
                      "\\1'\\2"
                   )

problematic_heights |> 
  head(30)

```

Likewise, we have the pattern in which they entered: feet + space + inches without any symbol. Let's make the change with what we learned.

```{r eval=TRUE}
index <- str_detect(problematic_heights, "^([4-7])\\s+(\\d*)\\s*$")

problematic_heights[index] # Match

problematic_heights <- str_replace_all(
                      problematic_heights, 
                      "^([4-7])\\s+(\\d*)\\s*$", "\\1'\\2"
                   )

problematic_heights |> 
  head(30)

```

We are ready to put all the patterns together and the power of patterns is that they can serve us for future exercises. Thus, we will create a function where we will place each change that we can verify to a string.

```{r eval=TRUE}
format_errors <- function(string){
  string |> 
    str_replace_all("feet|ft|foot", "'") |> # Change feet for '
    str_replace_all("inches|in|''|\"|cm|and", "") |> # Remove symbols
    str_replace_all("^([5-7])'$", "\\1'0") |> # Adds 0 to 5', 6' or 7'
    str_replace_all("^([5-7])$", "\\1'0") |> # Adds 0 to 5, 6 or 7
    str_replace_all("^([4-7])\\s*[,\\.]\\s*(\\d*)$", "\\1.\\2'0") |> # Change 5.3' to 5.3'0
    str_replace_all("^([4-7]\\.?\\d*)\\s*'\\s*(\\d+\\.?\\d*)\\s*$", "\\1'\\2") |> #Removes spaces in middle
    str_replace_all("^([4-7])\\s+(\\d*)\\s*$", "\\1'\\2") |> # Adds '
    str_replace("^([12])\\s*,\\s*(\\d*)$", "\\1.\\2") |> # Changes decimals from commas to dots
    str_trim() #Removes spaces at start and end
}

```

Thus, we have created two functions that could be useful to us if we were to work with surveys of the same type again.

Before applying it to our entire table let's extract the values to a vector again to apply the created functions.

```{r eval=TRUE, message=FALSE, warning=FALSE}
problematic_heights <- reported_heights |> 
  filter(is.na(as.numeric(height)) | # Does not convert to number
         (!is.na(as.numeric(height)) & as.numeric(height) >= 5 &
            as.numeric(height) <= 7 ) # or entered in feet and not inches
        ) |> 
  pull(height)
```

Now let's apply the created functions:

```{r eval=TRUE}
formatted_heights <- problematic_heights |> 
  words_to_number() |> 
  format_errors()

pattern <- "^([4-7]\\.?\\d*)\\s*'\\s*(\\d+\\.?\\d*)\\s*$"
index <- str_detect(formatted_heights, pattern)
formatted_heights[!index] # Do not match the pattern
```

We have managed to reduce from 168 errors of 1095 records, 15.3% of errors, to 12 errors of 1095, 1% of errors. We can now apply to our initial table.

```{r eval=TRUE}
# Apply created formulas
heights <- reported_heights |> 
  mutate(height) |> 
  mutate(height = words_to_number(height) |> format_errors())

# Get random samples to validate quality
random_indices <- sample(1:nrow(heights)) 
heights[random_indices, ] |> 
  head(15)

```

We still have to do some conversions. However, since they follow a determined pattern we can use the `extract(source_column, new_columns, pattern, remove_source)` function to confirm creating new columns for each value of our pattern.

```{r eval=TRUE}
pattern <- "^([4-7]\\.?\\d*)\\s*'\\s*(\\d+\\.?\\d*)\\s*$"

heights |> 
  extract(height, c("feet", "inches"), regex = pattern, remove = FALSE) |> 
  head(15)
```

Now that we have the data that matches the pattern in two other columns, and we know they are numbers, we can convert everything to number.

```{r eval=TRUE}
heights |> 
  extract(height, c("feet", "inches"), regex = pattern, remove = FALSE) |> 
  mutate(across(c("height", "feet", "inches"), ~as.numeric(.))) |> 
  head(15)
```

Now that our columns are numeric we can perform operations to calculate height.

```{r eval=TRUE}
heights |> 
  extract(height, c("feet", "inches"), regex = pattern, remove = FALSE) |> 
  mutate(across(c("height", "feet", "inches"), ~as.numeric(.))) |> 
  mutate(fixed_heights = feet*12 + inches) |> 
  head(15)
```

Finally, we will do a validation of whether the height is in an interval and/or if it was expressed in centimeters or meters.

```{r eval=TRUE}
# We assume for a person a minimum 50" (1.2m) and max 84" (2.1m)
min <- 50
max <- 84

heights <- heights |> 
  extract(height, c("feet", "inches"), regex = pattern, remove = FALSE) |> 
  mutate(across(c("height", "feet", "inches"), ~as.numeric(.))) |> 
  mutate(fixed_heights = feet*12 + inches) |> 
  mutate(final_height = case_when(
    !is.na(height) & between(height, min, max) ~ height, #inches 
    !is.na(height) & between(height/2.54, min, max) ~ height/2.54, #cm
    !is.na(height) & between(height*100/2.54, min, max) ~ height*100/2.54, #meters
    !is.na(fixed_heights) & inches < 12 & 
      between(fixed_heights, min, max) ~ fixed_heights, #feet'inches
    TRUE ~ as.numeric(NA)))

# Random Sample:
random_indices <- sample(1:nrow(heights)) 
heights[random_indices, ] |> 
  select(-time_stamp) |> # Shows all columns except time_stamp
  head(10)

```

We already have our sample validated, we would only have to take the columns we need and start using the object for the analyses we need.

```{r eval=TRUE}
final_heights <- heights |> 
  select(gender = sex, heights = final_height)

final_heights |> 
  head(10)

```

## From strings to dates
Regularly when we import data, we are not only going to want to transform numeric data. We will also have multiple cases where we need to transform our string to a date in some particular format. For this, we will use the `lubridate` library, included in `tidyverse`, which provides us with diverse functions to make date treatment more accessible.

```{r eval=FALSE}
library(lubridate)
```

When the text string is in the ISO 8601 date format (YYYY-MM-DD), we can directly use the `month()`, `day()`, `year()` function.

```{r eval=TRUE}
dates_char <- c("2010-05-19", "2020-05-06", "2010-02-03")

str(dates_char)

month(dates_char)

```

However, we do not always have the date in that format and `lubridate()` gives other functions that are more flexible when coercing data. Look at this example:

```{r eval=TRUE}
dates <- c(20090101, "2009-01-02", "2009 01 03", "2009-1-4",
       "2009-1, 5", "Created on 2009 1 6", "200901 !!! 07")

str(dates)

ymd(dates)
```

The first data entered was a number, but we already know that it coerces it to text. Then, we have different values entered, but all follow the same pattern. First is the year, then the month and then the day. When we know that first is the year, then month and then day we will use the `ymd()` function to convert all dates to ISO 8601 format.

In the same way, we will have the following functions that we can use depending on the form in which we have the date from our source. In all cases it will be convenient for us to convert to ISO 8601 format. For example here we can see when it correctly recognizes the format and when the formatting fails.

```{r eval=TRUE, warning=FALSE}
x <- "28/03/89"
ymd(x)
mdy(x)
ydm(x)
myd(x)
dmy(x)
dym(x)
```

Finally, in the same way that we can use these functions of days, months and years, we can also use to refer to hours, minutes and seconds.

```{r eval=TRUE, warning=FALSE}
# Format with hours, minutes and seconds
date_val <- "Feb/2/2012 12:34:56"
mdy_hms(date_val)

# Additional data: Showing system date:
now()

```

`r if(params$hidden){"<!--"}`

## Exercises

Before solving the following exercise run this Script:

```{r eval=FALSE}
sales <- tibble(
  month = c("April", "May", "June"),
  revenue = c("s/32,124", "s/35,465", "S/38,332"),
  profit = c("s/8,120", "s/9,432", "s/10,543")
)
```

`r ne`. Convert the `revenue` and `profit` columns in the `sales` object to numeric values, removing any currency symbols or formatting characters.

```{r echo=FALSE}
inc(params$hidden, ne)
```

<details>
  <summary type="button">Solution</summary>
```{r eval=FALSE}
# Solution 1
sales |> 
  mutate(across(c(2,3), ~parse_number(.)))

# Alternative solution, longer
sales |>
  mutate(across(c(2,3), ~str_replace_all(., "\\S/|,", ""))) |> 
  mutate(across(c(2,3), ~as.numeric(.)))
```
</details>


`r ne`. Clean the `universities` vector so that all university names are standardized. Specifically, replace abbreviations like "Univ." or "U." at the beginning of the string with the full word "University ".


```{r echo=FALSE}
inc(params$hidden, ne)
```

<details>
  <summary type="button">Solution</summary>
```{r eval=FALSE}
universities |> 
    str_replace("^Univ\\.?\\s|^U\\.?\\s", "University ")
```
</details>

For the following exercises, we are going to work on the survey data conducted prior to Brexit in the UK. Run the Script first:

```{r eval=FALSE}
library(rvest)
library(tidyverse)
url <- "https://en.wikipedia.org/w/index.php?title=Opinion_polling_for_the_United_Kingdom_European_Union_membership_referendum&oldid=896735054"
table_html <- read_html(url) |> html_nodes("table")
polls <- table_html[[5]] |> html_table(fill = TRUE)
```

`r ne`. Update the `polls` object by renaming columns to `c("date", "remain", "leave", "undecided", "spread", "sample", "pollster", "type", "notes")`. Then, filter the dataset to retain only rows where the `remain` column contains a percentage symbol ("%").

```{r echo=FALSE}
inc(params$hidden, ne)
```

<details>
  <summary type="button">Solution</summary>
```{r eval=FALSE}
names(polls) <- c("date", "remain", "leave", "undecided", "spread",
                  "sample", "pollster", "type", "notes")
polls <- polls[str_detect(polls$remain, "%"), ]
polls 

# If we want to validate the number of polls:
nrow(polls)
```
</details>


`r ne`. Extract the `remain` column into a vector and convert the text percentages into proper numeric probabilities (e.g., convert "50%" to 0.5).

```{r echo=FALSE}
inc(params$hidden, ne)
```

<details>
  <summary type="button">Solution</summary>
```{r eval=FALSE}
remain <- polls$remain

# Solution 1:
percentages <- parse_number(remain)/100

# Solution 1:
temp <- str_replace(remain, "%", "")
percentages <- as.numeric(temp)/100

# Solution 2:
temp <- str_remove(remain, "%")
percentages <- as.numeric(temp)/100

```
</details>


`r ne`. In the `undecided` column, the value "N/A" appears when the sum of `remain` and `leave` equals 100%. Create a vector for `undecided` where these "N/A" values are replaced with "0%".

```{r echo=FALSE}
inc(params$hidden, ne)
```

<details>
  <summary type="button">Solution</summary>
```{r eval=FALSE}
undecided <- polls$undecided

str_replace(undecided, "N/A", "0%")

```
</details>


`r ne`. encapsulate your cleaning logic into a single function named `format_percentage(string)`. Test this function with the vector `c("13.5%", "N/A", "10%")` to verify it handles both percentages and "N/A" values correctly.

```{r echo=FALSE}
inc(params$hidden, ne)
```

<details>
  <summary type="button">Solution</summary>
```{r eval=FALSE}

format_percentage <- function(string){
  string |> 
    str_replace("N/A", "0%") |> 
    parse_number()/100
}

# Function test:
test_vec <- c("13.5%", "N/A", "10%")

format_percentage(test_vec)

```
</details>


`r ne`. Apply `format_percentage` to the `remain`, `leave`, `undecided`, and `spread` columns in the `polls` dataset. Also, ensure the `sample` column is converted to a numeric type.

```{r echo=FALSE}
inc(params$hidden, ne)
```

<details>
  <summary type="button">Solution</summary>
```{r eval=FALSE}
polls <- polls |> 
  mutate(across(c("remain", "leave", "undecided", "spread"), ~format_percentage(.))) |> 
  mutate(across(c("sample"), ~parse_number(.)))
```
</details>


`r ne`. Import the Peruvian COVID-19 dataset from [this URL](https://www.datosabiertos.gob.pe/sites/default/files/DATOSABIERTOS_SISCOVID.csv) into an object named `covid_peru`. Convert the birth date column (`FECHA_NACIMIENTO`) to a proper Date format and calculate the age distribution of the infected individuals using a histogram.

```{r echo=FALSE}
inc(params$hidden, ne)
```

<details>
  <summary type="button">Solution</summary>
```{r eval=FALSE}
url <- "https://www.datosabiertos.gob.pe/sites/default/files/DATOSABIERTOS_SISCOVID.csv"
covid_peru <- read_csv(url)

# We look for those that do not follow the ISO 8601 standard:
index <- str_detect(covid_peru$FECHA_NACIMIENTO, "\\d{4}-\\d{2}-\\d{2}")
covid_peru$FECHA_NACIMIENTO[!index]

# We see dates in DD/MM/YYYY format
# We replace to ISO 8601 format:
covid_peru <- covid_peru |> 
  mutate(across("FECHA_NACIMIENTO", 
            ~str_replace(., "(\\d{2})/(\\d{2})/(\\d{4})", "\\3-\\2-\\1")
            ))

# We search again for those that do not follow ISO 8601 standard:
index <- str_detect(covid_peru$FECHA_NACIMIENTO, "\\d{4}-\\d{2}-\\d{2}")
covid_peru$FECHA_NACIMIENTO[!index]

# Convert column to date:
covid_peru <- covid_peru |> 
  mutate(across("FECHA_NACIMIENTO", ~ymd(.)))

# Now that it is date format we create histogram:
covid_peru |> 
  mutate(age = year(now()) - year(FECHA_NACIMIENTO)) |> 
  pull(age) |> 
  hist()
```
</details>

`r if(params$hidden){"-->"}`

## Text Mining using Tidy Data

Text mining is the discovery by computer of new information, previously unknown, by automatically extracting information from different written resources. Written resources can be websites, books, chats, comments, emails, reviews, articles, etc.

To perform text mining efficiently in R, we will use the `tidytext` package. The "tidy" text format is defined as a table with one token per row. A token can be a word, a sentence, or a paragraph, but usually, it is single words. This structure allows us to use all the standard tools we've learned (`dplyr`, `ggplot2`) to analyze text.

```{r eval=FALSE}
# Install packages if you haven't yet
# install.packages("tidytext")

library(tidytext)
library(tidyverse)
library(stringr)
library(syuzhet) # For sentiment analysis
library(wordcloud) 
```

### Importing data and Tokenization
Word maps or word clouds allow us to quickly identify which are the words that are repeated most in a text.

We are going to analyze the work "Pride and Prejudice" written by the author Jane Austen. We will obtain the text from the [Project Gutenberg](www.gutenberg.org)^[www.gutenberg.org] website. We will use the `get_text_as_string()` function from `syuzhet` to import properly.

```{r eval=TRUE}
url <- "https://www.gutenberg.org/cache/epub/1342/pg1342.txt"

# Import text as a single string
pride_book <- get_text_as_string(url)

# Convert to a data frame with sentences or just lines
# Here we will split by newline to create a rudimentary structure
text_df <- tibble(
  text = str_split(pride_book, "\n")[[1]]
)

# Remove empty lines
text_df <- text_df |> 
  filter(text != "")

head(text_df)
```

### Text cleaning and Tokenization

Now we will clean the text and convert it to specific tokens (words). The `unnest_tokens()` function automatically:
1. Splits text into tokens (words by default).
2. Removes punctuation.
3. Converts to lowercase.

> **Note on AI:** This process of breaking text into "tokens" is exactly how Large Language Models like GPT-4 work. In **Chapter 14 (Data Science in the Age of AI)**, we will see that LLMs are essentially probabilistic engines that predict the next token in a sequence. Understanding how to handle tokens here is the foundation for understanding Generative AI.

```{r eval=TRUE}
# We eliminate first rows of notes/prologue if needed, though unnest_tokens handles a lot.
# Let's clean some metadata lines roughly
start_line <- 115
text_df <- text_df[start_line:nrow(text_df), ]

# Tokenize
tidy_pride <- text_df |>
  unnest_tokens(word, text)

# See the result
head(tidy_pride)
```

Now we have a table where each row is a word. This is the "tidy" format.

However, we clearly have words that do not add meaning (stop words), such as "the", "and", "of". We can remove them using a list of stop words. The `tm` package provides a good list for English.

```{r eval=TRUE}
library(tm)
english_stop_words <- tibble(word = stopwords("english"))

# Remove stop words using anti_join
tidy_pride_clean <- tidy_pride |>
  anti_join(english_stop_words, by = "word")

head(tidy_pride_clean)
```

We might also want to remove custom words or numbers that appeared in the extraction.

```{r eval=TRUE}
custom_stop_words <- tibble(word = c("mr", "mrs", "miss", "said", "will", 
                                     "one", "much", "may", "can", "now", "sir", "lady"))

tidy_pride_clean <- tidy_pride_clean |>
  anti_join(custom_stop_words, by = "word") |>
  filter(!str_detect(word, "^\\d+$")) # Remove pure numbers

```

### Word Cloud
Now that we have our clean data, calculating word frequency is as simple as using `count()`.

```{r eval=TRUE}
word_counts <- tidy_pride_clean |>
  count(word, sort = TRUE)

head(word_counts)
```

We can create the word cloud directly from this data frame.

```{r eval=TRUE, warning=FALSE}
wordcloud(words = word_counts$word, 
          freq = word_counts$n,
          min.freq = 5,
          max.words = 80, 
          random.order = FALSE, 
          colors = brewer.pal(name = "Dark2", n = 8))
```

### Word Frequency Plot
Since we have the data in a tidy format, plotting a bar chart of the most frequent words is straightforward with `ggplot2`.

```{r eval=TRUE}
word_counts |>
  head(20) |>
  ggplot(aes(n, reorder(word, n))) +
  geom_col(fill = "blue") +
  labs(y = NULL, x = "Frequency", title = "Most common words in Pride and Prejudice")
```

## Sentiment Analysis
Sentiment analysis allows us to know the tone of the messages. We will use the `syuzhet` package combined with our tidy data skills.

Let's use the same example of tweets.

```{r eval=TRUE, warning=FALSE}
library(readxl)

# Download tweets
url <- "https://dparedesi.github.io/Data-Science-with-R-book/data/rmapalacios-tweets.xlsx"
temp_file <- tempfile()
download.file(url, temp_file)
posts <- read_excel(temp_file)
file.remove(temp_file)

# Filter for tweets only and create a tidy dataframe
tweets_df <- posts |> 
  filter(`Tweet Type` == "Tweet") |> 
  select(text = Text) |>
  mutate(tweet_id = row_number()) 

head(tweets_df)
```

Now we clean the tweets. `unnest_tokens` handles most of it, but for tweets, we might want to remove URL links first.

```{r eval=TRUE}
# Custom cleaning function for tweets before tokenization
clean_tweets <- tweets_df |>
  mutate(text = str_replace_all(text, "http\\S+", "")) |> # remove URLs
  mutate(text = str_replace_all(text, "@\\S+", "")) # remove mentions

# Get Sentiment scores for each tweet
# syuzhet works well with the full text vector for scoring
tweet_sentiments <- get_nrc_sentiment(clean_tweets$text, language = "spanish")

# Combine with original data
tweets_with_sentiment <- bind_cols(clean_tweets, tweet_sentiments)

head(tweets_with_sentiment)
```

We can now reshape this data to visualize emotions using `pivot_longer`, just like we do with any tidy dataset.

```{r eval=TRUE}
translate_emotions <- function(string){
  case_when(
    string == "anger" ~ "Anger",
    string == "anticipation" ~ "Anticipation",
    string == "disgust" ~ "Disgust",
    string == "fear" ~ "Fear",
    string == "joy" ~ "Joy",
    string == "sadness" ~ "Sadness",
    string == "surprise" ~ "Surprise",
    string == "trust" ~ "Trust",
    string == "negative" ~ "Negative",
    string == "positive" ~ "Positive",
    TRUE ~ string
  )
}

# Summarize totals
sentiment_totals <- tweets_with_sentiment |> 
  summarise(across(anger:positive, sum)) |>
  pivot_longer(cols = everything(), names_to = "sentiment", values_to = "total") |>
  mutate(sentiment = translate_emotions(sentiment))

sentiment_totals
```

Visualizing:

```{r eval=TRUE, warning = FALSE}
# Separate positive/negative from specific emotions
general_sentiments <- c("Positive", "Negative")

sentiment_totals |>
  filter(!sentiment %in% general_sentiments) |>
  ggplot(aes(reorder(sentiment, total), total, fill = sentiment)) +
  geom_col() +
  coord_flip() +
  labs(x = NULL, y = "Total Score", title = "Emotions in Tweets") +
  theme(legend.position = "none")

sentiment_totals |>
  filter(sentiment %in% general_sentiments) |>
  ggplot(aes(sentiment, total, fill = sentiment)) +
  geom_col() +
  labs(x = NULL, y = "Total Score", title = "Positive vs Negative Sentiment")
```

This tidy approach makes it much easier to inspect the data at every step and integrate valid data science workflows (filtering, joining, plotting) without learning a separate system just for text.

`r if(params$hidden){"<!--"}`

## Exercises

For these exercises we will use more books from Project Gutenberg using the `gutenbergr` library.

```{r eval=FALSE}
# install.packages("gutenbergr")
library(gutenbergr)

# Tibble: list of books in Gutenberg.org
gutenberg_metadata

# List of books in Spanish
gutenberg_works(languages = "es")
```

`r ne`. Use `gutenberg_download(2000)` to download the text of "El ingenioso hidalgo don Quijote de la Mancha" and store the result in an object named `download`.

```{r echo=FALSE}
inc(params$hidden, ne)
```

<details>
  <summary type="button">Solution</summary>
```{r eval=FALSE}
download <- gutenberg_download(2000)
quijote_text <- download$text
head(quijote_text)
```
</details>


`r ne`. Extract a random sample of 1,000 lines from the text. Clean this sample by tokenizing into words and removing standard Spanish stop words.

```{r echo=FALSE}
inc(params$hidden, ne)
```

<details>
  <summary type="button">Solution</summary>
```{r eval=FALSE}
set.seed(123)
sample_lines <- tibble(text = sample(quijote_text, 1000))

tidy_quijote <- sample_lines |>
  unnest_tokens(word, text) |>
  anti_join(spanish_stop_words, by = "word") |>
  # Remove extra stop words if needed
  filter(!word %in% c("don", "quijote", "sancho")) 

```
</details>


`r ne`. Visualize the most frequent words in your cleaned Quijote sample using a word cloud.

```{r echo=FALSE}
inc(params$hidden, ne)
```

<details>
  <summary type="button">Solution</summary>
```{r eval=FALSE}
quijote_counts <- tidy_quijote |>
  count(word, sort = TRUE)

wordcloud(words = quijote_counts$word, 
          freq = quijote_counts$n,
          min.freq = 2,
          max.words = 80, 
          colors = brewer.pal(8, "Dark2"))
```
</details>

`r ne`. Analyze the sentiments present in your text sample to determine the overall emotional tone.

```{r echo=FALSE}
inc(params$hidden, ne)
```

<details>
  <summary type="button">Solution</summary>
```{r eval=FALSE}
# Reconstruct text for syuzhet or do word-by-word sentiment if using tidytext lexicon
# Using syuzhet on the original sample lines is often better for context, 
# but let's try token-based simply for the exercise or just use the lines:

# Extract sentiments from the lines
quijote_sentiments <- get_nrc_sentiment(sample_lines$text, language = "spanish")

# Summarize/Plot
quijote_sentiments |>
  summarise(across(everything(), sum)) |>
  pivot_longer(everything(), names_to = "sentiment", values_to = "count") |>
  ggplot(aes(reorder(sentiment, count), count)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Sentiments in Don Quijote Sample")
```
</details>

`r if(params$hidden){"-->"}`