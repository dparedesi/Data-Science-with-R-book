# Data Frames

## Introduction to Data Frames

In previous chapters, we explored different types of objects in R, such as variables, vectors, lists, and matrices. These objects allow us to store information in more efficient ways. Now, in this chapter, we will delve into the world of **data frames**, an essential tool for organizing and analyzing information that will help you make the best decision about your move to the United States.

### What are data frames?

Imagine a spreadsheet, with rows and columns organizing information in a tabular way. In R, a data frame is precisely that: a data structure that stores information in a tabular format, with rows representing **observations** (for example, every US city) and columns representing **variables** (such as population, cost of living, crime rate).

Each column of a data frame can contain a different data type: numeric, character, logical, factor, etc. This makes data frames very versatile for storing diverse information.

For example, a data frame about US cities could serve as a comprehensive record. It might contain a character column for the `city` name and another for the `state` it belongs to. Numeric columns could store the `population` and the `area` in square kilometers, while a logical column like `has_beach` could indicate whether the city is coastal.

### Why data frames?

In R, there are various structures for organizing data, such as vectors, lists, and matrices. However, data frames stand out as a fundamental tool in data analysis. Why?

Data frames offer a unique combination of features that make them ideal for representing and manipulating complex information:

Data frames are uniquely suited for data analysis because of their specific features. Their **tabular structure** organizes data into rows and columns, similar to a spreadsheet, making it intuitive to visualize. They offer **flexibility** by allowing each column to hold a different data type, such as numbers, text, or dates. this structure also ensures **efficiency**, as most R analysis packages are optimized to work directly with data frames.

In summary, data frames are a versatile and powerful data structure that adapts to the needs of modern data analysis.

### Data Frames in action: exploring information about the United States

In the context of your move to the United States, data frames will be essential for organizing and analyzing the information you need to make the best decision. We can use data frames to store information about:

We can use data frames to store and correlate various aspects of your potential new home. You might track **crime rates** across different states, compare the **cost of living** (housing, food, transportation) in target cities, analyze **climate data** like temperature and precipitation, or study **demographics** such as population age and education levels.

With this information organized in data frames, you will be able to perform deeper analyses and make more informed decisions about your move.

## Creating Data Frames: Building your database for the move

Now that you know what data frames are and why they are so important in data analysis, it's time to learn how to create them. In R, we can create data frames in different ways: importing data from external files or creating them manually.

### Importing data from files: CSV, Excel

A common way to create data frames is by importing data from external files, such as CSV (Comma Separated Values) files or Excel files. R offers us functions to read data from different formats.

One of the most common ways to create data frames is by importing data from external files. For **CSV (Comma Separated Values)** files, we rely on the `read_csv()` function from the `readr` package (part of the tidyverse), which is faster and more robust than the base R equivalent. To import a file, you simply provide its URL or file path:

    ```{r}
    library(readr)
    url <- "https://dparedesi.github.io/DS-with-R-datasets/student-grades.csv"

    # Import data from a CSV file called "student-grades.csv"
    grades <- read_csv(url)
    
    grades
    ```

The `read_csv()` function offers several arguments to customize how files are read. The `header` argument allows you to specify if the first row contains column names, while `sep` defines the column separator (defaulting to a comma). You can also use `dec` to set the character used for decimal points.

For **Excel files**, we use the `read_excel()` function from the `readxl` package. This function works similarly but includes specific arguments like `sheet` to specify which spreadsheet tab to import.

    ```{r eval=FALSE}
    # Install the readxl package (if you don't have it installed)
    install.packages("readxl")
    
    # Load the readxl package
    library(readxl)
    
    # Import data from an Excel file called "states.xlsx"
    states <- read_excel("states.xlsx")
    ```

### Creating data frames manually

We can also create data frames manually, combining vectors with the `data.frame()` function.

```{r}
# Create vectors with information about cities
cities <- c("New York", "Los Angeles", "Chicago")
states <- c("New York", "California", "Illinois")
population <- c(8.4e6, 3.9e6, 2.7e6)

# Create a data frame with city information
df_cities_simple <- data.frame(city = cities, state = states, population = population)

df_cities_simple
```

In this example, we create a data frame called `df_cities_simple` with three columns: `city`, `state`, and `population`. Each column is created from a vector. Note that the vectors must have the same length to be combined into a data frame.

### Examples

We can use data frames to organize diverse information about our move to the United States. For example, we could create a data frame with information about different cities, including their cost of living, crime rate, and climate. We could also create a data frame with information about the different states, including their population, gross domestic product (GDP), and education system.

```{r}
# Create a data frame with information about cities
df_cities <- data.frame(
  city = c("New York", "Los Angeles", "Chicago", "Houston"),
  state = c("New York", "California", "Illinois", "Texas"),
  cost_of_living = c(3.5, 2.8, 2.5, 2.0),  # In thousands of dollars
  crime_rate = c(400, 350, 500, 450),  # Per 100,000 inhabitants
  climate = c("Temperate", "Mediterranean", "Continental", "Subtropical")
)

df_cities

# Create a data frame with information about states
df_states <- data.frame(
  state = c("California", "Texas", "Florida", "New York"),
  population = c(39.2e6, 29.0e6, 21.4e6, 19.4e6),
  gdp = c(3.2e12, 1.8e12, 1.1e12, 1.7e12),  # In dollars
  education_system = c("Good", "Regular", "Good", "Excellent")
)

df_states
```

These data frames will allow us to analyze the information more efficiently and make more informed decisions about our move.

## Exploring Data Frames: Discovering the secrets of your data

We have already learned to create data frames, now it is time to explore their content and discover the information they hide. R offers us various tools to examine and understand our data.

### Accessing rows, columns, and cells

A data frame is like a map organized in rows and columns. To access the information we need, we must know how to navigate this map. R provides us with different ways to access rows, columns, and cells of a data frame.

There are several ways to access specific data within a dataframe. To retrieve a **column**, you can use the `$` operator (e.g., `df_cities$state`) or bracket notation with the column name in quotes (e.g., `df_states["population"]`). To access a specific **row**, use brackets with the row number (e.g., `df_cities[3, ]`). For a precise **cell** at the intersection of a row and column, specify both indices (e.g., `df_states[2, 3]`). You can also **filter rows** based on conditions, such as extracting all cities where the cost of living is less than 3 using a logical expression inside the brackets.

### Functions for exploring data frames

R offers several useful functions for exploring data frames:

R provides useful functions for a quick overview of your data. `head()` displays the first six rows, while `tail()` shows the last six. To understand the structure—such as column names and data types—you can use `str()`. For a statistical overview including mean, median, and quartiles, `summary()` is the go-to function. Additionally, `View()` opens an interactive spreadsheet-style window to browse the data.

### Examples: exploring data frames with move information

By exploring the data frames we created in the previous section, we can obtain valuable information about US cities and states. For example, we could use `summary()` to get descriptive statistics of the cost of living in different cities, or `View()` to examine information about each state in detail.

```{r eval=TRUE}
# Get descriptive statistics of cost of living in different cities
summary(df_cities$cost_of_living)

```

```{r eval=FALSE}
# Examine detailed information about each state
View(df_states)
```

In addition to the mentioned functions, we can use other tools to explore our data frames. For example, we can use the `table()` function to get the frequency of each value in a categorical column, such as the `climate` column in the `df_cities` data frame.

```{r eval=TRUE}
table(df_cities$climate)
```

We can also use the `hist()` function to create a histogram of a numeric column, such as the population column in the `df_states` data frame.

```{r eval=TRUE}
hist(df_states$population)
```

These are just some ideas of how we can explore our data frames. As you become familiar with R, you will discover new functions and techniques for analyzing and visualizing your data.

## Manipulating Data Frames: Transforming your data

In the previous section, we learned to explore data frames and access the information they contain. Now, we will go a step further and learn to **manipulate** data frames, transforming data to answer specific questions and obtain relevant information for our move.

### Introduction to the pipeline operator (`|>`)

Before modifying data frames, we will introduce a tool to write more readable and efficient code: the **native pipeline operator** (`|>`). This operator was introduced in R 4.1 (2021) as a built-in language feature, meaning it works without any additional packages.

> **Note:** You may also encounter the `%>%` pipe operator from the `magrittr` package (part of the tidyverse). Both `|>` and `%>%` work similarly for most data analysis tasks. We use the native `|>` operator throughout this book as it is built into R, but `%>%` is still widely used in older codebases.

The pipeline operator allows us to chain several operations sequentially. Instead of writing nested code, we can use the pipeline operator to "pass" the result of one operation to the next.

To use additional data manipulation functions, we'll load the `tidyverse` package, which includes `dplyr` - a package with many useful functions for working with data frames.

A package in R is like a toolbox with additional functions and data for performing specific tasks. To use a package's functions, we must first install it and then load it into our working environment.

To install the `tidyverse` package, we can use the following instruction in the R console:

```{r eval=FALSE}
install.packages("tidyverse")
```

This will install `tidyverse` and all the packages it contains, including `dplyr`. Once the package is installed, we can load it with the `library()` function:

```{r}
library(tidyverse)
```

Now we can use the pipeline operator `(|>)` and functions from `dplyr.`

For example, we'll use the `murders` dataset from the `dslabs` package. This dataset contains gun murder data by US state in 2010, including variables like state name, abbreviation, region, population, and total murders. Let's use a pipeline to view selected columns:

```{r eval=FALSE}
install.packages("dslabs")
```

```{r}
# Load library and dataset
library(dslabs)
data(murders)

# Pipeline
murders |> select(state, population, total)

```

Code with pipeline is easier to read and understand, as it follows the natural flow of operations. Pipeline creates a view; we are not editing the `murders` data frame.

We can show the first rows using the `head()` function:

```{r}
head(murders |> select(state, population, total))
```

We can also use the pipeline operator to show the first rows:
```{r}
murders |> select(state, population, total) |> head()
```

For better readability, we will use one function per line, obtaining the same result:
```{r}
murders |>
  select(state, population, total) |> # Select columns
  head() # Show first 6 rows
```

### Transforming a table with `mutate()`
We can create new columns or modify existing ones using the `mutate()` function. For example, to add a column with the homicide rate per 100,000 inhabitants to the `murders` data frame:

```{r}
murders |>
  mutate(ratio = total / population * 100000) |>
  head()
```
This creates a view with the additional `ratio` column.

If we want to modify the `murders` data frame directly, we use the assignment operator `<-`:

```{r}
murders <- murders |>
  mutate(ratio = total / population * 100000)
```


### Filtering data: selecting cities that interest you

We can filter rows meeting a condition using the `filter()` function. For example, to get states with less than 1 homicide per 100,000 inhabitants:

```{r}
# Load dataset
data(murders)

murders |>
  mutate(ratio = total / population * 100000) |>
  filter(ratio < 1)
```

We can use different operators to create our conditions:

R supports standard comparison operators to create conditions: greater than (`>`), less than (`<`), greater than or equal to (`>=`), less than or equal to (`<=`), equal to (`==`), and different from (`!=`). You can combine multiple conditions using logical operators: `&` for AND, `|` for OR, and `!` for NOT.

For example, to filter by ratio less than 1 and West region:

```{r}
murders |>
  mutate(ratio = total / population * 100000) |>
  filter(ratio < 1 & region == "West")
```

### Sorting data: finding the safest cities

The `arrange()` function from the `dplyr` package allows us to order the rows of a data frame based on one or more columns. Imagine you have a data frame with information about different cities, and you want to order them from safest to least safe, based on their crime rate. Or perhaps you want to order them by cost of living, from cheapest to most expensive. `arrange()` allows you to do this easily.

For example, to order states by homicide rate (from lowest to highest):

```{r}
murders |>
  mutate(ratio = total / population * 100000) |>
  arrange(ratio) |>
  head()
```

If we want to sort in descending order, we use the `desc()` function:

```{r}
murders |>
  mutate(ratio = total / population * 100000) |>
  arrange(desc(ratio)) |>
  head()
```

We can also sort by multiple columns. For example, if we want to sort first by `region` and then by `state` (in alphabetical order):

```{r}
murders |> 
  arrange(region, state) |>
  head()
```

### Aggregating and summarizing data: obtaining general overview

The `summarize()` function from the `dplyr` package allows us to calculate descriptive statistics for one or more columns of a data frame. It's like summarizing information from our data frame into a single number or a set of numbers.

For example, to calculate the mean population of states:

```{r}
murders |>
  summarize(mean_population = mean(population))
```

We can combine `summarize()` with `group_by()` to calculate statistics by groups. For example, to calculate average population by region:

```{r}
murders |>
  group_by(region) |>
  summarize(mean_population = mean(population))
```

### Joining data frames: combining information

Imagine you have two data frames: one with information about cities (name, population, etc.) and another with information about the states those cities belong to (state name, governor, etc.). If you want to combine information from both data frames to have a single data frame with all information about cities and their states, you can use `dplyr` join functions.

`dplyr` offers several functions for joining data frames, such as `left_join()`, `right_join()`, `inner_join()`, and `full_join()`. Each function performs a different type of join, depending on how data frame rows are combined.

The `left_join()` function joins two data frames keeping all rows from the first data frame (the one on the left) and adding columns from the second data frame that match the first data frame's rows. If a row from the first data frame has no match in the second data frame, new columns will have `NA` values.

For example, if we have a data frame with city information and another with state information, we can join them by the `state` column:

```{r}
df_cities_states <- left_join(df_cities, df_states, by = "state")
```

The resulting data frame `df_cities_states` will contain information from both data frames combined. If a city in `df_cities` does not have a corresponding state in `df_states`, columns from `df_states` will have `NA` values for that city.

Let's see a concrete example. Suppose we have the following data frames:

```{r}
df_cities <- data.frame(
  city = c("New York", "Los Angeles", "Chicago", "Houston"),
  state = c("New York", "California", "Illinois", "Texas")
)

df_states <- data.frame(
  state = c("California", "Texas", "Florida"),
  governor = c("Gavin Newsom", "Greg Abbott", "Ron DeSantis")
)

# Join data frames by "state" column
df_cities_states <- left_join(df_cities, df_states, by = "state")

df_cities_states
```

In this example, `left_join()` combines `df_cities` and `df_states` data frames by the `state` column. Note that "New York" and "Chicago" cities have `NA` values in the `governor` column, since their states ("New York" and "Illinois") are not present in the `df_states` data frame.

The other join functions (`right_join()`, `inner_join()`, and `full_join()`) work similarly, but with different criteria for combining data frame rows.

The other join functions work similarly but with different inclusion criteria. `right_join()` does the opposite of `left_join()`, keeping all rows from the right data frame and only matching rows from the left. `inner_join()` is more restrictive, keeping only rows that have matches in both tables, while `full_join()` is the most inclusive, retaining all rows from both data frames and filling in `NA` where no match exists.

You can consult `dplyr` documentation for more information about these functions.

### Examples

The `dplyr` functions we have seen allow us to perform complex data transformations to answer specific questions about our move to the United States. Let's see some examples with R code:

**Examples of analysis questions**

We can combine these tools to answer specific questions. To find suitable locations, we might filtered for cities with a "Good" education system and a cost of living index below 2.5. Alternatively, to study economic prosperity, we could sort states by their GDP per capita (calculated as GDP divided by population) in descending order. For a more comprehensive climate analysis, we could join our city data with a separate climate table.

With these tools, you will be able to explore and analyze information about the United States to make the best decision about your move.

`r if(params$hidden){"<!--"}`

## Exercises

`r ne`. Report the state abbreviation `abb` and population `population` columns from the `murders` data frame

```{r echo=FALSE}
inc(params$hidden, ne)
```

<details>
  <summary type="button">Solution</summary>
```{r eval=FALSE}
murders |>
  select(abb, population)

```
</details>

`r ne`. Report all data frame data that are not from the South region.

```{r echo=FALSE}
inc(params$hidden, ne)
```

<details>
  <summary type="button">Solution</summary>
```{r eval=FALSE}
murders |>
  filter(region != "South")

```
</details>

> If we want to filter all records that are from the South and West region we will use `%in%` instead of `==` to compare versus a vector

`r ne`. Create the vector `south_and_west` containing values "South" and "West". Then filter records that are from those two regions.

```{r echo=FALSE}
inc(params$hidden, ne)
```

<details>
  <summary type="button">Solution</summary>
```{r eval=FALSE}
south_and_west <- c("South", "West")
  
murders |>
  filter(region %in% south_and_west)

```
</details>

`r ne`. Add the `ratio` column to the `murders` data frame with the murder ratio per 100,000 inhabitants. Then, filter those with a ratio less than 0.5 and are from "South" and "West" regions. Report `state`, `abb`, and `ratio` columns.

```{r echo=FALSE}
inc(params$hidden, ne)
```

<details>
  <summary type="button">Solution</summary>
```{r eval=FALSE}
data(murders)

south_and_west <- c("South", "West")
  
murders <- murders |>
  mutate(ratio = total/population*100000) |>
  filter(ratio < 0.5 & region %in% south_and_west) |>
  select(state, abb, ratio)

murders

```
</details>

> To sort using pipeline we use the `arrange(x)` function, where `x` is the name of the column we want to take as reference which will sort in ascending order or `arrange(desc(x))` to sort in descending order.

`r ne`. Modify the code generated in the previous exercise to sort the result by the `ratio` field.

```{r echo=FALSE}
inc(params$hidden, ne)
```

<details>
  <summary type="button">Solution</summary>
```{r eval=FALSE}
data(murders)

south_and_west <- c("South", "West")
  
murders <- murders |>
  mutate(ratio = total/population*100000) |>
  filter(ratio < 0.5 & region %in% south_and_west) |>
  select(state, abb, ratio) |>
  arrange(ratio)

murders

```
So, finally we can know what state options we have to be able to move and solve the presented case.
</details>

`r if(params$hidden){"-->"}`

## Data frames in plots

Now we will see some functions that allow us to visualize our data. Little by little we will build more complex and visually more aesthetic graphs to present. First let's see the most basic functions R presents us. In the next chapter we will see in more detail graph types and in which situations it is recommended to use one or another graph.

### Scatter plots
One of the most used plots in R is the scatter plot, which is a type of mathematical diagram using Cartesian coordinates to show values for two variables for a set of data [@jarrell1994, pp. 492]. By default we assume the variables to analyze are independent. Thus, the scatter plot will show the degree of correlation (not causality) between the two variables.

The simplest way to plot a scatter plot is with the `plot(x,y)` function, where `x` and `y` are vectors indicating the _x-axis_ coordinates and _y-axis_ coordinates of each point we want to plot. For example, let's see the relationship between population size and total murders.

```{r eval=TRUE}
# Let's store population data in the x_axis object
x_axis <- murders$population

# Let's store total murders data in the y_axis object
y_axis <- murders$total

# With this code we create the scatter plot
plot(x_axis, y_axis)

```

We can see a correlation between population and number of cases. Let's transform the `x_axis` dividing by one million (${10}^6$). Thus we will have the x axis expressed in millions.

```{r eval=TRUE}
x_axis <- murders$population/10^6
y_axis <- murders$total

plot(x_axis, y_axis)

```

### Histograms
We can also create histograms from a vector with the `hist` function.

```{r eval=TRUE}
data(murders)

murders <- murders |>
  mutate(ratio = total/population*100000)

hist(murders$ratio)

```

The ease R gives us to create graphs will save us time for analysis. From here we can quickly see that most states have a `ratio < 5`.

### Box plot

Finally, R allows us to create box plots easily with the `boxplot` function. So, if we wanted to analyze the distribution of `ratio` we would use the following code:

```{r eval=TRUE}
boxplot(murders$ratio)

```

## Data interpretation
We have seen graphs that can be generated with a line of code, but we need to interpret them. To do so, we need to learn or remember some statistics. Throughout this book we will learn statistical concepts not going deep into the math part, but from the practical part and leveraging that functions already exist in R.

Let's remember our case/problem. We have a list of murders in each of the 51 states. If we order them by the total column we would have:

```{r eval=TRUE}
murders |> 
  arrange(total) |>
  head()

```

R provides us with the `summary()` function, which gives us a summary of a vector's data.

```{r eval=TRUE}
summary(murders$total)
```

The summary provides key insights: the **Min** and **Max** show the range of the data; the **1st Qu** (first quartile) and **3rd Qu** (third quartile) indicate the 25th and 75th percentiles; the **Median** marks the exact middle of the distribution; and the **Mean** gives the arithmetic average.

### Quartiles
To understand quartiles let's visualize total data in an ordered way. To only obtain a single column in pipeline we will use `.$` before the variable name:

```{r eval=TRUE}
murders |> 
  arrange(total) |>
  pull(total)
```

Quartiles divide our vector into 4 parts with the same amount of data. Given we have 51 values, we would have groups of `51/4 = 12.75`. We would have groups of 13 values (3 groups of 13 elements and one of 12 elements).

For example, the first group would be composed of these numbers:

```{r eval=TRUE, echo=FALSE}
first13  <- murders |> 
  arrange(total) |>
  pull(total)

first13[1:13]
```

The second group would be composed of these numbers:

```{r eval=TRUE, echo=FALSE}
first13  <- murders |> 
  arrange(total) |>
  pull(total)

first13[14:26]
```
And so on. In total 4 groups made up of 25% of data each.

#### First quartile
Therefore, when we see the 1st quartile, `1st Qu.`, let's think that is the cut indicating up to where I can find 25% of the data.

```{r eval=TRUE}
summary(murders$total)
```

In our example `24.5` indicates that every number less than or equal to that number will be within the first 25% of data (25% of 51 data points = 12.75, rounded to 13 data points).

If we list numbers less than or equal to 24.5 we will have this list:
```{r eval=TRUE}
murders |>
  arrange(total) |>
  filter(total <= 24.5) |>
  pull(total)
```

Which is exactly the same list we obtained previously for the first group.

#### Second quartile or median
The second quartile, also called the median (`Median`), indicates the cut of the second group. The first group contains the first 25% of data, the second group has additional 25%. So this cut would give us exactly the value found in the middle.

```{r eval=TRUE}
summary(murders$total)
```

In our example `97` indicates that below that number we will find 50% of total data (50% of 51 data points = 25.5, rounded to 26 data points).

```{r eval=TRUE}
murders |>
  arrange(total) |>
  filter(total <= 97) |>
  pull(total)
```

#### Third quartile
The third quartile is the cut of the third group. Up to the median we already had 50%, if we add another 25% of data we would have 75%.

```{r eval=TRUE}
summary(murders$total)
```

In our example `268` indicates that below that number we will find 75% of total data (75% of 51 data points = 38.25, rounded to 38 data points).

### Interpretation of box plot
We are now ready to create a box plot with total murders and interpret results.

```{r eval=TRUE}
boxplot(murders$total)
```

The box starts at value 24.5 (first quartile) and ends at value 268 (third quartile). The thick line represents the median (second quartile), 97 in our example.

Between the first quartile and third quartile (between 24.5 and 97 for our example) we will find 50% of the data, also called interquartile range or `IQR`.

Outside the box we see a vertical line upwards and another downwards, showing the range of our data. Outside those lines we see dots which are atypical data very far from the mean, known as **outliers**.

We can quickly find to which states these extreme data belong if we sort the table descendingly using the `desc` function:

```{r eval=TRUE}
murders |>
  arrange(desc(total)) |>
  head()
```

We see that in California `1257` cases were reported. That is one of the extreme data points we see in the box plot.

### Examples
1. Create variable `pop_log10` and store log base 10 data of population (`log10()` function). Perform the same log base 10 transformation for total murders and store it in variable `tot_log10`. Generate a scatter plot of these two variables.

```{r eval=TRUE}
pop_log10 <- log10(murders$population)
tot_log10 <- log10(murders$total)

plot(pop_log10, tot_log10)
```

2. Create a histogram of population in millions (divided by ${10}^6$).

```{r eval=TRUE}
hist(murders$population/10^6)
```

3. Create a box plot of population.

```{r eval=TRUE}
boxplot(murders$population)
```

`r if(params$hidden){"<!--"}`

## Exercises

Below, you will find a series of exercises with different levels of difficulty. It is time to put into practice what you have learned in this chapter. Remember you can use `dplyr` functions like `filter()`, `arrange()`, `mutate()`, `summarize()`, `group_by()` and `left_join()` to manipulate data frames.

`r ne`. Create a data frame called `my_expenses`. It should contain a `category` factor with levels "Housing", "Transport", "Food", and "Entertainment", along with three numeric columns (`january`, `february`, `march`) recording expenses for each category.

```{r echo=FALSE}
inc(params$hidden, ne)
```

<details>
  <summary type="button">Solution</summary>
```{r}
my_expenses <- data.frame(
  category = factor(c("Housing", "Transport", "Food", "Entertainment")),
  january = c(1500, 300, 500, 200),
  february = c(1500, 250, 400, 150),
  march = c(1500, 350, 550, 250)
)

my_expenses
```
</details>

`r ne`. Use `head()`, `tail()`, `str()` and `summary()` functions to explore `my_expenses` data frame.

```{r echo=FALSE}
inc(params$hidden, ne)
```

<details>
  <summary type="button">Solution</summary>
```{r}
head(my_expenses)
tail(my_expenses)
str(my_expenses)
summary(my_expenses)
```
</details>

`r ne`. Access `february` column of `my_expenses` data frame using `$` operator. Then, access the second row of the data frame using brackets.

```{r echo=FALSE}
inc(params$hidden, ne)
```

<details>
  <summary type="button">Solution</summary>
```{r}
my_expenses$february
my_expenses[2, ]
```
</details>

`r ne`. Filter `my_expenses` data frame to get only rows where expenses in `january` are greater than 400.

```{r echo=FALSE}
inc(params$hidden, ne)
```

<details>
  <summary type="button">Solution</summary>
```{r}
my_expenses |>
  filter(january > 400)
```
</details>

`r ne`. Sort `my_expenses` data frame descendingly by expenses in `march`.

```{r echo=FALSE}
inc(params$hidden, ne)
```

<details>
  <summary type="button">Solution</summary>
```{r}
my_expenses |>
  arrange(desc(march))
```
</details>

`r ne`. Add a column called `total` to `my_expenses` data frame containing the sum of January, February, and March expenses for each category.

```{r echo=FALSE}
inc(params$hidden, ne)
```

<details>
  <summary type="button">Solution</summary>
```{r}
my_expenses <- my_expenses |> 
  mutate(total = january + february + march)
```
</details>

`r ne`. Calculate mean and standard deviation of total expenses for each category in `my_expenses` data frame.

```{r echo=FALSE}
inc(params$hidden, ne)
```

<details>
  <summary type="button">Solution</summary>
```{r}
my_expenses |> 
  summarize(mean_total = mean(total), 
            std_total = sd(total))
```
</details>

`r ne`. Group `my_expenses` data frame by category and calculate sum of expenses for each month.

```{r echo=FALSE}
inc(params$hidden, ne)
```

<details>
  <summary type="button">Solution</summary>
```{r}
my_expenses |> 
  group_by(category) |> 
  summarize(sum_january = sum(january), 
            sum_february = sum(february), 
            sum_march = sum(march))
```
</details>

`r ne`. Visually analyze the following chart describing total murder distribution by regions. Just by visualizing it, could you point out which region has the smallest data range, ignoring outliers? Which region has the highest median?

```{r echo=FALSE}
inc(params$hidden, ne)
```

```{r eval=TRUE, echo=FALSE}
boxplot(total~region, data=murders)
```

<details>
  <summary type="button">Solution</summary>
  West has the smallest data range and has two outliers. South has the highest median among all regions.
  
  Analyzing solely by seeing a chart allows us to put ourselves in the final observer's shoes and understand if decisions can be made just with presented information.
</details>

`r ne`. Create `south` vector where you store filtered data of total murders occurred in South region. Then, create a histogram of `south` vector.

```{r echo=FALSE}
inc(params$hidden, ne)
```

<details>
  <summary type="button">Solution</summary>
```{r eval=FALSE}
south <- murders |>
  filter(region == "South") |>
  pull(total)

hist(south)
```
</details>

`r ne`. Create a new data frame called `df_cities_climate` combining information from `df_cities` and `df_climate` (you must create `df_climate` data frame with city climate information). Ensure resulting data frame contains all cities from `df_cities`, even if they don't have climate information in `df_climate`.

```{r echo=FALSE}
inc(params$hidden, ne)
```

<details>
  <summary type="button">Solution</summary>
```{r}
df_climate <- data.frame(
  city = c("New York", "Los Angeles", "Chicago"),
  average_temperature = c(12.8, 17.7, 10.7),  # In degrees Celsius
  annual_precipitation = c(1269, 373, 965)  # In millimeters
)

df_cities_climate <- left_join(df_cities, df_climate, by = "city")
```
</details>

`r ne`. Create a data frame with some missing values (`NA`). Then, replace missing values with mean of non-missing values in same column.

```{r echo=FALSE}
inc(params$hidden, ne)
```

<details>
  <summary type="button">Solution</summary>
```{r}
# Create data frame with missing values
df_with_na <- data.frame(
  x = c(1, 2, NA, 4, 5),
  y = c(NA, 7, 8, NA, 10)
)

df_with_na

# Replace missing values with mean
df_with_na <- df_with_na |> 
  mutate(x = ifelse(is.na(x), mean(x, na.rm = TRUE), x),
         y = ifelse(is.na(y), mean(y, na.rm = TRUE), y))

df_with_na
```
</details>

`r ne`. Create a function called `clean_data_frame()` receiving a data frame as argument and replacing missing values with mean of non-missing values in each column.

```{r echo=FALSE}
inc(params$hidden, ne)
```

<details>
  <summary type="button">Solution</summary>
```{r}
clean_data_frame <- function(df) {
  for (col in names(df)) {
    if (is.numeric(df[[col]])) {
      df[[col]] <- ifelse(is.na(df[[col]]), mean(df[[col]], na.rm = TRUE), df[[col]])
    }
  }
  return(df)
}

## Test created function
# Create data frame with missing values to test function
df_test <- data.frame(
  age = c(25, 30, NA, 28, 35),
  height = c(1.75, 1.80, 1.65, NA, 1.70),
  weight = c(70, 80, 75, 65, NA)
)

df_test

# Apply function to test data frame
df_clean <- clean_data_frame(df_test)

# Show clean data frame
df_clean

```
</details>


`r if(params$hidden){"-->"}`
