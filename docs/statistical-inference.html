<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Statistical Inference | Data Science with R</title>
  <meta name="description" content="A comprehensive guide to data science using R, covering fundamentals, visualization, statistics, machine learning with tidymodels, and generative AI integration. Third Edition by Daniel Paredes." />
  <meta name="generator" content="bookdown 0.46 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Statistical Inference | Data Science with R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="A comprehensive guide to data science using R, covering fundamentals, visualization, statistics, machine learning with tidymodels, and generative AI integration. Third Edition by Daniel Paredes." />
  <meta name="github-repo" content="dparedesi/Data-Science-with-R-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Statistical Inference | Data Science with R" />
  
  <meta name="twitter:description" content="A comprehensive guide to data science using R, covering fundamentals, visualization, statistics, machine learning with tidymodels, and generative AI integration. Third Edition by Daniel Paredes." />
  

<meta name="author" content="Author: Mg. Daniel Paredes Inilupu" />


<meta name="date" content="2025-12-25" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="continuous-probabilities.html"/>
<link rel="next" href="introduction-1.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics: Sin Vista -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VJJ963010J"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VJJ963010J');
</script>

<!-- Global site tag (gtag.js) - Google Analytics: Vista -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-165839710-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-165839710-1');
</script>

<!-- Global site tag (gtag.js) - Google Analytics: Agentedec4mbio -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-17876479-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-17876479-1');
</script>


<style type="text/css">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Data Science with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#support-this-work"><i class="fa fa-check"></i>Support This Work</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#stay-connected"><i class="fa fa-check"></i>Stay Connected</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a>
<ul>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html#family"><i class="fa fa-check"></i>Family</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html#mentors-and-inspirations"><i class="fa fa-check"></i>Mentors and Inspirations</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html#contributors"><i class="fa fa-check"></i>Contributors</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#why-r"><i class="fa fa-check"></i>Why R?</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#installing-r"><i class="fa fa-check"></i>Installing R</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#installing-rstudio"><i class="fa fa-check"></i>Installing RStudio</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#rstudio-sections"><i class="fa fa-check"></i>RStudio Sections</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#essential-keyboard-shortcuts"><i class="fa fa-check"></i>Essential Keyboard Shortcuts</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#testing-your-installation"><i class="fa fa-check"></i>Testing Your Installation</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#writing-scripts"><i class="fa fa-check"></i>Writing Scripts</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#whats-next"><i class="fa fa-check"></i>What’s Next?</a></li>
</ul></li>
<li class="part"><span><b>I Fundamentals and Key Tools</b></span></li>
<li class="chapter" data-level="1" data-path="objects.html"><a href="objects.html"><i class="fa fa-check"></i><b>1</b> Objects</a>
<ul>
<li class="chapter" data-level="1.1" data-path="objects.html"><a href="objects.html#what-are-objects-in-r"><i class="fa fa-check"></i><b>1.1</b> What are objects in R?</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="objects.html"><a href="objects.html#r-as-an-object-oriented-language"><i class="fa fa-check"></i><b>1.1.1</b> R as an object-oriented language</a></li>
<li class="chapter" data-level="1.1.2" data-path="objects.html"><a href="objects.html#the-power-of-abstraction"><i class="fa fa-check"></i><b>1.1.2</b> The power of abstraction</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="objects.html"><a href="objects.html#variables-the-first-objects-on-your-journey"><i class="fa fa-check"></i><b>1.2</b> Variables: The first objects on your journey</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="objects.html"><a href="objects.html#creating-variables-in-r"><i class="fa fa-check"></i><b>1.2.1</b> Creating variables in R</a></li>
<li class="chapter" data-level="1.2.2" data-path="objects.html"><a href="objects.html#operations-with-variables"><i class="fa fa-check"></i><b>1.2.2</b> Operations with variables</a></li>
<li class="chapter" data-level="1.2.3" data-path="objects.html"><a href="objects.html#best-practices-for-naming-variables"><i class="fa fa-check"></i><b>1.2.3</b> Best practices for naming variables</a></li>
<li class="chapter" data-level="1.2.4" data-path="objects.html"><a href="objects.html#data-types"><i class="fa fa-check"></i><b>1.2.4</b> Data types</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="objects.html"><a href="objects.html#object-types-for-complex-data"><i class="fa fa-check"></i><b>1.3</b> Object types for complex data</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="objects.html"><a href="objects.html#vectors-organizing-information-of-the-same-type"><i class="fa fa-check"></i><b>1.3.1</b> Vectors: organizing information of the same type</a></li>
<li class="chapter" data-level="1.3.2" data-path="objects.html"><a href="objects.html#lists-grouping-objects-of-different-types"><i class="fa fa-check"></i><b>1.3.2</b> Lists: grouping objects of different types</a></li>
<li class="chapter" data-level="1.3.3" data-path="objects.html"><a href="objects.html#matrices-organizing-data-in-rows-and-columns"><i class="fa fa-check"></i><b>1.3.3</b> Matrices: organizing data in rows and columns</a></li>
<li class="chapter" data-level="1.3.4" data-path="objects.html"><a href="objects.html#arrays-multidimensional-matrices"><i class="fa fa-check"></i><b>1.3.4</b> Arrays: multidimensional matrices</a></li>
<li class="chapter" data-level="1.3.5" data-path="objects.html"><a href="objects.html#factors-representing-categorical-data"><i class="fa fa-check"></i><b>1.3.5</b> Factors: representing categorical data</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="objects.html"><a href="objects.html#the-universe-of-objects-in-r"><i class="fa fa-check"></i><b>1.4</b> The Universe of Objects in R</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="objects.html"><a href="objects.html#philosophy-of-objects-in-r"><i class="fa fa-check"></i><b>1.4.1</b> Philosophy of objects in R</a></li>
<li class="chapter" data-level="1.4.2" data-path="objects.html"><a href="objects.html#comparison-with-other-languages"><i class="fa fa-check"></i><b>1.4.2</b> Comparison with other languages</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="objects.html"><a href="objects.html#exercises"><i class="fa fa-check"></i><b>1.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="functions.html"><a href="functions.html"><i class="fa fa-check"></i><b>2</b> Functions</a>
<ul>
<li class="chapter" data-level="2.1" data-path="functions.html"><a href="functions.html#introduction-to-the-world-of-functions"><i class="fa fa-check"></i><b>2.1</b> Introduction to the world of functions</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="functions.html"><a href="functions.html#what-are-functions"><i class="fa fa-check"></i><b>2.1.1</b> What are functions?</a></li>
<li class="chapter" data-level="2.1.2" data-path="functions.html"><a href="functions.html#why-use-functions"><i class="fa fa-check"></i><b>2.1.2</b> Why use functions?</a></li>
<li class="chapter" data-level="2.1.3" data-path="functions.html"><a href="functions.html#first-functions-exploring-basic-r-functions"><i class="fa fa-check"></i><b>2.1.3</b> First functions: exploring basic R functions</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="functions.html"><a href="functions.html#anatomy-of-a-function"><i class="fa fa-check"></i><b>2.2</b> Anatomy of a function</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="functions.html"><a href="functions.html#arguments-the-ingredients-of-the-function"><i class="fa fa-check"></i><b>2.2.1</b> Arguments: the ingredients of the function</a></li>
<li class="chapter" data-level="2.2.2" data-path="functions.html"><a href="functions.html#body-the-instructions-of-the-function"><i class="fa fa-check"></i><b>2.2.2</b> Body: the instructions of the function</a></li>
<li class="chapter" data-level="2.2.3" data-path="functions.html"><a href="functions.html#return-value-the-result-of-the-function"><i class="fa fa-check"></i><b>2.2.3</b> Return value: the result of the function</a></li>
<li class="chapter" data-level="2.2.4" data-path="functions.html"><a href="functions.html#examples-creating-simple-functions-step-by-step"><i class="fa fa-check"></i><b>2.2.4</b> Examples: creating simple functions step by step</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="functions.html"><a href="functions.html#mastering-the-use-of-functions"><i class="fa fa-check"></i><b>2.3</b> Mastering the use of functions</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="functions.html"><a href="functions.html#functions-with-a-variable-number-of-arguments-...-adapting-to-different-situations"><i class="fa fa-check"></i><b>2.3.1</b> Functions with a variable number of arguments (<code>...</code>): Adapting to different situations</a></li>
<li class="chapter" data-level="2.3.2" data-path="functions.html"><a href="functions.html#variable-scope-local-and-global-variables"><i class="fa fa-check"></i><b>2.3.2</b> Variable scope: local and global variables</a></li>
<li class="chapter" data-level="2.3.3" data-path="functions.html"><a href="functions.html#examples-functions-to-calculate-taxes-discounts-etc."><i class="fa fa-check"></i><b>2.3.3</b> Examples: functions to calculate taxes, discounts, etc.</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="functions.html"><a href="functions.html#higher-order-functions"><i class="fa fa-check"></i><b>2.4</b> Higher-order functions</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="functions.html"><a href="functions.html#lapply-and-sapply-applying-a-function-to-each-element"><i class="fa fa-check"></i><b>2.4.1</b> <code>lapply()</code> and <code>sapply()</code>: applying a function to each element</a></li>
<li class="chapter" data-level="2.4.2" data-path="functions.html"><a href="functions.html#apply-applying-a-function-to-rows-or-columns"><i class="fa fa-check"></i><b>2.4.2</b> <code>apply()</code>: applying a function to rows or columns</a></li>
<li class="chapter" data-level="2.4.3" data-path="functions.html"><a href="functions.html#mapply-applying-a-function-to-multiple-arguments"><i class="fa fa-check"></i><b>2.4.3</b> <code>mapply()</code>: applying a function to multiple arguments</a></li>
<li class="chapter" data-level="2.4.4" data-path="functions.html"><a href="functions.html#examples-data-analysis-with-higher-order-functions"><i class="fa fa-check"></i><b>2.4.4</b> Examples: data analysis with higher-order functions</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="functions.html"><a href="functions.html#closures-functions-with-memory"><i class="fa fa-check"></i><b>2.5</b> Closures: functions with memory</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="functions.html"><a href="functions.html#concept-functions-that-remember"><i class="fa fa-check"></i><b>2.5.1</b> Concept: functions that “remember”</a></li>
<li class="chapter" data-level="2.5.2" data-path="functions.html"><a href="functions.html#applications-creating-counters-functions-with-internal-state"><i class="fa fa-check"></i><b>2.5.2</b> Applications: creating counters, functions with internal state</a></li>
<li class="chapter" data-level="2.5.3" data-path="functions.html"><a href="functions.html#examples-simulating-a-game-creating-an-operation-history"><i class="fa fa-check"></i><b>2.5.3</b> Examples: simulating a game, creating an operation history</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="functions.html"><a href="functions.html#debugging-and-error-handling-solving-the-mysteries-of-your-code"><i class="fa fa-check"></i><b>2.6</b> Debugging and error handling: solving the mysteries of your code</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="functions.html"><a href="functions.html#identifying-errors-common-error-messages-in-r"><i class="fa fa-check"></i><b>2.6.1</b> Identifying errors: common error messages in R</a></li>
<li class="chapter" data-level="2.6.2" data-path="functions.html"><a href="functions.html#debugging-tools-debug-traceback"><i class="fa fa-check"></i><b>2.6.2</b> Debugging tools: <code>debug()</code>, <code>traceback()</code></a></li>
<li class="chapter" data-level="2.6.3" data-path="functions.html"><a href="functions.html#error-handling-trycatch"><i class="fa fa-check"></i><b>2.6.3</b> Error handling: <code>tryCatch()</code></a></li>
<li class="chapter" data-level="2.6.4" data-path="functions.html"><a href="functions.html#examples-debugging-functions-with-errors-handling-exceptions"><i class="fa fa-check"></i><b>2.6.4</b> Examples: debugging functions with errors, handling exceptions</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="functions.html"><a href="functions.html#exercises-1"><i class="fa fa-check"></i><b>2.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data-frames.html"><a href="data-frames.html"><i class="fa fa-check"></i><b>3</b> Data Frames</a>
<ul>
<li class="chapter" data-level="3.1" data-path="data-frames.html"><a href="data-frames.html#introduction-to-data-frames"><i class="fa fa-check"></i><b>3.1</b> Introduction to Data Frames</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="data-frames.html"><a href="data-frames.html#what-are-data-frames"><i class="fa fa-check"></i><b>3.1.1</b> What are data frames?</a></li>
<li class="chapter" data-level="3.1.2" data-path="data-frames.html"><a href="data-frames.html#why-data-frames"><i class="fa fa-check"></i><b>3.1.2</b> Why data frames?</a></li>
<li class="chapter" data-level="3.1.3" data-path="data-frames.html"><a href="data-frames.html#data-frames-in-action-exploring-information-about-the-united-states"><i class="fa fa-check"></i><b>3.1.3</b> Data Frames in action: exploring information about the United States</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="data-frames.html"><a href="data-frames.html#creating-data-frames-building-your-database-for-the-move"><i class="fa fa-check"></i><b>3.2</b> Creating Data Frames: Building your database for the move</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="data-frames.html"><a href="data-frames.html#importing-data-from-files-csv-excel"><i class="fa fa-check"></i><b>3.2.1</b> Importing data from files: CSV, Excel</a></li>
<li class="chapter" data-level="3.2.2" data-path="data-frames.html"><a href="data-frames.html#creating-data-frames-manually"><i class="fa fa-check"></i><b>3.2.2</b> Creating data frames manually</a></li>
<li class="chapter" data-level="3.2.3" data-path="data-frames.html"><a href="data-frames.html#examples"><i class="fa fa-check"></i><b>3.2.3</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="data-frames.html"><a href="data-frames.html#exploring-data-frames-discovering-the-secrets-of-your-data"><i class="fa fa-check"></i><b>3.3</b> Exploring Data Frames: Discovering the secrets of your data</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="data-frames.html"><a href="data-frames.html#accessing-rows-columns-and-cells"><i class="fa fa-check"></i><b>3.3.1</b> Accessing rows, columns, and cells</a></li>
<li class="chapter" data-level="3.3.2" data-path="data-frames.html"><a href="data-frames.html#functions-for-exploring-data-frames"><i class="fa fa-check"></i><b>3.3.2</b> Functions for exploring data frames</a></li>
<li class="chapter" data-level="3.3.3" data-path="data-frames.html"><a href="data-frames.html#examples-exploring-data-frames-with-move-information"><i class="fa fa-check"></i><b>3.3.3</b> Examples: exploring data frames with move information</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="data-frames.html"><a href="data-frames.html#manipulating-data-frames-transforming-your-data"><i class="fa fa-check"></i><b>3.4</b> Manipulating Data Frames: Transforming your data</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="data-frames.html"><a href="data-frames.html#introduction-to-the-pipeline-operator"><i class="fa fa-check"></i><b>3.4.1</b> Introduction to the pipeline operator (<code>|&gt;</code>)</a></li>
<li class="chapter" data-level="3.4.2" data-path="data-frames.html"><a href="data-frames.html#transforming-a-table-with-mutate"><i class="fa fa-check"></i><b>3.4.2</b> Transforming a table with <code>mutate()</code></a></li>
<li class="chapter" data-level="3.4.3" data-path="data-frames.html"><a href="data-frames.html#filtering-data-selecting-cities-that-interest-you"><i class="fa fa-check"></i><b>3.4.3</b> Filtering data: selecting cities that interest you</a></li>
<li class="chapter" data-level="3.4.4" data-path="data-frames.html"><a href="data-frames.html#sorting-data-finding-the-safest-cities"><i class="fa fa-check"></i><b>3.4.4</b> Sorting data: finding the safest cities</a></li>
<li class="chapter" data-level="3.4.5" data-path="data-frames.html"><a href="data-frames.html#aggregating-and-summarizing-data-obtaining-general-overview"><i class="fa fa-check"></i><b>3.4.5</b> Aggregating and summarizing data: obtaining general overview</a></li>
<li class="chapter" data-level="3.4.6" data-path="data-frames.html"><a href="data-frames.html#joining-data-frames-combining-information"><i class="fa fa-check"></i><b>3.4.6</b> Joining data frames: combining information</a></li>
<li class="chapter" data-level="3.4.7" data-path="data-frames.html"><a href="data-frames.html#examples-1"><i class="fa fa-check"></i><b>3.4.7</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="data-frames.html"><a href="data-frames.html#exercises-2"><i class="fa fa-check"></i><b>3.5</b> Exercises</a></li>
<li class="chapter" data-level="3.6" data-path="data-frames.html"><a href="data-frames.html#data-frames-in-plots"><i class="fa fa-check"></i><b>3.6</b> Data frames in plots</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="data-frames.html"><a href="data-frames.html#scatter-plots"><i class="fa fa-check"></i><b>3.6.1</b> Scatter plots</a></li>
<li class="chapter" data-level="3.6.2" data-path="data-frames.html"><a href="data-frames.html#histograms"><i class="fa fa-check"></i><b>3.6.2</b> Histograms</a></li>
<li class="chapter" data-level="3.6.3" data-path="data-frames.html"><a href="data-frames.html#box-plot"><i class="fa fa-check"></i><b>3.6.3</b> Box plot</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="data-frames.html"><a href="data-frames.html#data-interpretation"><i class="fa fa-check"></i><b>3.7</b> Data interpretation</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="data-frames.html"><a href="data-frames.html#quartiles"><i class="fa fa-check"></i><b>3.7.1</b> Quartiles</a></li>
<li class="chapter" data-level="3.7.2" data-path="data-frames.html"><a href="data-frames.html#interpretation-of-box-plot"><i class="fa fa-check"></i><b>3.7.2</b> Interpretation of box plot</a></li>
<li class="chapter" data-level="3.7.3" data-path="data-frames.html"><a href="data-frames.html#examples-2"><i class="fa fa-check"></i><b>3.7.3</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="data-frames.html"><a href="data-frames.html#exercises-3"><i class="fa fa-check"></i><b>3.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="advanced-techniques.html"><a href="advanced-techniques.html"><i class="fa fa-check"></i><b>4</b> Advanced Techniques</a>
<ul>
<li class="chapter" data-level="4.1" data-path="advanced-techniques.html"><a href="advanced-techniques.html#metaprogramming-writing-code-that-writes-code"><i class="fa fa-check"></i><b>4.1</b> Metaprogramming: writing code that writes code</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="advanced-techniques.html"><a href="advanced-techniques.html#manipulating-expressions-the-art-of-sculpting-code"><i class="fa fa-check"></i><b>4.1.1</b> Manipulating expressions: The art of sculpting code</a></li>
<li class="chapter" data-level="4.1.2" data-path="advanced-techniques.html"><a href="advanced-techniques.html#examples-3"><i class="fa fa-check"></i><b>4.1.2</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="advanced-techniques.html"><a href="advanced-techniques.html#functional-programming-a-new-paradigm"><i class="fa fa-check"></i><b>4.2</b> Functional programming: a new paradigm</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="advanced-techniques.html"><a href="advanced-techniques.html#basic-principles-of-functional-programming"><i class="fa fa-check"></i><b>4.2.1</b> Basic principles of functional programming</a></li>
<li class="chapter" data-level="4.2.2" data-path="advanced-techniques.html"><a href="advanced-techniques.html#higher-order-functions-in-r"><i class="fa fa-check"></i><b>4.2.2</b> Higher-order functions in R</a></li>
<li class="chapter" data-level="4.2.3" data-path="advanced-techniques.html"><a href="advanced-techniques.html#examples-4"><i class="fa fa-check"></i><b>4.2.3</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="advanced-techniques.html"><a href="advanced-techniques.html#r6-the-future-of-oop-in-r"><i class="fa fa-check"></i><b>4.3</b> R6: The future of OOP in R</a></li>
<li class="chapter" data-level="4.4" data-path="advanced-techniques.html"><a href="advanced-techniques.html#exercises-4"><i class="fa fa-check"></i><b>4.4</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>II Data Visualization and Summarization</b></span></li>
<li class="chapter" data-level="5" data-path="ggplot-and-dplyr.html"><a href="ggplot-and-dplyr.html"><i class="fa fa-check"></i><b>5</b> Ggplot and dplyr</a>
<ul>
<li class="chapter" data-level="5.1" data-path="ggplot-and-dplyr.html"><a href="ggplot-and-dplyr.html#creating-the-ggplot-object"><i class="fa fa-check"></i><b>5.1</b> Creating the ggplot object</a></li>
<li class="chapter" data-level="5.2" data-path="ggplot-and-dplyr.html"><a href="ggplot-and-dplyr.html#aesthetic-mapping-layer"><i class="fa fa-check"></i><b>5.2</b> Aesthetic mapping layer</a></li>
<li class="chapter" data-level="5.3" data-path="ggplot-and-dplyr.html"><a href="ggplot-and-dplyr.html#geoms-layer"><i class="fa fa-check"></i><b>5.3</b> Geoms layer</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="ggplot-and-dplyr.html"><a href="ggplot-and-dplyr.html#tweaking-aes-and-geoms"><i class="fa fa-check"></i><b>5.3.1</b> Tweaking aes and geoms</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="ggplot-and-dplyr.html"><a href="ggplot-and-dplyr.html#scale-layer"><i class="fa fa-check"></i><b>5.4</b> Scale layer</a></li>
<li class="chapter" data-level="5.5" data-path="ggplot-and-dplyr.html"><a href="ggplot-and-dplyr.html#label-title-and-legend-layer"><i class="fa fa-check"></i><b>5.5</b> Label, title and legend layer</a></li>
<li class="chapter" data-level="5.6" data-path="ggplot-and-dplyr.html"><a href="ggplot-and-dplyr.html#reference-lines"><i class="fa fa-check"></i><b>5.6</b> Reference lines</a></li>
<li class="chapter" data-level="5.7" data-path="ggplot-and-dplyr.html"><a href="ggplot-and-dplyr.html#changing-the-plot-style"><i class="fa fa-check"></i><b>5.7</b> Changing the plot style</a></li>
<li class="chapter" data-level="5.8" data-path="ggplot-and-dplyr.html"><a href="ggplot-and-dplyr.html#saving-plots"><i class="fa fa-check"></i><b>5.8</b> Saving plots</a></li>
<li class="chapter" data-level="5.9" data-path="ggplot-and-dplyr.html"><a href="ggplot-and-dplyr.html#summarizing-data-with-dplyr"><i class="fa fa-check"></i><b>5.9</b> Summarizing data with dplyr</a>
<ul>
<li class="chapter" data-level="5.9.1" data-path="ggplot-and-dplyr.html"><a href="ggplot-and-dplyr.html#summarize-function"><i class="fa fa-check"></i><b>5.9.1</b> Summarize function</a></li>
<li class="chapter" data-level="5.9.2" data-path="ggplot-and-dplyr.html"><a href="ggplot-and-dplyr.html#group-by-function"><i class="fa fa-check"></i><b>5.9.2</b> Group By Function</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="ggplot-and-dplyr.html"><a href="ggplot-and-dplyr.html#exercises-5"><i class="fa fa-check"></i><b>5.10</b> Exercises</a></li>
<li class="chapter" data-level="5.11" data-path="ggplot-and-dplyr.html"><a href="ggplot-and-dplyr.html#key-takeaways"><i class="fa fa-check"></i><b>5.11</b> Key Takeaways</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="gapminder.html"><a href="gapminder.html"><i class="fa fa-check"></i><b>6</b> Gapminder</a>
<ul>
<li class="chapter" data-level="6.1" data-path="gapminder.html"><a href="gapminder.html#initial-gapminder-plots"><i class="fa fa-check"></i><b>6.1</b> Initial gapminder plots</a></li>
<li class="chapter" data-level="6.2" data-path="gapminder.html"><a href="gapminder.html#facets"><i class="fa fa-check"></i><b>6.2</b> Facets</a></li>
<li class="chapter" data-level="6.3" data-path="gapminder.html"><a href="gapminder.html#time-series"><i class="fa fa-check"></i><b>6.3</b> Time series</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="gapminder.html"><a href="gapminder.html#individual-time-series"><i class="fa fa-check"></i><b>6.3.1</b> Individual time series</a></li>
<li class="chapter" data-level="6.3.2" data-path="gapminder.html"><a href="gapminder.html#multiple-time-series"><i class="fa fa-check"></i><b>6.3.2</b> Multiple time series</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="gapminder.html"><a href="gapminder.html#exercises-6"><i class="fa fa-check"></i><b>6.4</b> Exercises</a></li>
<li class="chapter" data-level="6.5" data-path="gapminder.html"><a href="gapminder.html#histograms-with-ggplot"><i class="fa fa-check"></i><b>6.5</b> Histograms with ggplot</a></li>
<li class="chapter" data-level="6.6" data-path="gapminder.html"><a href="gapminder.html#box-plots-with-ggplot"><i class="fa fa-check"></i><b>6.6</b> Box plots with ggplot</a></li>
<li class="chapter" data-level="6.7" data-path="gapminder.html"><a href="gapminder.html#comparison-of-distributions"><i class="fa fa-check"></i><b>6.7</b> Comparison of distributions</a></li>
<li class="chapter" data-level="6.8" data-path="gapminder.html"><a href="gapminder.html#exercises-7"><i class="fa fa-check"></i><b>6.8</b> Exercises</a></li>
<li class="chapter" data-level="6.9" data-path="gapminder.html"><a href="gapminder.html#key-takeaways-1"><i class="fa fa-check"></i><b>6.9</b> Key Takeaways</a></li>
</ul></li>
<li class="part"><span><b>III Statistics</b></span></li>
<li class="chapter" data-level="" data-path="introduction-to-probabilities.html"><a href="introduction-to-probabilities.html"><i class="fa fa-check"></i>Introduction to Probabilities</a></li>
<li class="chapter" data-level="7" data-path="discrete-probabilities.html"><a href="discrete-probabilities.html"><i class="fa fa-check"></i><b>7</b> Discrete Probabilities</a>
<ul>
<li class="chapter" data-level="7.1" data-path="discrete-probabilities.html"><a href="discrete-probabilities.html#calculation-using-the-mathematical-definition"><i class="fa fa-check"></i><b>7.1</b> Calculation using the mathematical definition</a></li>
<li class="chapter" data-level="7.2" data-path="discrete-probabilities.html"><a href="discrete-probabilities.html#monte-carlo-simulation-for-discrete-variables"><i class="fa fa-check"></i><b>7.2</b> Monte Carlo Simulation for Discrete Variables</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="discrete-probabilities.html"><a href="discrete-probabilities.html#other-functions-to-create-vectors"><i class="fa fa-check"></i><b>7.2.1</b> Other functions to create vectors</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="discrete-probabilities.html"><a href="discrete-probabilities.html#exercises-8"><i class="fa fa-check"></i><b>7.3</b> Exercises</a></li>
<li class="chapter" data-level="7.4" data-path="discrete-probabilities.html"><a href="discrete-probabilities.html#combinations-and-permutations"><i class="fa fa-check"></i><b>7.4</b> Combinations and Permutations</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="discrete-probabilities.html"><a href="discrete-probabilities.html#permutations"><i class="fa fa-check"></i><b>7.4.1</b> Permutations</a></li>
<li class="chapter" data-level="7.4.2" data-path="discrete-probabilities.html"><a href="discrete-probabilities.html#combinations"><i class="fa fa-check"></i><b>7.4.2</b> Combinations</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="discrete-probabilities.html"><a href="discrete-probabilities.html#sufficient-experiments-with-monte-carlo-simulation"><i class="fa fa-check"></i><b>7.5</b> Sufficient Experiments with Monte Carlo Simulation</a></li>
<li class="chapter" data-level="7.6" data-path="discrete-probabilities.html"><a href="discrete-probabilities.html#case-birthdays-in-classrooms"><i class="fa fa-check"></i><b>7.6</b> Case: Birthdays in Classrooms</a></li>
<li class="chapter" data-level="7.7" data-path="discrete-probabilities.html"><a href="discrete-probabilities.html#exercises-9"><i class="fa fa-check"></i><b>7.7</b> Exercises</a></li>
<li class="chapter" data-level="7.8" data-path="discrete-probabilities.html"><a href="discrete-probabilities.html#integrative-exercise"><i class="fa fa-check"></i><b>7.8</b> Integrative Exercise</a>
<ul>
<li class="chapter" data-level="7.8.1" data-path="discrete-probabilities.html"><a href="discrete-probabilities.html#monty-hall-problem"><i class="fa fa-check"></i><b>7.8.1</b> Monty Hall Problem</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="continuous-probabilities.html"><a href="continuous-probabilities.html"><i class="fa fa-check"></i><b>8</b> Continuous Probabilities</a>
<ul>
<li class="chapter" data-level="8.1" data-path="continuous-probabilities.html"><a href="continuous-probabilities.html#learning-objectives"><i class="fa fa-check"></i><b>8.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="8.2" data-path="continuous-probabilities.html"><a href="continuous-probabilities.html#empirical-distribution"><i class="fa fa-check"></i><b>8.2</b> Empirical Distribution</a></li>
<li class="chapter" data-level="8.3" data-path="continuous-probabilities.html"><a href="continuous-probabilities.html#theoretical-distribution"><i class="fa fa-check"></i><b>8.3</b> Theoretical Distribution</a></li>
<li class="chapter" data-level="8.4" data-path="continuous-probabilities.html"><a href="continuous-probabilities.html#key-takeaways-2"><i class="fa fa-check"></i><b>8.4</b> Key Takeaways</a></li>
<li class="chapter" data-level="8.5" data-path="continuous-probabilities.html"><a href="continuous-probabilities.html#exercises-10"><i class="fa fa-check"></i><b>8.5</b> Exercises</a></li>
<li class="chapter" data-level="8.6" data-path="continuous-probabilities.html"><a href="continuous-probabilities.html#monte-carlo-simulation-for-continuous-variables"><i class="fa fa-check"></i><b>8.6</b> Monte Carlo Simulation for Continuous Variables</a></li>
<li class="chapter" data-level="8.7" data-path="continuous-probabilities.html"><a href="continuous-probabilities.html#exercises-11"><i class="fa fa-check"></i><b>8.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="statistical-inference.html"><a href="statistical-inference.html"><i class="fa fa-check"></i><b>9</b> Statistical Inference</a>
<ul>
<li class="chapter" data-level="9.1" data-path="statistical-inference.html"><a href="statistical-inference.html#learning-objectives-1"><i class="fa fa-check"></i><b>9.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="9.2" data-path="statistical-inference.html"><a href="statistical-inference.html#expected-value"><i class="fa fa-check"></i><b>9.2</b> Expected Value</a></li>
<li class="chapter" data-level="9.3" data-path="statistical-inference.html"><a href="statistical-inference.html#central-limit-theorem"><i class="fa fa-check"></i><b>9.3</b> Central Limit Theorem</a></li>
<li class="chapter" data-level="9.4" data-path="statistical-inference.html"><a href="statistical-inference.html#key-takeaways-3"><i class="fa fa-check"></i><b>9.4</b> Key Takeaways</a></li>
<li class="chapter" data-level="9.5" data-path="statistical-inference.html"><a href="statistical-inference.html#exercises-12"><i class="fa fa-check"></i><b>9.5</b> Exercises</a></li>
<li class="chapter" data-level="9.6" data-path="statistical-inference.html"><a href="statistical-inference.html#parameter-estimation-method"><i class="fa fa-check"></i><b>9.6</b> Parameter Estimation Method</a>
<ul>
<li class="chapter" data-level="9.6.1" data-path="statistical-inference.html"><a href="statistical-inference.html#margin-of-error"><i class="fa fa-check"></i><b>9.6.1</b> Margin of Error</a></li>
<li class="chapter" data-level="9.6.2" data-path="statistical-inference.html"><a href="statistical-inference.html#confidence-intervals"><i class="fa fa-check"></i><b>9.6.2</b> Confidence Intervals</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="statistical-inference.html"><a href="statistical-inference.html#spread-estimation"><i class="fa fa-check"></i><b>9.7</b> Spread Estimation</a></li>
<li class="chapter" data-level="9.8" data-path="statistical-inference.html"><a href="statistical-inference.html#estimates-outside-election-polls"><i class="fa fa-check"></i><b>9.8</b> Estimates Outside Election Polls</a>
<ul>
<li class="chapter" data-level="9.8.1" data-path="statistical-inference.html"><a href="statistical-inference.html#example-estimating-average-height"><i class="fa fa-check"></i><b>9.8.1</b> Example: Estimating Average Height</a></li>
</ul></li>
<li class="chapter" data-level="9.9" data-path="statistical-inference.html"><a href="statistical-inference.html#exercises-13"><i class="fa fa-check"></i><b>9.9</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>IV Data Wrangling</b></span></li>
<li class="chapter" data-level="" data-path="introduction-1.html"><a href="introduction-1.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="10" data-path="data-import-and-consolidation.html"><a href="data-import-and-consolidation.html"><i class="fa fa-check"></i><b>10</b> Data import and consolidation</a>
<ul>
<li class="chapter" data-level="10.1" data-path="data-import-and-consolidation.html"><a href="data-import-and-consolidation.html#importing-from-files"><i class="fa fa-check"></i><b>10.1</b> Importing from files</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="data-import-and-consolidation.html"><a href="data-import-and-consolidation.html#working-directory"><i class="fa fa-check"></i><b>10.1.1</b> Working Directory</a></li>
<li class="chapter" data-level="10.1.2" data-path="data-import-and-consolidation.html"><a href="data-import-and-consolidation.html#readr-and-readxl-packages"><i class="fa fa-check"></i><b>10.1.2</b> readr and readxl packages</a></li>
<li class="chapter" data-level="10.1.3" data-path="data-import-and-consolidation.html"><a href="data-import-and-consolidation.html#importing-files-from-the-internet"><i class="fa fa-check"></i><b>10.1.3</b> Importing files from the internet</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="data-import-and-consolidation.html"><a href="data-import-and-consolidation.html#tidy-data"><i class="fa fa-check"></i><b>10.2</b> Tidy data</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="data-import-and-consolidation.html"><a href="data-import-and-consolidation.html#transforming-to-tidy-data"><i class="fa fa-check"></i><b>10.2.1</b> Transforming to tidy data</a></li>
<li class="chapter" data-level="10.2.2" data-path="data-import-and-consolidation.html"><a href="data-import-and-consolidation.html#separate-function"><i class="fa fa-check"></i><b>10.2.2</b> separate function</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="data-import-and-consolidation.html"><a href="data-import-and-consolidation.html#exercises-14"><i class="fa fa-check"></i><b>10.3</b> Exercises</a></li>
<li class="chapter" data-level="10.4" data-path="data-import-and-consolidation.html"><a href="data-import-and-consolidation.html#joining-tables"><i class="fa fa-check"></i><b>10.4</b> Joining tables</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="data-import-and-consolidation.html"><a href="data-import-and-consolidation.html#join-functions"><i class="fa fa-check"></i><b>10.4.1</b> Join functions</a></li>
<li class="chapter" data-level="10.4.2" data-path="data-import-and-consolidation.html"><a href="data-import-and-consolidation.html#joining-without-a-common-identifier"><i class="fa fa-check"></i><b>10.4.2</b> Joining without a common identifier</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="data-import-and-consolidation.html"><a href="data-import-and-consolidation.html#web-scraping"><i class="fa fa-check"></i><b>10.5</b> Web Scraping</a></li>
<li class="chapter" data-level="10.6" data-path="data-import-and-consolidation.html"><a href="data-import-and-consolidation.html#exercises-15"><i class="fa fa-check"></i><b>10.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>V Machine learning</b></span></li>
<li class="chapter" data-level="" data-path="introduction-2.html"><a href="introduction-2.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="10.7" data-path="introduction-2.html"><a href="introduction-2.html#learning-objectives-2"><i class="fa fa-check"></i><b>10.7</b> Learning Objectives</a></li>
<li class="chapter" data-level="10.8" data-path="introduction-2.html"><a href="introduction-2.html#chapter-structure"><i class="fa fa-check"></i><b>10.8</b> Chapter Structure</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="supervised-learning.html"><a href="supervised-learning.html"><i class="fa fa-check"></i><b>11</b> Supervised Learning</a>
<ul>
<li class="chapter" data-level="11.1" data-path="supervised-learning.html"><a href="supervised-learning.html#classification-and-regression"><i class="fa fa-check"></i><b>11.1</b> Classification and Regression</a></li>
<li class="chapter" data-level="11.2" data-path="supervised-learning.html"><a href="supervised-learning.html#knn-k-nearest-neighbors"><i class="fa fa-check"></i><b>11.2</b> kNN: k-Nearest Neighbors</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="supervised-learning.html"><a href="supervised-learning.html#two-variables-as-input"><i class="fa fa-check"></i><b>11.2.1</b> Two variables as input</a></li>
<li class="chapter" data-level="11.2.2" data-path="supervised-learning.html"><a href="supervised-learning.html#multiple-variables-as-input"><i class="fa fa-check"></i><b>11.2.2</b> Multiple variables as input</a></li>
<li class="chapter" data-level="11.2.3" data-path="supervised-learning.html"><a href="supervised-learning.html#diverse-values-of-k"><i class="fa fa-check"></i><b>11.2.3</b> Diverse values of k</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="supervised-learning.html"><a href="supervised-learning.html#tidymodels-framework"><i class="fa fa-check"></i><b>11.3</b> tidymodels Framework</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="supervised-learning.html"><a href="supervised-learning.html#creation-of-training-and-test-data"><i class="fa fa-check"></i><b>11.3.1</b> Creation of training and test data</a></li>
<li class="chapter" data-level="11.3.2" data-path="supervised-learning.html"><a href="supervised-learning.html#training-our-prediction-algorithm"><i class="fa fa-check"></i><b>11.3.2</b> Training our prediction algorithm</a></li>
<li class="chapter" data-level="11.3.3" data-path="supervised-learning.html"><a href="supervised-learning.html#data-pre-processing-with-recipes"><i class="fa fa-check"></i><b>11.3.3</b> Data Pre-processing with Recipes</a></li>
<li class="chapter" data-level="11.3.4" data-path="supervised-learning.html"><a href="supervised-learning.html#creating-a-workflow"><i class="fa fa-check"></i><b>11.3.4</b> Creating a Workflow</a></li>
<li class="chapter" data-level="11.3.5" data-path="supervised-learning.html"><a href="supervised-learning.html#parameter-tuning-with-cross-validation"><i class="fa fa-check"></i><b>11.3.5</b> Parameter Tuning with Cross-Validation</a></li>
<li class="chapter" data-level="11.3.6" data-path="supervised-learning.html"><a href="supervised-learning.html#finalizing-the-model"><i class="fa fa-check"></i><b>11.3.6</b> Finalizing the Model</a></li>
<li class="chapter" data-level="11.3.7" data-path="supervised-learning.html"><a href="supervised-learning.html#testing-the-prediction-model"><i class="fa fa-check"></i><b>11.3.7</b> Testing the prediction model</a></li>
<li class="chapter" data-level="11.3.8" data-path="supervised-learning.html"><a href="supervised-learning.html#model-evaluation-with-yardstick"><i class="fa fa-check"></i><b>11.3.8</b> Model Evaluation with yardstick</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="supervised-learning.html"><a href="supervised-learning.html#confusion-matrix"><i class="fa fa-check"></i><b>11.4</b> Confusion Matrix</a>
<ul>
<li class="chapter" data-level="11.4.1" data-path="supervised-learning.html"><a href="supervised-learning.html#accuracy"><i class="fa fa-check"></i><b>11.4.1</b> Accuracy</a></li>
<li class="chapter" data-level="11.4.2" data-path="supervised-learning.html"><a href="supervised-learning.html#sensitivity"><i class="fa fa-check"></i><b>11.4.2</b> Sensitivity</a></li>
<li class="chapter" data-level="11.4.3" data-path="supervised-learning.html"><a href="supervised-learning.html#specificity"><i class="fa fa-check"></i><b>11.4.3</b> Specificity</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="supervised-learning.html"><a href="supervised-learning.html#exercises-16"><i class="fa fa-check"></i><b>11.5</b> Exercises</a></li>
<li class="chapter" data-level="11.6" data-path="supervised-learning.html"><a href="supervised-learning.html#simple-linear-regression"><i class="fa fa-check"></i><b>11.6</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="11.7" data-path="supervised-learning.html"><a href="supervised-learning.html#multiple-linear-regression"><i class="fa fa-check"></i><b>11.7</b> Multiple Linear Regression</a></li>
<li class="chapter" data-level="11.8" data-path="supervised-learning.html"><a href="supervised-learning.html#standard-method-for-evaluating-accuracy"><i class="fa fa-check"></i><b>11.8</b> Standard Method for Evaluating Accuracy</a></li>
<li class="chapter" data-level="11.9" data-path="supervised-learning.html"><a href="supervised-learning.html#selection-of-the-most-optimal-model"><i class="fa fa-check"></i><b>11.9</b> Selection of the Most Optimal Model</a>
<ul>
<li class="chapter" data-level="11.9.1" data-path="supervised-learning.html"><a href="supervised-learning.html#k-nearest-neighbors-model"><i class="fa fa-check"></i><b>11.9.1</b> k-Nearest Neighbors Model</a></li>
<li class="chapter" data-level="11.9.2" data-path="supervised-learning.html"><a href="supervised-learning.html#generalized-linear-model---glm"><i class="fa fa-check"></i><b>11.9.2</b> Generalized Linear Model - GLM</a></li>
<li class="chapter" data-level="11.9.3" data-path="supervised-learning.html"><a href="supervised-learning.html#random-forest-model"><i class="fa fa-check"></i><b>11.9.3</b> Random Forest Model</a></li>
<li class="chapter" data-level="11.9.4" data-path="supervised-learning.html"><a href="supervised-learning.html#support-vector-machine-model---svm"><i class="fa fa-check"></i><b>11.9.4</b> Support Vector Machine Model - SVM</a></li>
<li class="chapter" data-level="11.9.5" data-path="supervised-learning.html"><a href="supervised-learning.html#naive-bayes-model"><i class="fa fa-check"></i><b>11.9.5</b> Naive Bayes Model</a></li>
<li class="chapter" data-level="11.9.6" data-path="supervised-learning.html"><a href="supervised-learning.html#model-comparison"><i class="fa fa-check"></i><b>11.9.6</b> Model Comparison</a></li>
<li class="chapter" data-level="11.9.7" data-path="supervised-learning.html"><a href="supervised-learning.html#predicting-using-the-best-model"><i class="fa fa-check"></i><b>11.9.7</b> Predicting using the best model</a></li>
</ul></li>
<li class="chapter" data-level="11.10" data-path="supervised-learning.html"><a href="supervised-learning.html#exercises-17"><i class="fa fa-check"></i><b>11.10</b> Exercises</a></li>
<li class="chapter" data-level="11.11" data-path="supervised-learning.html"><a href="supervised-learning.html#ethics-bias-in-algorithmic-decision-making"><i class="fa fa-check"></i><b>11.11</b> Ethics: Bias in Algorithmic Decision Making</a>
<ul>
<li class="chapter" data-level="11.11.1" data-path="supervised-learning.html"><a href="supervised-learning.html#the-risk-of-proxy-variables"><i class="fa fa-check"></i><b>11.11.1</b> The Risk of Proxy Variables</a></li>
<li class="chapter" data-level="11.11.2" data-path="supervised-learning.html"><a href="supervised-learning.html#feedback-loops"><i class="fa fa-check"></i><b>11.11.2</b> Feedback Loops</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html"><i class="fa fa-check"></i><b>12</b> Unsupervised Learning</a>
<ul>
<li class="chapter" data-level="12.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#learning-objectives-3"><i class="fa fa-check"></i><b>12.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="12.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#applications-of-unsupervised-learning"><i class="fa fa-check"></i><b>12.2</b> Applications of Unsupervised Learning</a></li>
<li class="chapter" data-level="12.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#k-means-clustering"><i class="fa fa-check"></i><b>12.3</b> K-Means Clustering</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#clustering-with-k-2"><i class="fa fa-check"></i><b>12.3.1</b> Clustering with k = 2</a></li>
<li class="chapter" data-level="12.3.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#clustering-with-k-3"><i class="fa fa-check"></i><b>12.3.2</b> Clustering with k &gt;= 3</a></li>
<li class="chapter" data-level="12.3.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#determination-of-optimal-clusters"><i class="fa fa-check"></i><b>12.3.3</b> Determination of Optimal Clusters</a></li>
<li class="chapter" data-level="12.3.4" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#k-means-for-more-than-2-variables"><i class="fa fa-check"></i><b>12.3.4</b> k-means for more than 2 variables</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#hierarchical-clustering"><i class="fa fa-check"></i><b>12.4</b> Hierarchical Clustering</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#clustering-with-two-variables"><i class="fa fa-check"></i><b>12.4.1</b> Clustering with two variables</a></li>
<li class="chapter" data-level="12.4.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#determination-of-optimal-clusters-1"><i class="fa fa-check"></i><b>12.4.2</b> Determination of Optimal Clusters</a></li>
<li class="chapter" data-level="12.4.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#obtain-the-grouping"><i class="fa fa-check"></i><b>12.4.3</b> Obtain the grouping</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#dimensionality-reduction"><i class="fa fa-check"></i><b>12.5</b> Dimensionality Reduction</a></li>
<li class="chapter" data-level="12.6" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#exercises-18"><i class="fa fa-check"></i><b>12.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="string-processing-and-text-mining.html"><a href="string-processing-and-text-mining.html"><i class="fa fa-check"></i><b>13</b> String processing and text mining</a>
<ul>
<li class="chapter" data-level="13.1" data-path="string-processing-and-text-mining.html"><a href="string-processing-and-text-mining.html#basic-functions"><i class="fa fa-check"></i><b>13.1</b> Basic functions</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="string-processing-and-text-mining.html"><a href="string-processing-and-text-mining.html#replacing-characters"><i class="fa fa-check"></i><b>13.1.1</b> Replacing characters</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="string-processing-and-text-mining.html"><a href="string-processing-and-text-mining.html#regular-expressions"><i class="fa fa-check"></i><b>13.2</b> Regular expressions</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="string-processing-and-text-mining.html"><a href="string-processing-and-text-mining.html#alternation"><i class="fa fa-check"></i><b>13.2.1</b> Alternation</a></li>
<li class="chapter" data-level="13.2.2" data-path="string-processing-and-text-mining.html"><a href="string-processing-and-text-mining.html#anchoring"><i class="fa fa-check"></i><b>13.2.2</b> Anchoring</a></li>
<li class="chapter" data-level="13.2.3" data-path="string-processing-and-text-mining.html"><a href="string-processing-and-text-mining.html#repetitions"><i class="fa fa-check"></i><b>13.2.3</b> Repetitions</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="string-processing-and-text-mining.html"><a href="string-processing-and-text-mining.html#from-strings-to-dates"><i class="fa fa-check"></i><b>13.3</b> From strings to dates</a></li>
<li class="chapter" data-level="13.4" data-path="string-processing-and-text-mining.html"><a href="string-processing-and-text-mining.html#exercises-19"><i class="fa fa-check"></i><b>13.4</b> Exercises</a></li>
<li class="chapter" data-level="13.5" data-path="string-processing-and-text-mining.html"><a href="string-processing-and-text-mining.html#text-mining-using-tidy-data"><i class="fa fa-check"></i><b>13.5</b> Text Mining using Tidy Data</a>
<ul>
<li class="chapter" data-level="13.5.1" data-path="string-processing-and-text-mining.html"><a href="string-processing-and-text-mining.html#importing-data-and-tokenization"><i class="fa fa-check"></i><b>13.5.1</b> Importing data and Tokenization</a></li>
<li class="chapter" data-level="13.5.2" data-path="string-processing-and-text-mining.html"><a href="string-processing-and-text-mining.html#text-cleaning-and-tokenization"><i class="fa fa-check"></i><b>13.5.2</b> Text cleaning and Tokenization</a></li>
<li class="chapter" data-level="13.5.3" data-path="string-processing-and-text-mining.html"><a href="string-processing-and-text-mining.html#word-cloud"><i class="fa fa-check"></i><b>13.5.3</b> Word Cloud</a></li>
<li class="chapter" data-level="13.5.4" data-path="string-processing-and-text-mining.html"><a href="string-processing-and-text-mining.html#word-frequency-plot"><i class="fa fa-check"></i><b>13.5.4</b> Word Frequency Plot</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="string-processing-and-text-mining.html"><a href="string-processing-and-text-mining.html#sentiment-analysis"><i class="fa fa-check"></i><b>13.6</b> Sentiment Analysis</a></li>
<li class="chapter" data-level="13.7" data-path="string-processing-and-text-mining.html"><a href="string-processing-and-text-mining.html#exercises-20"><i class="fa fa-check"></i><b>13.7</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>VI Generative AI</b></span></li>
<li class="chapter" data-level="14" data-path="genai-intro.html"><a href="genai-intro.html"><i class="fa fa-check"></i><b>14</b> Data Science in the Age of AI</a>
<ul>
<li class="chapter" data-level="14.1" data-path="genai-intro.html"><a href="genai-intro.html#what-is-a-large-language-model"><i class="fa fa-check"></i><b>14.1</b> What is a Large Language Model?</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="genai-intro.html"><a href="genai-intro.html#its-all-about-probability"><i class="fa fa-check"></i><b>14.1.1</b> It’s all about Probability</a></li>
<li class="chapter" data-level="14.1.2" data-path="genai-intro.html"><a href="genai-intro.html#tokens-vs.-words"><i class="fa fa-check"></i><b>14.1.2</b> Tokens vs. Words</a></li>
<li class="chapter" data-level="14.1.3" data-path="genai-intro.html"><a href="genai-intro.html#temperature-controlling-creativity"><i class="fa fa-check"></i><b>14.1.3</b> Temperature: Controlling Creativity</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="genai-intro.html"><a href="genai-intro.html#setting-up-your-ai-environment"><i class="fa fa-check"></i><b>14.2</b> Setting Up Your AI Environment</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="genai-intro.html"><a href="genai-intro.html#the-solution-local-llms"><i class="fa fa-check"></i><b>14.2.1</b> The Solution: Local LLMs</a></li>
<li class="chapter" data-level="14.2.2" data-path="genai-intro.html"><a href="genai-intro.html#the-.renviron-file"><i class="fa fa-check"></i><b>14.2.2</b> The <code>.Renviron</code> File</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="genai-intro.html"><a href="genai-intro.html#ai-as-the-pair-programmer"><i class="fa fa-check"></i><b>14.3</b> AI as the “Pair Programmer”</a>
<ul>
<li class="chapter" data-level="14.3.1" data-path="genai-intro.html"><a href="genai-intro.html#the-great-refactorer"><i class="fa fa-check"></i><b>14.3.1</b> The Great Refactorer</a></li>
<li class="chapter" data-level="14.3.2" data-path="genai-intro.html"><a href="genai-intro.html#the-translator"><i class="fa fa-check"></i><b>14.3.2</b> The Translator</a></li>
<li class="chapter" data-level="14.3.3" data-path="genai-intro.html"><a href="genai-intro.html#pro-tip-prompt-engineering-101"><i class="fa fa-check"></i><b>14.3.3</b> Pro Tip: Prompt Engineering 101</a></li>
<li class="chapter" data-level="14.3.4" data-path="genai-intro.html"><a href="genai-intro.html#the-regex-master"><i class="fa fa-check"></i><b>14.3.4</b> The Regex Master</a></li>
<li class="chapter" data-level="14.3.5" data-path="genai-intro.html"><a href="genai-intro.html#the-error-decoder"><i class="fa fa-check"></i><b>14.3.5</b> The Error Decoder</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="genai-intro.html"><a href="genai-intro.html#the-risks-hallucinations"><i class="fa fa-check"></i><b>14.4</b> The Risks: Hallucinations</a>
<ul>
<li class="chapter" data-level="14.4.1" data-path="genai-intro.html"><a href="genai-intro.html#the-package-hallucination"><i class="fa fa-check"></i><b>14.4.1</b> The “Package” Hallucination</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="genai-intro.html"><a href="genai-intro.html#building-a-robust-request"><i class="fa fa-check"></i><b>14.5</b> Building a Robust Request</a></li>
<li class="chapter" data-level="14.6" data-path="genai-intro.html"><a href="genai-intro.html#the-holy-grail-structured-data-extraction"><i class="fa fa-check"></i><b>14.6</b> The Holy Grail: Structured Data extraction</a>
<ul>
<li class="chapter" data-level="14.6.1" data-path="genai-intro.html"><a href="genai-intro.html#forcing-json-output"><i class="fa fa-check"></i><b>14.6.1</b> Forcing JSON Output</a></li>
</ul></li>
<li class="chapter" data-level="14.7" data-path="genai-intro.html"><a href="genai-intro.html#batch-processing-the-purrr-workflow"><i class="fa fa-check"></i><b>14.7</b> Batch Processing: The <code>purrr</code> Workflow</a></li>
<li class="chapter" data-level="14.8" data-path="genai-intro.html"><a href="genai-intro.html#summary"><i class="fa fa-check"></i><b>14.8</b> Summary</a></li>
<li class="chapter" data-level="14.9" data-path="genai-intro.html"><a href="genai-intro.html#beyond-generation-embeddings"><i class="fa fa-check"></i><b>14.9</b> Beyond Generation: Embeddings</a>
<ul>
<li class="chapter" data-level="14.9.1" data-path="genai-intro.html"><a href="genai-intro.html#r-implementation"><i class="fa fa-check"></i><b>14.9.1</b> R Implementation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="genai-embeddings.html"><a href="genai-embeddings.html"><i class="fa fa-check"></i><b>15</b> Text Analysis with Embeddings</a>
<ul>
<li class="chapter" data-level="15.1" data-path="genai-embeddings.html"><a href="genai-embeddings.html#beyond-bag-of-words"><i class="fa fa-check"></i><b>15.1</b> Beyond Bag-of-Words</a></li>
<li class="chapter" data-level="15.2" data-path="genai-embeddings.html"><a href="genai-embeddings.html#what-is-an-embedding"><i class="fa fa-check"></i><b>15.2</b> What is an Embedding?</a></li>
<li class="chapter" data-level="15.3" data-path="genai-embeddings.html"><a href="genai-embeddings.html#getting-embeddings-in-r"><i class="fa fa-check"></i><b>15.3</b> Getting Embeddings in R</a></li>
<li class="chapter" data-level="15.4" data-path="genai-embeddings.html"><a href="genai-embeddings.html#visualizing-meaning-dimensionality-reduction"><i class="fa fa-check"></i><b>15.4</b> Visualizing Meaning (Dimensionality Reduction)</a></li>
<li class="chapter" data-level="15.5" data-path="genai-embeddings.html"><a href="genai-embeddings.html#building-a-semantic-search-engine"><i class="fa fa-check"></i><b>15.5</b> Building a Semantic Search Engine</a></li>
<li class="chapter" data-level="15.6" data-path="genai-embeddings.html"><a href="genai-embeddings.html#summary-the-ai-workflow"><i class="fa fa-check"></i><b>15.6</b> Summary: The AI Workflow</a></li>
</ul></li>
<li class="part"><span><b>VII Real Cases</b></span></li>
<li class="chapter" data-level="" data-path="introduction-3.html"><a href="introduction-3.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="16" data-path="case-study-real-estate-market-analysis.html"><a href="case-study-real-estate-market-analysis.html"><i class="fa fa-check"></i><b>16</b> Case Study: Real Estate Market Analysis</a>
<ul>
<li class="chapter" data-level="16.1" data-path="case-study-real-estate-market-analysis.html"><a href="case-study-real-estate-market-analysis.html#objectives"><i class="fa fa-check"></i><b>16.1</b> Objectives</a></li>
<li class="chapter" data-level="16.2" data-path="case-study-real-estate-market-analysis.html"><a href="case-study-real-estate-market-analysis.html#loading-libraries"><i class="fa fa-check"></i><b>16.2</b> Loading Libraries</a></li>
<li class="chapter" data-level="16.3" data-path="case-study-real-estate-market-analysis.html"><a href="case-study-real-estate-market-analysis.html#exploring-the-data"><i class="fa fa-check"></i><b>16.3</b> Exploring the Data</a></li>
<li class="chapter" data-level="16.4" data-path="case-study-real-estate-market-analysis.html"><a href="case-study-real-estate-market-analysis.html#data-cleaning"><i class="fa fa-check"></i><b>16.4</b> Data Cleaning</a></li>
<li class="chapter" data-level="16.5" data-path="case-study-real-estate-market-analysis.html"><a href="case-study-real-estate-market-analysis.html#exploratory-analysis"><i class="fa fa-check"></i><b>16.5</b> Exploratory Analysis</a>
<ul>
<li class="chapter" data-level="16.5.1" data-path="case-study-real-estate-market-analysis.html"><a href="case-study-real-estate-market-analysis.html#market-volume-over-time"><i class="fa fa-check"></i><b>16.5.1</b> Market Volume Over Time</a></li>
<li class="chapter" data-level="16.5.2" data-path="case-study-real-estate-market-analysis.html"><a href="case-study-real-estate-market-analysis.html#comparing-cities"><i class="fa fa-check"></i><b>16.5.2</b> Comparing Cities</a></li>
</ul></li>
<li class="chapter" data-level="16.6" data-path="case-study-real-estate-market-analysis.html"><a href="case-study-real-estate-market-analysis.html#creating-indicators"><i class="fa fa-check"></i><b>16.6</b> Creating Indicators</a></li>
<li class="chapter" data-level="16.7" data-path="case-study-real-estate-market-analysis.html"><a href="case-study-real-estate-market-analysis.html#try-it-yourself"><i class="fa fa-check"></i><b>16.7</b> Try It Yourself</a></li>
<li class="chapter" data-level="16.8" data-path="case-study-real-estate-market-analysis.html"><a href="case-study-real-estate-market-analysis.html#conclusions"><i class="fa fa-check"></i><b>16.8</b> Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="google-analytics-from-r.html"><a href="google-analytics-from-r.html"><i class="fa fa-check"></i><b>17</b> Google Analytics from R</a>
<ul>
<li class="chapter" data-level="17.1" data-path="google-analytics-from-r.html"><a href="google-analytics-from-r.html#problem"><i class="fa fa-check"></i><b>17.1</b> Problem</a></li>
<li class="chapter" data-level="17.2" data-path="google-analytics-from-r.html"><a href="google-analytics-from-r.html#access-to-data"><i class="fa fa-check"></i><b>17.2</b> Access to data</a></li>
<li class="chapter" data-level="17.3" data-path="google-analytics-from-r.html"><a href="google-analytics-from-r.html#visualization"><i class="fa fa-check"></i><b>17.3</b> Visualization</a></li>
<li class="chapter" data-level="17.4" data-path="google-analytics-from-r.html"><a href="google-analytics-from-r.html#conclusion"><i class="fa fa-check"></i><b>17.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="ethics-checklist.html"><a href="ethics-checklist.html"><i class="fa fa-check"></i><b>18</b> Appendix A: Responsible AI Checklist</a>
<ul>
<li class="chapter" data-level="18.1" data-path="ethics-checklist.html"><a href="ethics-checklist.html#data-quality-lineage"><i class="fa fa-check"></i><b>18.1</b> Data Quality &amp; Lineage</a></li>
<li class="chapter" data-level="18.2" data-path="ethics-checklist.html"><a href="ethics-checklist.html#fairness-bias"><i class="fa fa-check"></i><b>18.2</b> Fairness &amp; Bias</a></li>
<li class="chapter" data-level="18.3" data-path="ethics-checklist.html"><a href="ethics-checklist.html#transparency-explainability"><i class="fa fa-check"></i><b>18.3</b> Transparency &amp; Explainability</a></li>
<li class="chapter" data-level="18.4" data-path="ethics-checklist.html"><a href="ethics-checklist.html#reproducibility-integrity"><i class="fa fa-check"></i><b>18.4</b> Reproducibility &amp; Integrity</a></li>
<li class="chapter" data-level="18.5" data-path="ethics-checklist.html"><a href="ethics-checklist.html#genai-specifics"><i class="fa fa-check"></i><b>18.5</b> GenAI Specifics</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="r6-intro.html"><a href="r6-intro.html"><i class="fa fa-check"></i><b>19</b> Appendix B: Object Oriented Programming with R6</a>
<ul>
<li class="chapter" data-level="19.1" data-path="r6-intro.html"><a href="r6-intro.html#the-r6-package-classes-methods-encapsulation-and-inheritance"><i class="fa fa-check"></i><b>19.1</b> The R6 package: Classes, methods, encapsulation, and inheritance</a></li>
<li class="chapter" data-level="19.2" data-path="r6-intro.html"><a href="r6-intro.html#exercises-21"><i class="fa fa-check"></i><b>19.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Science with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="statistical-inference" class="section level1 hasAnchor" number="9">
<h1><span class="header-section-number">Chapter 9</span> Statistical Inference<a href="statistical-inference.html#statistical-inference" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>To infer means to draw a conclusion from general or particular facts. Statistical inference is a set of methods and techniques that allow deducing characteristics of a population using data from a random sample. The method we are going to use most to infer is the parameter estimation method.</p>
<p>We estimate parameters of a population from a sample because very rarely we will be able to have access to all the data of the population. Such is the case of election polls, disease studies, etc.</p>
<div id="learning-objectives-1" class="section level2 hasAnchor callout-note" number="9.1">
<h2><span class="header-section-number">9.1</span> Learning Objectives<a href="statistical-inference.html#learning-objectives-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>After completing this chapter, you will be able to:</p>
<p>In this chapter, we will build the foundation for making valid conclusions from data. We will start by mastering the calculation of expected values and standard errors, which are critical for characterizing random variables. Then, we will explore the Central Limit Theorem to understand how sampling distributions behave. Finally, we will apply these concepts to estimate population parameters from sample data, calculating confidence intervals and margins of error to analyze real-world scenarios like election polling.</p>
</div>
<p>We will introduce fundamental concepts such as <strong>expected value</strong> and <strong>standard error</strong>, which will be useful to us to make inferences.</p>
<div id="expected-value" class="section level2 hasAnchor" number="9.2">
<h2><span class="header-section-number">9.2</span> Expected Value<a href="statistical-inference.html#expected-value" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s use the following case to understand this concept intuitively.</p>
<blockquote>
<p>We have been hired in a casino to analyze if it is reasonable to install a roulette with 37 values ranging from 0 to 36. The house wants to open the game with a special offer if the ball lands on 0 or 21 paying 10 to 1. This means that if a player plays and wins we pay them 10 dollars and if they lose they would pay us 1 dollar.</p>
</blockquote>
<div align="center">
<p><img src="assets/images/04-statistics/roulette-casino.png" alt="Casino roulette wheel with numbered red and black pockets" width="300pt" style="display: block; margin: auto;" /></p>
</div>
<p>With what we have learned so far we can simulate our game with the case data. We have 37 values, of which in 2 of them give a player a profit of +10 or a loss -1. Let’s also define <code>prob_win</code> as the probability that a player wins.</p>
<div class="sourceCode" id="cb330"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb330-1"><a href="statistical-inference.html#cb330-1" tabindex="-1"></a><span class="co"># Total times played</span></span>
<span id="cb330-2"><a href="statistical-inference.html#cb330-2" tabindex="-1"></a>plays <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb330-3"><a href="statistical-inference.html#cb330-3" tabindex="-1"></a></span>
<span id="cb330-4"><a href="statistical-inference.html#cb330-4" tabindex="-1"></a><span class="co"># Probability that a player wins each time</span></span>
<span id="cb330-5"><a href="statistical-inference.html#cb330-5" tabindex="-1"></a>prob_win <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">/</span><span class="dv">37</span></span>
<span id="cb330-6"><a href="statistical-inference.html#cb330-6" tabindex="-1"></a>prob_lose <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> prob_win</span>
<span id="cb330-7"><a href="statistical-inference.html#cb330-7" tabindex="-1"></a></span>
<span id="cb330-8"><a href="statistical-inference.html#cb330-8" tabindex="-1"></a><span class="co"># Random sample</span></span>
<span id="cb330-9"><a href="statistical-inference.html#cb330-9" tabindex="-1"></a>sample_vec <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">c</span>(<span class="dv">10</span>, <span class="sc">-</span><span class="dv">1</span>), plays, <span class="at">replace =</span> <span class="cn">TRUE</span>, <span class="at">prob =</span> <span class="fu">c</span>(prob_win, prob_lose))</span>
<span id="cb330-10"><a href="statistical-inference.html#cb330-10" tabindex="-1"></a></span>
<span id="cb330-11"><a href="statistical-inference.html#cb330-11" tabindex="-1"></a>sample_vec</span>
<span id="cb330-12"><a href="statistical-inference.html#cb330-12" tabindex="-1"></a><span class="co">#&gt; [1] -1</span></span></code></pre></div>
<p>The distribution of this variable is simple given that it can only take two values: 10 or -1. When we simulate a very large number of games it can be seen how it is distributed according to the indicated probability of winning and losing.</p>
<div class="sourceCode" id="cb331"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb331-1"><a href="statistical-inference.html#cb331-1" tabindex="-1"></a>plays <span class="ot">&lt;-</span> <span class="dv">100000</span></span>
<span id="cb331-2"><a href="statistical-inference.html#cb331-2" tabindex="-1"></a></span>
<span id="cb331-3"><a href="statistical-inference.html#cb331-3" tabindex="-1"></a>prob_win <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">/</span><span class="dv">37</span></span>
<span id="cb331-4"><a href="statistical-inference.html#cb331-4" tabindex="-1"></a>prob_lose <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> prob_win</span>
<span id="cb331-5"><a href="statistical-inference.html#cb331-5" tabindex="-1"></a></span>
<span id="cb331-6"><a href="statistical-inference.html#cb331-6" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">outcome =</span> sample_vec) <span class="sc">|&gt;</span></span>
<span id="cb331-7"><a href="statistical-inference.html#cb331-7" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">factor</span>(outcome))) <span class="sc">+</span></span>
<span id="cb331-8"><a href="statistical-inference.html#cb331-8" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">fill =</span> <span class="st">&quot;steelblue&quot;</span>) <span class="sc">+</span></span>
<span id="cb331-9"><a href="statistical-inference.html#cb331-9" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Outcome ($)&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Count&quot;</span>, <span class="at">title =</span> <span class="st">&quot;Distribution of Roulette Outcomes&quot;</span>)</span></code></pre></div>
<p><img src="Data-Science-with-R_files/figure-html/unnamed-chunk-438-1.png" alt="" width="80%" style="display: block; margin: auto;" /></p>
<p>We have been using Monte Carlo simulation to estimate what the <strong>mean</strong> of the game results would be in real life.</p>
<div class="sourceCode" id="cb332"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb332-1"><a href="statistical-inference.html#cb332-1" tabindex="-1"></a><span class="co"># Estimation of the mean by Monte Carlo simulation</span></span>
<span id="cb332-2"><a href="statistical-inference.html#cb332-2" tabindex="-1"></a><span class="fu">mean</span>(sample_vec)</span>
<span id="cb332-3"><a href="statistical-inference.html#cb332-3" tabindex="-1"></a><span class="co">#&gt; [1] -1</span></span></code></pre></div>
<p>In addition, we have seen that, the more the sample grows, our <strong>mean</strong> in the Monte Carlo simulation converges to a value, in this case the probability of winning mainly in the roulette. That value to which it converges we will call <strong>expected value</strong>, which as its name indicates will be the value we expect to obtain in reality. The more the sample size grows the more our sample mean converges to this expected value. The notation we will use will be <span class="math inline">\(E[X]\)</span>.</p>
<p>When there are only two possible results <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> with proportions <span class="math inline">\(p\)</span> and <span class="math inline">\(1-p\)</span> respectively, the expected value will be calculated using this formula:</p>
<p><span class="math inline">\(E[X] = ap + b(1-p)\)</span></p>
<div class="sourceCode" id="cb333"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb333-1"><a href="statistical-inference.html#cb333-1" tabindex="-1"></a><span class="co"># Expected Value:</span></span>
<span id="cb333-2"><a href="statistical-inference.html#cb333-2" tabindex="-1"></a>(<span class="dv">10</span>) <span class="sc">*</span> prob_win <span class="sc">+</span> (<span class="sc">-</span><span class="dv">1</span>) <span class="sc">*</span> prob_lose</span>
<span id="cb333-3"><a href="statistical-inference.html#cb333-3" tabindex="-1"></a><span class="co">#&gt; [1] -0.4054054</span></span></code></pre></div>
<p>Previously we had calculated the mean using Monte Carlo simulation. If we compare it with the expected value we see how both numbers are approximately the same, as the theory predicts.</p>
<!--
Likewise, we can calculate the standard deviation which we will call **standard error**. The mathematical notation is $SE[X]$ and we will calculate it using the following formula:

$SE[X] = |a-b|\sqrt{p(1-p)}$


``` r
# Standard error:
abs(10 - -1)*sqrt(prob_win*prob_lose)
#> [1] 2.487368
```

-->
<p>Returning to the simulation of roulette games, a single person does not play so many times. Each person plays about 40 times a day at roulette. Thus, we can generate 40 games that a random player could play and find how much he would win:</p>
<div class="sourceCode" id="cb334"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb334-1"><a href="statistical-inference.html#cb334-1" tabindex="-1"></a>plays <span class="ot">&lt;-</span> <span class="dv">40</span></span>
<span id="cb334-2"><a href="statistical-inference.html#cb334-2" tabindex="-1"></a></span>
<span id="cb334-3"><a href="statistical-inference.html#cb334-3" tabindex="-1"></a>prob_win <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">/</span><span class="dv">37</span></span>
<span id="cb334-4"><a href="statistical-inference.html#cb334-4" tabindex="-1"></a>prob_lose <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> prob_win</span>
<span id="cb334-5"><a href="statistical-inference.html#cb334-5" tabindex="-1"></a></span>
<span id="cb334-6"><a href="statistical-inference.html#cb334-6" tabindex="-1"></a>sample_vec <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">c</span>(<span class="dv">10</span>, <span class="sc">-</span><span class="dv">1</span>), plays, <span class="at">replace =</span> <span class="cn">TRUE</span>, <span class="at">prob =</span> <span class="fu">c</span>(prob_win, prob_lose))</span>
<span id="cb334-7"><a href="statistical-inference.html#cb334-7" tabindex="-1"></a></span>
<span id="cb334-8"><a href="statistical-inference.html#cb334-8" tabindex="-1"></a><span class="fu">sum</span>(sample_vec)</span>
<span id="cb334-9"><a href="statistical-inference.html#cb334-9" tabindex="-1"></a><span class="co">#&gt; [1] -18</span></span></code></pre></div>
<p>Finally, not only one person will play. Let’s replicate this sample about 100,000 times to simulate the number of players we would have in a quarter.</p>
<div class="sourceCode" id="cb335"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb335-1"><a href="statistical-inference.html#cb335-1" tabindex="-1"></a></span>
<span id="cb335-2"><a href="statistical-inference.html#cb335-2" tabindex="-1"></a>players <span class="ot">&lt;-</span> <span class="dv">100000</span></span>
<span id="cb335-3"><a href="statistical-inference.html#cb335-3" tabindex="-1"></a>plays <span class="ot">&lt;-</span> <span class="dv">40</span></span>
<span id="cb335-4"><a href="statistical-inference.html#cb335-4" tabindex="-1"></a></span>
<span id="cb335-5"><a href="statistical-inference.html#cb335-5" tabindex="-1"></a>prob_win <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">/</span><span class="dv">37</span></span>
<span id="cb335-6"><a href="statistical-inference.html#cb335-6" tabindex="-1"></a>prob_lose <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> prob_win</span>
<span id="cb335-7"><a href="statistical-inference.html#cb335-7" tabindex="-1"></a></span>
<span id="cb335-8"><a href="statistical-inference.html#cb335-8" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2025</span>)</span>
<span id="cb335-9"><a href="statistical-inference.html#cb335-9" tabindex="-1"></a>winnings_simulation <span class="ot">&lt;-</span> <span class="fu">replicate</span>(players, {</span>
<span id="cb335-10"><a href="statistical-inference.html#cb335-10" tabindex="-1"></a>  </span>
<span id="cb335-11"><a href="statistical-inference.html#cb335-11" tabindex="-1"></a>  sample_vec <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">c</span>(<span class="dv">10</span>, <span class="sc">-</span><span class="dv">1</span>), plays, <span class="at">replace =</span> <span class="cn">TRUE</span>, <span class="at">prob =</span> <span class="fu">c</span>(prob_win, prob_lose))</span>
<span id="cb335-12"><a href="statistical-inference.html#cb335-12" tabindex="-1"></a></span>
<span id="cb335-13"><a href="statistical-inference.html#cb335-13" tabindex="-1"></a>  <span class="fu">sum</span>(sample_vec)  </span>
<span id="cb335-14"><a href="statistical-inference.html#cb335-14" tabindex="-1"></a>  </span>
<span id="cb335-15"><a href="statistical-inference.html#cb335-15" tabindex="-1"></a>})</span></code></pre></div>
<p>So far we have done the same as we have learned in previous chapters. However, we could also see how the players’ winnings are distributed. And for that it is enough to create a histogram of the result.</p>
<div class="sourceCode" id="cb336"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb336-1"><a href="statistical-inference.html#cb336-1" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">winnings =</span> winnings_simulation) <span class="sc">|&gt;</span></span>
<span id="cb336-2"><a href="statistical-inference.html#cb336-2" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> winnings)) <span class="sc">+</span></span>
<span id="cb336-3"><a href="statistical-inference.html#cb336-3" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">bins =</span> <span class="dv">30</span>, <span class="at">fill =</span> <span class="st">&quot;steelblue&quot;</span>, <span class="at">color =</span> <span class="st">&quot;white&quot;</span>) <span class="sc">+</span></span>
<span id="cb336-4"><a href="statistical-inference.html#cb336-4" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Total Winnings ($)&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Count&quot;</span>, <span class="at">title =</span> <span class="st">&quot;Distribution of Player Winnings&quot;</span>)</span></code></pre></div>
<p><img src="Data-Science-with-R_files/figure-html/unnamed-chunk-444-1.png" alt="" width="80%" style="display: block; margin: auto;" /></p>
<p>It is not a coincidence that if we create a histogram with all the winnings of all the players the result looks like a normal distribution. In fact, that was the main approach that George Pólya made in 1920 when he presented his <strong>Central Limit Theorem</strong>.</p>
</div>
<div id="central-limit-theorem" class="section level2 hasAnchor" number="9.3">
<h2><span class="header-section-number">9.3</span> Central Limit Theorem<a href="statistical-inference.html#central-limit-theorem" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The Central Limit Theorem tells us that if we take several samples of the same size <span class="math inline">\(n\)</span> and in each sample we sum the values within each sample we will obtain a value <span class="math inline">\(S\)</span> (the sum) then we will find that its distribution approximates well to a normal curve.</p>
<p>If we replicate this language to our example it would be: The central limit theorem tells us that if we take samples of 40 games for each player and then calculate for each player the total they won, then we will find that the distribution of the amount won by many players approximates a normal distribution.</p>
<p>Since it is a new distribution, we can calculate its mean and standard deviation. Being samples we will use the learned term <strong>expected value</strong> of the sum to refer to the sample mean and we will add the term of <strong>standard error</strong> of the sum to refer to the sample standard deviation</p>
<p>This would be the formula to calculate the expected value of the sum:</p>
<p><span class="math inline">\(E[S_n] = n (ap+b(1-p))\)</span></p>
<div class="sourceCode" id="cb337"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb337-1"><a href="statistical-inference.html#cb337-1" tabindex="-1"></a>plays <span class="ot">&lt;-</span> <span class="dv">40</span></span>
<span id="cb337-2"><a href="statistical-inference.html#cb337-2" tabindex="-1"></a></span>
<span id="cb337-3"><a href="statistical-inference.html#cb337-3" tabindex="-1"></a>prob_win <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">/</span><span class="dv">37</span></span>
<span id="cb337-4"><a href="statistical-inference.html#cb337-4" tabindex="-1"></a>prob_lose <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> prob_win</span>
<span id="cb337-5"><a href="statistical-inference.html#cb337-5" tabindex="-1"></a></span>
<span id="cb337-6"><a href="statistical-inference.html#cb337-6" tabindex="-1"></a><span class="co"># Expected value of the sum</span></span>
<span id="cb337-7"><a href="statistical-inference.html#cb337-7" tabindex="-1"></a>E_sum <span class="ot">&lt;-</span> plays <span class="sc">*</span> ( (<span class="dv">10</span>)<span class="sc">*</span>prob_win <span class="sc">+</span> (<span class="sc">-</span><span class="dv">1</span>)<span class="sc">*</span>prob_lose )</span>
<span id="cb337-8"><a href="statistical-inference.html#cb337-8" tabindex="-1"></a>E_sum</span>
<span id="cb337-9"><a href="statistical-inference.html#cb337-9" tabindex="-1"></a><span class="co">#&gt; [1] -16.21622</span></span></code></pre></div>
<p>And to calculate the standard error of the sum we will use the following formula:</p>
<p><span class="math inline">\(SE[S_n]=\sqrt{n}\ |a-b|\ \sqrt{p(1-p)}\)</span></p>
<div class="sourceCode" id="cb338"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb338-1"><a href="statistical-inference.html#cb338-1" tabindex="-1"></a>plays <span class="ot">&lt;-</span> <span class="dv">40</span></span>
<span id="cb338-2"><a href="statistical-inference.html#cb338-2" tabindex="-1"></a></span>
<span id="cb338-3"><a href="statistical-inference.html#cb338-3" tabindex="-1"></a>prob_win <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">/</span><span class="dv">37</span></span>
<span id="cb338-4"><a href="statistical-inference.html#cb338-4" tabindex="-1"></a>prob_lose <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> prob_win</span>
<span id="cb338-5"><a href="statistical-inference.html#cb338-5" tabindex="-1"></a></span>
<span id="cb338-6"><a href="statistical-inference.html#cb338-6" tabindex="-1"></a><span class="co"># Standard error of the sum</span></span>
<span id="cb338-7"><a href="statistical-inference.html#cb338-7" tabindex="-1"></a>SE_sum <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(plays) <span class="sc">*</span> <span class="fu">abs</span>(<span class="dv">10</span> <span class="sc">-</span> <span class="sc">-</span><span class="dv">1</span>) <span class="sc">*</span> <span class="fu">sqrt</span>(prob_win<span class="sc">*</span>prob_lose)</span>
<span id="cb338-8"><a href="statistical-inference.html#cb338-8" tabindex="-1"></a>SE_sum</span>
<span id="cb338-9"><a href="statistical-inference.html#cb338-9" tabindex="-1"></a><span class="co">#&gt; [1] 15.73149</span></span></code></pre></div>
<p>With these two theoretical data, the expected value and the standard error, we can graph the normal curve of the sum of winnings of our game.</p>
<p><img src="Data-Science-with-R_files/figure-html/unnamed-chunk-447-1.png" alt="" width="80%" style="display: block; margin: auto;" /></p>
<p>What does this mean? That if theoretically we can already graph the normal curve then we can also calculate the probability that the sum is greater or less than some value. This is the main advantage of the Central Limit Theorem since we can calculate probabilities of the population using this approximation and the data of a single sample.</p>
<p>For example, if we want to know what is the probability that a player wins money after playing 40 times in roulette we would have to calculate the probability that <span class="math inline">\(S\)</span> is greater than <strong>zero</strong>, represented by the blue shaded area:</p>
<p><img src="Data-Science-with-R_files/figure-html/unnamed-chunk-448-1.png" alt="" width="80%" style="display: block; margin: auto;" /></p>
<p>To perform this calculation in R we would use the <code>pnorm</code> function:</p>
<div class="sourceCode" id="cb339"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb339-1"><a href="statistical-inference.html#cb339-1" tabindex="-1"></a><span class="co"># Probability of getting more than 0 dollars having played 40 games:</span></span>
<span id="cb339-2"><a href="statistical-inference.html#cb339-2" tabindex="-1"></a><span class="dv">1</span><span class="sc">-</span> <span class="fu">pnorm</span>(<span class="dv">0</span>, E_sum, SE_sum)</span>
<span id="cb339-3"><a href="statistical-inference.html#cb339-3" tabindex="-1"></a><span class="co">#&gt; [1] 0.1513144</span></span></code></pre></div>
<p>Let’s validate that the Monte Carlo simulation approximates this theoretical value we just calculated:</p>
<div class="sourceCode" id="cb340"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb340-1"><a href="statistical-inference.html#cb340-1" tabindex="-1"></a><span class="co"># Probability of getting more than 0 dollars having played 40 games:</span></span>
<span id="cb340-2"><a href="statistical-inference.html#cb340-2" tabindex="-1"></a><span class="fu">mean</span>(winnings_simulation <span class="sc">&gt;</span> <span class="dv">0</span>)</span>
<span id="cb340-3"><a href="statistical-inference.html#cb340-3" tabindex="-1"></a><span class="co">#&gt; [1] 0.16813</span></span></code></pre></div>
<p>We have used two ways to estimate the probability, the theoretical estimation using the central limit theorem and the Monte Carlo simulation. These two numbers are quite close to the real probability. In both cases, the larger the sample, the more reasonable our estimation will be.</p>
<p>On the other hand, the same happens if we wanted to analyze the average and not the sum of the winnings. But for the average case we will use the following formulas:</p>
<ol style="list-style-type: decimal">
<li>Expected value of the average: <span class="math inline">\(E[\overline{X}]=ap+b(1-p)\)</span>.</li>
<li>Standard error of the average: <span class="math inline">\(SE[\overline{X}]=|a-b|\sqrt{\frac{p(1-p) }{n}}\)</span>.</li>
</ol>
</div>
<div id="key-takeaways-3" class="section level2 hasAnchor callout-tip" number="9.4">
<h2><span class="header-section-number">9.4</span> Key Takeaways<a href="statistical-inference.html#key-takeaways-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This chapter provided the tools to quantify uncertainty. We learned that the <strong>Expected Value</strong>, <span class="math inline">\(E[X]\)</span>, represents the long-term average outcome of a random variable. The <strong>Central Limit Theorem</strong> is the bridge that allows us to approximate the sum of independent samples using a normal distribution, regardless of the original population’s shape. This theorem enables us to calculate the <strong>Expected Value of the Sum</strong> (<span class="math inline">\(E[S_n] = n \cdot E[X]\)</span>) and the <strong>Standard Error of the Sum</strong> (<span class="math inline">\(SE[S_n] = \sqrt{n} \cdot SE[X]\)</span>), empowering us to estimate probabilities and make inferences about a population without needing to measure every single individual.</p>
</div>
<div id="exercises-12" class="section level2 hasAnchor" number="9.5">
<h2><span class="header-section-number">9.5</span> Exercises<a href="statistical-inference.html#exercises-12" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The admission exam of the National Univ. of San Marcos consists of 100 multiple choice questions (A, B, C, D, E) with a value of 20 points for each correct question and 1.125 for each wrong answer. We want to analyze what would happen if a student answers all 100 questions randomly and if there are chances of getting a vacancy knowing that minimum 900 points are needed to enter some career.</p>
<ol start="77" style="list-style-type: decimal">
<li>Consider a multiple-choice exam with 100 questions, where each correct answer awards 20 points and each wrong answer deducts 1.125 points. Determine the expected value of points a student would receive for a single question if they guessed randomly among the 5 options.</li>
</ol>
<details>
<summary type="button">
Solution
</summary>
<div class="sourceCode" id="cb341"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb341-1"><a href="statistical-inference.html#cb341-1" tabindex="-1"></a>points_correct <span class="ot">&lt;-</span> <span class="dv">20</span></span>
<span id="cb341-2"><a href="statistical-inference.html#cb341-2" tabindex="-1"></a>points_wrong <span class="ot">&lt;-</span> <span class="sc">-</span><span class="fl">1.125</span></span>
<span id="cb341-3"><a href="statistical-inference.html#cb341-3" tabindex="-1"></a>  </span>
<span id="cb341-4"><a href="statistical-inference.html#cb341-4" tabindex="-1"></a>prob_correct <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">/</span><span class="dv">5</span></span>
<span id="cb341-5"><a href="statistical-inference.html#cb341-5" tabindex="-1"></a>prob_wrong <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> prob_correct</span>
<span id="cb341-6"><a href="statistical-inference.html#cb341-6" tabindex="-1"></a></span>
<span id="cb341-7"><a href="statistical-inference.html#cb341-7" tabindex="-1"></a><span class="co"># Expected value of guessing a question:</span></span>
<span id="cb341-8"><a href="statistical-inference.html#cb341-8" tabindex="-1"></a>E <span class="ot">&lt;-</span> points_correct <span class="sc">*</span> prob_correct <span class="sc">+</span> points_wrong <span class="sc">*</span> prob_wrong</span>
<span id="cb341-9"><a href="statistical-inference.html#cb341-9" tabindex="-1"></a></span>
<span id="cb341-10"><a href="statistical-inference.html#cb341-10" tabindex="-1"></a>E</span></code></pre></div>
</details>
<ol start="78" style="list-style-type: decimal">
<li>Based on the expected value for a single question, calculate the total expected value if a student guesses on all 100 questions of the exam.</li>
</ol>
<details>
<summary type="button">
Solution
</summary>
<div class="sourceCode" id="cb342"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb342-1"><a href="statistical-inference.html#cb342-1" tabindex="-1"></a><span class="co"># Total questions:</span></span>
<span id="cb342-2"><a href="statistical-inference.html#cb342-2" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb342-3"><a href="statistical-inference.html#cb342-3" tabindex="-1"></a></span>
<span id="cb342-4"><a href="statistical-inference.html#cb342-4" tabindex="-1"></a><span class="co"># Expected value of the sum</span></span>
<span id="cb342-5"><a href="statistical-inference.html#cb342-5" tabindex="-1"></a>E_sum <span class="ot">&lt;-</span> n <span class="sc">*</span> E</span>
<span id="cb342-6"><a href="statistical-inference.html#cb342-6" tabindex="-1"></a></span>
<span id="cb342-7"><a href="statistical-inference.html#cb342-7" tabindex="-1"></a>E_sum</span></code></pre></div>
</details>
<ol start="79" style="list-style-type: decimal">
<li>Calculate the standard error associated with the total score if a student guesses on all 100 questions.</li>
</ol>
<details>
<summary type="button">
Solution
</summary>
<div class="sourceCode" id="cb343"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb343-1"><a href="statistical-inference.html#cb343-1" tabindex="-1"></a><span class="co"># Standard error of the sum</span></span>
<span id="cb343-2"><a href="statistical-inference.html#cb343-2" tabindex="-1"></a>SE_sum <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(n)<span class="sc">*</span><span class="fu">abs</span>(points_correct <span class="sc">-</span> points_wrong) <span class="sc">*</span> <span class="fu">sqrt</span>(prob_correct<span class="sc">*</span>prob_wrong)</span>
<span id="cb343-3"><a href="statistical-inference.html#cb343-3" tabindex="-1"></a></span>
<span id="cb343-4"><a href="statistical-inference.html#cb343-4" tabindex="-1"></a>SE_sum</span></code></pre></div>
</details>
<ol start="80" style="list-style-type: decimal">
<li>Using the Central Limit Theorem and the values calculated previously, determine the probability that a student guessing on all questions would achieve a score higher than 900 points.</li>
</ol>
<details>
<summary type="button">
Solution
</summary>
<div class="sourceCode" id="cb344"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb344-1"><a href="statistical-inference.html#cb344-1" tabindex="-1"></a>min_score <span class="ot">&lt;-</span> <span class="dv">900</span></span>
<span id="cb344-2"><a href="statistical-inference.html#cb344-2" tabindex="-1"></a></span>
<span id="cb344-3"><a href="statistical-inference.html#cb344-3" tabindex="-1"></a><span class="co"># Probability of obtaining less than the minimum:</span></span>
<span id="cb344-4"><a href="statistical-inference.html#cb344-4" tabindex="-1"></a>prob <span class="ot">&lt;-</span> <span class="fu">pnorm</span>(min_score, E_sum, SE_sum)</span>
<span id="cb344-5"><a href="statistical-inference.html#cb344-5" tabindex="-1"></a></span>
<span id="cb344-6"><a href="statistical-inference.html#cb344-6" tabindex="-1"></a><span class="co"># Probability of obtaining more than the minimum:</span></span>
<span id="cb344-7"><a href="statistical-inference.html#cb344-7" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> prob</span></code></pre></div>
<p>This means that the probability that a student obtains the minimum score by guessing all the questions is: <code>0.0000000000014525</code>.</p>
<p>Conclusion: let’s study before taking the exam. It is not reasonable to take the exam randomly and mark randomly.</p>
</details>
<blockquote>
<p>Recall that <code>e-n</code> is the representation of <span class="math inline">\(10^{-n}\)</span>.</p>
</blockquote>
<ol start="81" style="list-style-type: decimal">
<li>Validate your theoretical calculation by running a Monte Carlo simulation. Simulate the exam scores for 22,000 applicants guessing randomly and calculate the proportion who score above 900 points.</li>
</ol>
<details>
<summary type="button">
Solution
</summary>
<div class="sourceCode" id="cb345"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb345-1"><a href="statistical-inference.html#cb345-1" tabindex="-1"></a>total <span class="ot">&lt;-</span> <span class="dv">22000</span></span>
<span id="cb345-2"><a href="statistical-inference.html#cb345-2" tabindex="-1"></a></span>
<span id="cb345-3"><a href="statistical-inference.html#cb345-3" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2025</span>)</span>
<span id="cb345-4"><a href="statistical-inference.html#cb345-4" tabindex="-1"></a>admission_simulation <span class="ot">&lt;-</span> <span class="fu">replicate</span>(total, {</span>
<span id="cb345-5"><a href="statistical-inference.html#cb345-5" tabindex="-1"></a>  exam_score <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">c</span>(points_correct, points_wrong), n, <span class="at">replace =</span> <span class="cn">TRUE</span>, <span class="at">prob =</span> <span class="fu">c</span>(prob_correct, prob_wrong) )</span>
<span id="cb345-6"><a href="statistical-inference.html#cb345-6" tabindex="-1"></a></span>
<span id="cb345-7"><a href="statistical-inference.html#cb345-7" tabindex="-1"></a>  <span class="fu">sum</span>(exam_score)  </span>
<span id="cb345-8"><a href="statistical-inference.html#cb345-8" tabindex="-1"></a>})</span>
<span id="cb345-9"><a href="statistical-inference.html#cb345-9" tabindex="-1"></a></span>
<span id="cb345-10"><a href="statistical-inference.html#cb345-10" tabindex="-1"></a><span class="co"># Probability of obtaining more than 900 points:</span></span>
<span id="cb345-11"><a href="statistical-inference.html#cb345-11" tabindex="-1"></a><span class="fu">mean</span>(admission_simulation <span class="sc">&gt;</span> <span class="dv">900</span>)</span>
<span id="cb345-12"><a href="statistical-inference.html#cb345-12" tabindex="-1"></a></span>
<span id="cb345-13"><a href="statistical-inference.html#cb345-13" tabindex="-1"></a><span class="co"># Histogram if we want to see the distribution of points obtained:</span></span>
<span id="cb345-14"><a href="statistical-inference.html#cb345-14" tabindex="-1"></a><span class="fu">hist</span>(admission_simulation)</span></code></pre></div>
<p>We see that the simulation gives us practically the same result. Practically there are no possibilities of entering UNMSM by guessing the answers.</p>
</details>
</div>
<div id="parameter-estimation-method" class="section level2 hasAnchor" number="9.6">
<h2><span class="header-section-number">9.6</span> Parameter Estimation Method<a href="statistical-inference.html#parameter-estimation-method" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>So far, using Monte Carlo simulation we have built samples randomly, but knowing the probability of occurrence. However, we will not always know the proportion previously. If we have, for example, a population and we want to know how many have been infected by Covid-19, we cannot test everyone. Or if we have the total voters for an election, we cannot survey everyone to know who would win. Not only is it very expensive, but it would take us a lot of time.</p>
<p>The <strong>parameter estimation method</strong> is the procedure used to know the characteristics of a population parameter, from the knowledge of a sample of <span class="math inline">\(n\)</span> respondents</p>
<p>We will analyze this following case.</p>
<blockquote>
<p>We have two political parties: Blue and Red. We do not know how much the total population is, nor the proportion that will vote for one or the other party. The only thing we can do is conduct voting intention polls.</p>
</blockquote>
<p>For example, these would be the results of the poll of a random sample of 10 people:</p>
<p><img src="Data-Science-with-R_files/figure-html/unnamed-chunk-461-1.png" alt="" width="80%" style="display: block; margin: auto;" /></p>
<p>Intuitively we know that we cannot deduce which party will win given that the sample is very small. To know which party will win we need to estimate as precisely as possible the parameter <span class="math inline">\(p\)</span> that represents the proportion of voters of the Blue Party in the population and the parameter <span class="math inline">\(1-p\)</span> that represents the proportion of voters of the Red party.</p>
<p>Making some mathematical transformations to our theoretical estimates seen previously and defining <span class="math inline">\(a=1\)</span> as value if they vote Blue and <span class="math inline">\(b=0\)</span> if they do not vote for Blue, we can obtain the following theoretical estimates for this case:</p>
<p>By defining a vote for Blue as <span class="math inline">\(a=1\)</span> and a vote for others as <span class="math inline">\(b=0\)</span>, we can derive theoretical estimates that link our sample data to the unknown population parameter <span class="math inline">\(p\)</span>.</p>
<p>First, consider the <strong>expected value of a single vote</strong>. Mathematically, <span class="math inline">\(E[X]=p\)</span>. This confirms that the value we expect to obtain from a single random voter matches the proportion <span class="math inline">\(p\)</span> we are trying to find.</p>
<p>Next, we look at the <strong>expected value of the average</strong> across multiple surveys. If we were to conduct many surveys of <span class="math inline">\(n\)</span> respondents each, the average of these sample means, denoted as <span class="math inline">\(E[\overline{X}]\)</span>, would also equal <span class="math inline">\(p\)</span>. This reinforces that our sample mean is an unbiased estimator of the population proportion.</p>
<p>Finally, we must account for variability using the <strong>standard error of the average</strong>, defined as <span class="math inline">\(SE[\overline{X}]=\sqrt{\frac{p(1-p) }{n}}\)</span>. This metric tells us how much the results of our multiple surveys would fluctuate around the true parameter <span class="math inline">\(p\)</span>, taking into account the sample size <span class="math inline">\(n\)</span>.</p>
<p>The expected value of the average <span class="math inline">\(E[\overline{X}]\)</span>, formula 2, is theoretically equal to the parameter <span class="math inline">\(p\)</span> that we are looking to estimate. However, without knowing how much <span class="math inline">\(p\)</span> is we would have to have multiple samples of <span class="math inline">\(n\)</span> respondents, then calculate the mean for each case <span class="math inline">\(\overline{X}\)</span> and finally calculate the average of these values. This is very expensive, so we will look for another way to estimate <span class="math inline">\(E[\overline{X}]\)</span>.</p>
<p>Given that we do not have so far how to estimate <span class="math inline">\(E[\overline{X}]\)</span>, and given that we know that <span class="math inline">\(E[\overline{X}]=p\)</span> then we could give several values to <span class="math inline">\(p\)</span> and see the impact on the standard error of the average that we know also depends on <span class="math inline">\(p\)</span>.</p>
<p>Let’s generate, first, a sequence of parameter <span class="math inline">\(p\)</span>, from 0% to 100%, 100 different values:</p>
<div class="sourceCode" id="cb346"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb346-1"><a href="statistical-inference.html#cb346-1" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">length=</span><span class="dv">100</span>)</span>
<span id="cb346-2"><a href="statistical-inference.html#cb346-2" tabindex="-1"></a></span>
<span id="cb346-3"><a href="statistical-inference.html#cb346-3" tabindex="-1"></a>p</span>
<span id="cb346-4"><a href="statistical-inference.html#cb346-4" tabindex="-1"></a><span class="co">#&gt;   [1] 0.00000000 0.01010101 0.02020202 0.03030303 0.04040404 0.05050505</span></span>
<span id="cb346-5"><a href="statistical-inference.html#cb346-5" tabindex="-1"></a><span class="co">#&gt;   [7] 0.06060606 0.07070707 0.08080808 0.09090909 0.10101010 0.11111111</span></span>
<span id="cb346-6"><a href="statistical-inference.html#cb346-6" tabindex="-1"></a><span class="co">#&gt;  [13] 0.12121212 0.13131313 0.14141414 0.15151515 0.16161616 0.17171717</span></span>
<span id="cb346-7"><a href="statistical-inference.html#cb346-7" tabindex="-1"></a><span class="co">#&gt;  [19] 0.18181818 0.19191919 0.20202020 0.21212121 0.22222222 0.23232323</span></span>
<span id="cb346-8"><a href="statistical-inference.html#cb346-8" tabindex="-1"></a><span class="co">#&gt;  [25] 0.24242424 0.25252525 0.26262626 0.27272727 0.28282828 0.29292929</span></span>
<span id="cb346-9"><a href="statistical-inference.html#cb346-9" tabindex="-1"></a><span class="co">#&gt;  [31] 0.30303030 0.31313131 0.32323232 0.33333333 0.34343434 0.35353535</span></span>
<span id="cb346-10"><a href="statistical-inference.html#cb346-10" tabindex="-1"></a><span class="co">#&gt;  [37] 0.36363636 0.37373737 0.38383838 0.39393939 0.40404040 0.41414141</span></span>
<span id="cb346-11"><a href="statistical-inference.html#cb346-11" tabindex="-1"></a><span class="co">#&gt;  [43] 0.42424242 0.43434343 0.44444444 0.45454545 0.46464646 0.47474747</span></span>
<span id="cb346-12"><a href="statistical-inference.html#cb346-12" tabindex="-1"></a><span class="co">#&gt;  [49] 0.48484848 0.49494949 0.50505051 0.51515152 0.52525253 0.53535354</span></span>
<span id="cb346-13"><a href="statistical-inference.html#cb346-13" tabindex="-1"></a><span class="co">#&gt;  [55] 0.54545455 0.55555556 0.56565657 0.57575758 0.58585859 0.59595960</span></span>
<span id="cb346-14"><a href="statistical-inference.html#cb346-14" tabindex="-1"></a><span class="co">#&gt;  [61] 0.60606061 0.61616162 0.62626263 0.63636364 0.64646465 0.65656566</span></span>
<span id="cb346-15"><a href="statistical-inference.html#cb346-15" tabindex="-1"></a><span class="co">#&gt;  [67] 0.66666667 0.67676768 0.68686869 0.69696970 0.70707071 0.71717172</span></span>
<span id="cb346-16"><a href="statistical-inference.html#cb346-16" tabindex="-1"></a><span class="co">#&gt;  [73] 0.72727273 0.73737374 0.74747475 0.75757576 0.76767677 0.77777778</span></span>
<span id="cb346-17"><a href="statistical-inference.html#cb346-17" tabindex="-1"></a><span class="co">#&gt;  [79] 0.78787879 0.79797980 0.80808081 0.81818182 0.82828283 0.83838384</span></span>
<span id="cb346-18"><a href="statistical-inference.html#cb346-18" tabindex="-1"></a><span class="co">#&gt;  [85] 0.84848485 0.85858586 0.86868687 0.87878788 0.88888889 0.89898990</span></span>
<span id="cb346-19"><a href="statistical-inference.html#cb346-19" tabindex="-1"></a><span class="co">#&gt;  [91] 0.90909091 0.91919192 0.92929293 0.93939394 0.94949495 0.95959596</span></span>
<span id="cb346-20"><a href="statistical-inference.html#cb346-20" tabindex="-1"></a><span class="co">#&gt;  [97] 0.96969697 0.97979798 0.98989899 1.00000000</span></span></code></pre></div>
<p>Thinking of 100 different values of <span class="math inline">\(p\)</span> would be like thinking of 100 different elections where the Blue party and the red one have participation, like the election for mayors nationwide. In some districts the candidate of the Blue party loses with 0%, in others ties at 50% and in others wins clearly with 100% of the votes.</p>
<p>Intuitively we know that if our real proportion was <span class="math inline">\(p=80\%\)</span> for the Blue party, that is that 8 out of every 10 will vote Blue, then it is very likely that in each survey we take we will find that in that district the Blue party has the majority of votes. This is predicted with the formula seen before and also includes the size of the survey <span class="math inline">\(n\)</span> as part of the calculation:</p>
<p><span class="math inline">\(SE[\overline{X}]=\sqrt{\frac{p(1-p) }{n}}\)</span></p>
<p>That said, let’s return to our vector <code>p</code> that contains several values of parameter <span class="math inline">\(p\)</span>. On those values we can calculate what would happen if we survey groups of 20 people. Knowing the sample size we can calculate the standard error of the average for each of the values of <span class="math inline">\(p\)</span>:</p>
<div class="sourceCode" id="cb347"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb347-1"><a href="statistical-inference.html#cb347-1" tabindex="-1"></a><span class="co"># Total people in each survey:</span></span>
<span id="cb347-2"><a href="statistical-inference.html#cb347-2" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">20</span></span>
<span id="cb347-3"><a href="statistical-inference.html#cb347-3" tabindex="-1"></a></span>
<span id="cb347-4"><a href="statistical-inference.html#cb347-4" tabindex="-1"></a><span class="co"># Standard error of the average:</span></span>
<span id="cb347-5"><a href="statistical-inference.html#cb347-5" tabindex="-1"></a>SE_avg <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(p<span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span>p))<span class="sc">/</span><span class="fu">sqrt</span>(n)</span>
<span id="cb347-6"><a href="statistical-inference.html#cb347-6" tabindex="-1"></a></span>
<span id="cb347-7"><a href="statistical-inference.html#cb347-7" tabindex="-1"></a>SE_avg  </span>
<span id="cb347-8"><a href="statistical-inference.html#cb347-8" tabindex="-1"></a><span class="co">#&gt;   [1] 0.00000000 0.02235954 0.03145942 0.03833064 0.04402928 0.04896646</span></span>
<span id="cb347-9"><a href="statistical-inference.html#cb347-9" tabindex="-1"></a><span class="co">#&gt;   [7] 0.05335399 0.05731823 0.06094183 0.06428243 0.06738214 0.07027284</span></span>
<span id="cb347-10"><a href="statistical-inference.html#cb347-10" tabindex="-1"></a><span class="co">#&gt;  [13] 0.07297936 0.07552152 0.07791540 0.08017428 0.08230929 0.08432982</span></span>
<span id="cb347-11"><a href="statistical-inference.html#cb347-11" tabindex="-1"></a><span class="co">#&gt;  [19] 0.08624394 0.08805856 0.08977974 0.09141275 0.09296223 0.09443229</span></span>
<span id="cb347-12"><a href="statistical-inference.html#cb347-12" tabindex="-1"></a><span class="co">#&gt;  [25] 0.09582660 0.09714840 0.09840064 0.09958592 0.10070661 0.10176486</span></span>
<span id="cb347-13"><a href="statistical-inference.html#cb347-13" tabindex="-1"></a><span class="co">#&gt;  [31] 0.10276258 0.10370152 0.10458327 0.10540926 0.10618079 0.10689904</span></span>
<span id="cb347-14"><a href="statistical-inference.html#cb347-14" tabindex="-1"></a><span class="co">#&gt;  [37] 0.10756509 0.10817988 0.10874431 0.10925913 0.10972506 0.11014270</span></span>
<span id="cb347-15"><a href="statistical-inference.html#cb347-15" tabindex="-1"></a><span class="co">#&gt;  [43] 0.11051262 0.11083529 0.11111111 0.11134044 0.11152357 0.11166072</span></span>
<span id="cb347-16"><a href="statistical-inference.html#cb347-16" tabindex="-1"></a><span class="co">#&gt;  [49] 0.11175205 0.11179770 0.11179770 0.11175205 0.11166072 0.11152357</span></span>
<span id="cb347-17"><a href="statistical-inference.html#cb347-17" tabindex="-1"></a><span class="co">#&gt;  [55] 0.11134044 0.11111111 0.11083529 0.11051262 0.11014270 0.10972506</span></span>
<span id="cb347-18"><a href="statistical-inference.html#cb347-18" tabindex="-1"></a><span class="co">#&gt;  [61] 0.10925913 0.10874431 0.10817988 0.10756509 0.10689904 0.10618079</span></span>
<span id="cb347-19"><a href="statistical-inference.html#cb347-19" tabindex="-1"></a><span class="co">#&gt;  [67] 0.10540926 0.10458327 0.10370152 0.10276258 0.10176486 0.10070661</span></span>
<span id="cb347-20"><a href="statistical-inference.html#cb347-20" tabindex="-1"></a><span class="co">#&gt;  [73] 0.09958592 0.09840064 0.09714840 0.09582660 0.09443229 0.09296223</span></span>
<span id="cb347-21"><a href="statistical-inference.html#cb347-21" tabindex="-1"></a><span class="co">#&gt;  [79] 0.09141275 0.08977974 0.08805856 0.08624394 0.08432982 0.08230929</span></span>
<span id="cb347-22"><a href="statistical-inference.html#cb347-22" tabindex="-1"></a><span class="co">#&gt;  [85] 0.08017428 0.07791540 0.07552152 0.07297936 0.07027284 0.06738214</span></span>
<span id="cb347-23"><a href="statistical-inference.html#cb347-23" tabindex="-1"></a><span class="co">#&gt;  [91] 0.06428243 0.06094183 0.05731823 0.05335399 0.04896646 0.04402928</span></span>
<span id="cb347-24"><a href="statistical-inference.html#cb347-24" tabindex="-1"></a><span class="co">#&gt;  [97] 0.03833064 0.03145942 0.02235954 0.00000000</span></span></code></pre></div>
<p>Now let’s generate a scatter plot of both the different values of <span class="math inline">\(p\)</span> and the standard errors for each <span class="math inline">\(p\)</span>.</p>
<div class="sourceCode" id="cb348"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb348-1"><a href="statistical-inference.html#cb348-1" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">p =</span> p, <span class="at">SE_avg =</span> SE_avg) <span class="sc">|&gt;</span></span>
<span id="cb348-2"><a href="statistical-inference.html#cb348-2" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> p, <span class="at">y =</span> SE_avg)) <span class="sc">+</span></span>
<span id="cb348-3"><a href="statistical-inference.html#cb348-3" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">color =</span> <span class="st">&quot;steelblue&quot;</span>) <span class="sc">+</span></span>
<span id="cb348-4"><a href="statistical-inference.html#cb348-4" tabindex="-1"></a>  <span class="fu">coord_cartesian</span>(<span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.12</span>)) <span class="sc">+</span></span>
<span id="cb348-5"><a href="statistical-inference.html#cb348-5" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Proportion (p)&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Standard Error&quot;</span>, <span class="at">title =</span> <span class="st">&quot;Standard Error vs. Proportion&quot;</span>)</span></code></pre></div>
<p><img src="Data-Science-with-R_files/figure-html/unnamed-chunk-464-1.png" alt="" width="80%" style="display: block; margin: auto;" /></p>
<p>Thus, we see how we can obtain different standard errors of the average for different values of <span class="math inline">\(p\)</span>.</p>
<p>Intuitively we had the notion of what would happen given a <span class="math inline">\(p=80\%\)</span>. Now in the graph we see it better. If the real intention of vote was 80% in that district then when taking several surveys and seeing the results of each survey we would obtain as expected value 80% and as standard error 8.8% or 0.088 as seen in the graph highlighted in blue:</p>
<div class="sourceCode" id="cb349"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb349-1"><a href="statistical-inference.html#cb349-1" tabindex="-1"></a>coord_x <span class="ot">&lt;-</span> <span class="fl">0.8</span></span>
<span id="cb349-2"><a href="statistical-inference.html#cb349-2" tabindex="-1"></a>coord_y <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(coord_x <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> coord_x)) <span class="sc">/</span> <span class="fu">sqrt</span>(n)</span>
<span id="cb349-3"><a href="statistical-inference.html#cb349-3" tabindex="-1"></a></span>
<span id="cb349-4"><a href="statistical-inference.html#cb349-4" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">p =</span> p, <span class="at">SE_avg =</span> SE_avg) <span class="sc">|&gt;</span></span>
<span id="cb349-5"><a href="statistical-inference.html#cb349-5" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> p, <span class="at">y =</span> SE_avg)) <span class="sc">+</span></span>
<span id="cb349-6"><a href="statistical-inference.html#cb349-6" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">color =</span> <span class="st">&quot;steelblue&quot;</span>) <span class="sc">+</span></span>
<span id="cb349-7"><a href="statistical-inference.html#cb349-7" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> coord_y, <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>, <span class="at">color =</span> <span class="st">&quot;blue&quot;</span>) <span class="sc">+</span></span>
<span id="cb349-8"><a href="statistical-inference.html#cb349-8" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> coord_x, <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>, <span class="at">color =</span> <span class="st">&quot;blue&quot;</span>) <span class="sc">+</span></span>
<span id="cb349-9"><a href="statistical-inference.html#cb349-9" tabindex="-1"></a>  <span class="fu">coord_cartesian</span>(<span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.12</span>)) <span class="sc">+</span></span>
<span id="cb349-10"><a href="statistical-inference.html#cb349-10" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Proportion (p)&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Standard Error&quot;</span>, <span class="at">title =</span> <span class="st">&quot;SE at p = 80%&quot;</span>)</span></code></pre></div>
<p><img src="Data-Science-with-R_files/figure-html/unnamed-chunk-465-1.png" alt="" width="80%" style="display: block; margin: auto;" /></p>
<p>With these values of <span class="math inline">\(E[\overline{X}]=p=80\%\)</span> and <span class="math inline">\(SE[\overline{X}]=8.8\%\)</span> of standard error we can calculate a range of one standard error around <span class="math inline">\(80\%\)</span>, which would go from <span class="math inline">\(71.2\%\)</span> to <span class="math inline">\(88.8\%\)</span> and then calculate what would be the probability that the mean <span class="math inline">\(\overline{X}\)</span> found in one of the surveys falls in this range. Visually it would be:</p>
<p><img src="Data-Science-with-R_files/figure-html/unnamed-chunk-466-1.png" alt="" width="80%" style="display: block; margin: auto;" /></p>
<p>In R, calculating the probability that a data point falls in the range of 1 standard error would be:</p>
<div class="sourceCode" id="cb350"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb350-1"><a href="statistical-inference.html#cb350-1" tabindex="-1"></a><span class="co"># Calculation of probability that dat is between -1 and 1 standard error:</span></span>
<span id="cb350-2"><a href="statistical-inference.html#cb350-2" tabindex="-1"></a><span class="fu">pnorm</span>(<span class="dv">1</span>) <span class="sc">-</span> <span class="fu">pnorm</span>(<span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb350-3"><a href="statistical-inference.html#cb350-3" tabindex="-1"></a><span class="co">#&gt; [1] 0.6826895</span></span></code></pre></div>
<p>We can expand to have a greater range of <strong>2 standard errors</strong> around <span class="math inline">\(80\%\)</span> and increase our probability:</p>
<p><img src="Data-Science-with-R_files/figure-html/unnamed-chunk-468-1.png" alt="" width="80%" style="display: block; margin: auto;" /></p>
<p>In R it would be:</p>
<div class="sourceCode" id="cb351"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb351-1"><a href="statistical-inference.html#cb351-1" tabindex="-1"></a><span class="co"># Calculation of probability that dat is between -2 and 2 standard errors:</span></span>
<span id="cb351-2"><a href="statistical-inference.html#cb351-2" tabindex="-1"></a><span class="fu">pnorm</span>(<span class="dv">2</span>) <span class="sc">-</span> <span class="fu">pnorm</span>(<span class="sc">-</span><span class="dv">2</span>)</span>
<span id="cb351-3"><a href="statistical-inference.html#cb351-3" tabindex="-1"></a><span class="co">#&gt; [1] 0.9544997</span></span></code></pre></div>
<p>The probability increases to 95%, however how do we interpret this?. We haven’t even calculated the real value of the mean <span class="math inline">\(\overline{X}\)</span> of some survey.</p>
<p>Simple, this means that, theoretically, there is a 95% probability that the mean <span class="math inline">\(\overline{X}\)</span> that we find in each survey is in the range of 62% to 98%, two standard errors around <span class="math inline">\(80\%\)</span>. 95% of the time in the worst case, in a survey of 20 people, the Blue party would obtain 62% and in the best case 98%, so we could predict that the Blue party will win. Or not?</p>
<p>Several things should make noise to us so far. First, the range so large, from 62% to 98%. Second, we have assumed a scenario: that the Blue voting intention was known and was 80%. That is, we have assumed <span class="math inline">\(p=80\%\)</span> which allowed us to calculate <span class="math inline">\(E[\overline{X}]=80\%\)</span> and place that value at the center of the normal. However, <span class="math inline">\(p\)</span> is unknown and is precisely what we are trying to estimate.</p>
<p>If, on the contrary, the result was tighter, for example <span class="math inline">\(p=55\%\)</span>, such a wide range would not serve us. Let’s see how it would be:</p>
<div class="sourceCode" id="cb352"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb352-1"><a href="statistical-inference.html#cb352-1" tabindex="-1"></a>coord_x <span class="ot">&lt;-</span> <span class="fl">0.55</span></span>
<span id="cb352-2"><a href="statistical-inference.html#cb352-2" tabindex="-1"></a>coord_y <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(coord_x <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> coord_x)) <span class="sc">/</span> <span class="fu">sqrt</span>(n)</span>
<span id="cb352-3"><a href="statistical-inference.html#cb352-3" tabindex="-1"></a></span>
<span id="cb352-4"><a href="statistical-inference.html#cb352-4" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">p =</span> p, <span class="at">SE_avg =</span> SE_avg) <span class="sc">|&gt;</span></span>
<span id="cb352-5"><a href="statistical-inference.html#cb352-5" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> p, <span class="at">y =</span> SE_avg)) <span class="sc">+</span></span>
<span id="cb352-6"><a href="statistical-inference.html#cb352-6" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">color =</span> <span class="st">&quot;steelblue&quot;</span>) <span class="sc">+</span></span>
<span id="cb352-7"><a href="statistical-inference.html#cb352-7" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> coord_y, <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>, <span class="at">color =</span> <span class="st">&quot;blue&quot;</span>) <span class="sc">+</span></span>
<span id="cb352-8"><a href="statistical-inference.html#cb352-8" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> coord_x, <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>, <span class="at">color =</span> <span class="st">&quot;blue&quot;</span>) <span class="sc">+</span></span>
<span id="cb352-9"><a href="statistical-inference.html#cb352-9" tabindex="-1"></a>  <span class="fu">coord_cartesian</span>(<span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.12</span>)) <span class="sc">+</span></span>
<span id="cb352-10"><a href="statistical-inference.html#cb352-10" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Proportion (p)&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Standard Error&quot;</span>, <span class="at">title =</span> <span class="st">&quot;SE at p = 55%&quot;</span>)</span></code></pre></div>
<p><img src="Data-Science-with-R_files/figure-html/unnamed-chunk-470-1.png" alt="" width="80%" style="display: block; margin: auto;" /></p>
<p>If the real voting intention was <span class="math inline">\(55\%\)</span> we would have an expected value of the average <span class="math inline">\(E[\overline{X}]=p=55\%\)</span> and a corresponding standard error of the average <span class="math inline">\(SE[\overline{X}]=11\%\)</span>. Again, by Central Limit Theorem we can calculate a range of <strong>two standard errors</strong> around <span class="math inline">\(55\%\)</span>:</p>
<p><img src="Data-Science-with-R_files/figure-html/unnamed-chunk-471-1.png" alt="" width="80%" style="display: block; margin: auto;" /></p>
<p>The calculation of the probability of being in that range in R would be the same because we continue in the range of 2 standard errors. Therefore the probability would be the same.</p>
<div class="sourceCode" id="cb353"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb353-1"><a href="statistical-inference.html#cb353-1" tabindex="-1"></a><span class="co"># Calculation of probability that dat is between -2 and 2 standard errors:</span></span>
<span id="cb353-2"><a href="statistical-inference.html#cb353-2" tabindex="-1"></a><span class="fu">pnorm</span>(<span class="dv">2</span>) <span class="sc">-</span> <span class="fu">pnorm</span>(<span class="sc">-</span><span class="dv">2</span>)</span>
<span id="cb353-3"><a href="statistical-inference.html#cb353-3" tabindex="-1"></a><span class="co">#&gt; [1] 0.9544997</span></span></code></pre></div>
<p>However, what does change is the range. Now the range goes from 32.8% to 77.2%, two standard errors around the expected value of the average <span class="math inline">\(E[\overline{X}]\)</span>. Although the probability is still 95%, that does not help us at all this time because there is 95% that what we find in our sample is a value between 33% and 77%. Some survey samples will give us 33% of votes for Blue and other samples 77%.</p>
<p>And the problem lies in the number of samples taken <span class="math inline">\(n\)</span>. If we see again the formula we see how <span class="math inline">\(n\)</span> influences the result.</p>
<p><span class="math inline">\(SE[\overline{X}]=\sqrt{\frac{p(1-p) }{n}}\)</span></p>
<p>Let’s increment then our number of respondents to 500:</p>
<div class="sourceCode" id="cb354"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb354-1"><a href="statistical-inference.html#cb354-1" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">500</span></span>
<span id="cb354-2"><a href="statistical-inference.html#cb354-2" tabindex="-1"></a></span>
<span id="cb354-3"><a href="statistical-inference.html#cb354-3" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">length =</span> <span class="dv">100</span>)</span>
<span id="cb354-4"><a href="statistical-inference.html#cb354-4" tabindex="-1"></a>SE_avg <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(p<span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span>p))<span class="sc">/</span><span class="fu">sqrt</span>(n)</span>
<span id="cb354-5"><a href="statistical-inference.html#cb354-5" tabindex="-1"></a></span>
<span id="cb354-6"><a href="statistical-inference.html#cb354-6" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">p =</span> p, <span class="at">SE_avg =</span> SE_avg) <span class="sc">|&gt;</span></span>
<span id="cb354-7"><a href="statistical-inference.html#cb354-7" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> p, <span class="at">y =</span> SE_avg)) <span class="sc">+</span></span>
<span id="cb354-8"><a href="statistical-inference.html#cb354-8" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">color =</span> <span class="st">&quot;steelblue&quot;</span>) <span class="sc">+</span></span>
<span id="cb354-9"><a href="statistical-inference.html#cb354-9" tabindex="-1"></a>  <span class="fu">coord_cartesian</span>(<span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.12</span>)) <span class="sc">+</span></span>
<span id="cb354-10"><a href="statistical-inference.html#cb354-10" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Proportion (p)&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Standard Error&quot;</span>, <span class="at">title =</span> <span class="st">&quot;SE vs Proportion (n = 500)&quot;</span>)</span></code></pre></div>
<p><img src="Data-Science-with-R_files/figure-html/unnamed-chunk-473-1.png" alt="" width="80%" style="display: block; margin: auto;" /></p>
<p>This sample gives us smaller standard errors. For example, if the real proportion of voters of the Blue party was <span class="math inline">\(p=55\%\)</span> we would have <span class="math inline">\(E[\overline{X}]=p=55\%\)</span> and a <span class="math inline">\(SE[\overline{X}]=2.2\%\)</span>:</p>
<div class="sourceCode" id="cb355"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb355-1"><a href="statistical-inference.html#cb355-1" tabindex="-1"></a>coord_x <span class="ot">&lt;-</span> <span class="fl">0.55</span></span>
<span id="cb355-2"><a href="statistical-inference.html#cb355-2" tabindex="-1"></a>coord_y <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(coord_x <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> coord_x)) <span class="sc">/</span> <span class="fu">sqrt</span>(n)</span>
<span id="cb355-3"><a href="statistical-inference.html#cb355-3" tabindex="-1"></a></span>
<span id="cb355-4"><a href="statistical-inference.html#cb355-4" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">p =</span> p, <span class="at">SE_avg =</span> SE_avg) <span class="sc">|&gt;</span></span>
<span id="cb355-5"><a href="statistical-inference.html#cb355-5" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> p, <span class="at">y =</span> SE_avg)) <span class="sc">+</span></span>
<span id="cb355-6"><a href="statistical-inference.html#cb355-6" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">color =</span> <span class="st">&quot;steelblue&quot;</span>) <span class="sc">+</span></span>
<span id="cb355-7"><a href="statistical-inference.html#cb355-7" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> coord_y, <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>, <span class="at">color =</span> <span class="st">&quot;blue&quot;</span>) <span class="sc">+</span></span>
<span id="cb355-8"><a href="statistical-inference.html#cb355-8" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> coord_x, <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>, <span class="at">color =</span> <span class="st">&quot;blue&quot;</span>) <span class="sc">+</span></span>
<span id="cb355-9"><a href="statistical-inference.html#cb355-9" tabindex="-1"></a>  <span class="fu">coord_cartesian</span>(<span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.12</span>)) <span class="sc">+</span></span>
<span id="cb355-10"><a href="statistical-inference.html#cb355-10" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Proportion (p)&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Standard Error&quot;</span>, <span class="at">title =</span> <span class="st">&quot;SE at p = 55% (n = 500)&quot;</span>)</span></code></pre></div>
<p><img src="Data-Science-with-R_files/figure-html/unnamed-chunk-474-1.png" alt="" width="80%" style="display: block; margin: auto;" /></p>
<p>If we now calculate a range of two standard errors around <span class="math inline">\(55\%\)</span> we would have a range that goes from <span class="math inline">\(50.6\%\)</span> to <span class="math inline">\(59.4\%\)</span>. Again, interpretation is that the mean that we find in our random survey has a 95% probability of being in that range.</p>
<p>We see then that this theoretical prediction, the standard error, becomes smaller as the sample size <span class="math inline">\(n\)</span> increases and in turn depends on the probability of the population <span class="math inline">\(p\)</span> that we do not know. Moreover, with a real value of <span class="math inline">\(p=0.5\)</span>, (50%), we have the maximum value of the standard error that we can obtain. Thus, if we correct <span class="math inline">\(p\)</span> at 50%, which would be the extreme of cases, a tie, we can calculate how the value of the standard error of the average changes according to the sample size:</p>
<div class="sourceCode" id="cb356"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb356-1"><a href="statistical-inference.html#cb356-1" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fl">0.5</span></span>
<span id="cb356-2"><a href="statistical-inference.html#cb356-2" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">20</span>, <span class="dv">5000</span>, <span class="dv">20</span>)</span>
<span id="cb356-3"><a href="statistical-inference.html#cb356-3" tabindex="-1"></a>SE_avg <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(p<span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span>p)<span class="sc">/</span>n)</span>
<span id="cb356-4"><a href="statistical-inference.html#cb356-4" tabindex="-1"></a></span>
<span id="cb356-5"><a href="statistical-inference.html#cb356-5" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">n =</span> n, <span class="at">SE_avg =</span> SE_avg) <span class="sc">|&gt;</span></span>
<span id="cb356-6"><a href="statistical-inference.html#cb356-6" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> n, <span class="at">y =</span> SE_avg)) <span class="sc">+</span></span>
<span id="cb356-7"><a href="statistical-inference.html#cb356-7" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">color =</span> <span class="st">&quot;steelblue&quot;</span>) <span class="sc">+</span></span>
<span id="cb356-8"><a href="statistical-inference.html#cb356-8" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="fl">0.015</span>, <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>, <span class="at">color =</span> <span class="st">&quot;blue&quot;</span>) <span class="sc">+</span></span>
<span id="cb356-9"><a href="statistical-inference.html#cb356-9" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="dv">1000</span>, <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>, <span class="at">color =</span> <span class="st">&quot;blue&quot;</span>) <span class="sc">+</span></span>
<span id="cb356-10"><a href="statistical-inference.html#cb356-10" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Sample Size (n)&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Standard Error&quot;</span>, <span class="at">title =</span> <span class="st">&quot;SE vs Sample Size (p = 50%)&quot;</span>)</span></code></pre></div>
<p><img src="Data-Science-with-R_files/figure-html/unnamed-chunk-475-1.png" alt="" width="80%" style="display: block; margin: auto;" /></p>
<p>A sample of 1,000 people, for example, generates us a maximum standard error of 0.015 or 1.5%.</p>
<div id="margin-of-error" class="section level3 hasAnchor" number="9.6.1">
<h3><span class="header-section-number">9.6.1</span> Margin of Error<a href="statistical-inference.html#margin-of-error" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As we have already seen, we could consider a range of 1 standard error or 2 standard errors around <span class="math inline">\(E[\overline{X}]\)</span> and calculate the probability that our sample mean <span class="math inline">\(\overline{X}\)</span> is in that range. Or what is mathematically the same, we could say that if we build a range of 1 or 2 standard errors around our sample mean <span class="math inline">\(\overline{X}\)</span> there is a determined probability that in that range is included the expected value <span class="math inline">\(E[\overline{X}]\)</span> which is, by formula equal to <span class="math inline">\(p\)</span>, the value we want to estimate.</p>
<p>It is crucial then to calculate the standard error of the average <span class="math inline">\(SE[\overline{X}]\)</span>, but we see ourselves limited because it depends on <span class="math inline">\(p\)</span>.</p>
<p>There is another way to calculate <span class="math inline">\(SE[\overline{X}]\)</span> without using <span class="math inline">\(p\)</span> and is known as the standard error of estimation <span class="math inline">\(\hat{SE}[\overline{X}]\)</span>. For this we will use the following formula:</p>
<p><span class="math inline">\(\hat{SE}[\overline{X}]=\sqrt{\frac{\overline{X}(1-\overline{X})}{n}}\)</span></p>
<p>Where, as we already know, <span class="math inline">\(\overline{X}\)</span> is the mean of our sample or sample mean. For our example case, it is the percentage that the Blue party obtained in the survey we conducted.</p>
<p>Now that we have the sample mean <span class="math inline">\(\overline{X}\)</span> and we can already calculate the standard error of estimation <span class="math inline">\(\hat{SE}[\overline{X}]\)</span> we can start building ranges around <span class="math inline">\(\overline{X}\)</span> that increase the probability of finding <span class="math inline">\(p\)</span>.</p>
<p>To make communication simpler, we will use by convention the notation <strong>margin of error</strong> to indicate that we are going to take a <strong>range of 2 standard errors of estimation</strong>.</p>
<p>For example, we have a sample of 1100 people and after reviewing the survey results we have a sample mean of <span class="math inline">\(\overline{X}=56\%\)</span> for the Blue party. With this we can estimate the standard error of estimation <span class="math inline">\(\hat{SE}[\overline{X}]\)</span> with the formula we just described and finally calculate the margin of error.</p>
<div class="sourceCode" id="cb357"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb357-1"><a href="statistical-inference.html#cb357-1" tabindex="-1"></a><span class="co"># Total respondents</span></span>
<span id="cb357-2"><a href="statistical-inference.html#cb357-2" tabindex="-1"></a>total <span class="ot">&lt;-</span> <span class="dv">1100</span></span>
<span id="cb357-3"><a href="statistical-inference.html#cb357-3" tabindex="-1"></a></span>
<span id="cb357-4"><a href="statistical-inference.html#cb357-4" tabindex="-1"></a><span class="co"># Survey results, 56% indicated Blue:</span></span>
<span id="cb357-5"><a href="statistical-inference.html#cb357-5" tabindex="-1"></a>X_avg <span class="ot">&lt;-</span> <span class="fl">0.56</span></span>
<span id="cb357-6"><a href="statistical-inference.html#cb357-6" tabindex="-1"></a></span>
<span id="cb357-7"><a href="statistical-inference.html#cb357-7" tabindex="-1"></a><span class="co"># Standard error estimation</span></span>
<span id="cb357-8"><a href="statistical-inference.html#cb357-8" tabindex="-1"></a>SE_est <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(X_avg <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> X_avg)<span class="sc">/</span>total)</span>
<span id="cb357-9"><a href="statistical-inference.html#cb357-9" tabindex="-1"></a>SE_est</span>
<span id="cb357-10"><a href="statistical-inference.html#cb357-10" tabindex="-1"></a><span class="co">#&gt; [1] 0.01496663</span></span>
<span id="cb357-11"><a href="statistical-inference.html#cb357-11" tabindex="-1"></a></span>
<span id="cb357-12"><a href="statistical-inference.html#cb357-12" tabindex="-1"></a><span class="co"># Margin of error, MoE</span></span>
<span id="cb357-13"><a href="statistical-inference.html#cb357-13" tabindex="-1"></a>MoE <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> SE_est</span>
<span id="cb357-14"><a href="statistical-inference.html#cb357-14" tabindex="-1"></a>MoE</span>
<span id="cb357-15"><a href="statistical-inference.html#cb357-15" tabindex="-1"></a><span class="co">#&gt; [1] 0.02993326</span></span></code></pre></div>
<p>With this we would have that from a sample of 1100 people, we have estimated 56% voting intention for the Blue party with a margin of error of <span class="math inline">\(+- 2.99\%\)</span>.</p>
<p>Finally, let’s see examples of the different surveys conducted in April and early May 2020 to measure voting intentions in the US.</p>
<p><img src="assets/images/04-statistics/polls-us.png" alt="Table of US election polls from April-May 2020 showing sample sizes, margins of error, and candidate percentages" width="80%" style="display: block; margin: auto;" /></p>
<p>We see as columns:</p>
<p>The table columns provide key details about each survey. The <strong>Poll</strong> column identifies the surveying company, while <strong>Date</strong> indicates when the survey was conducted. <strong>Sample</strong> shows the number of respondents, which varies by pollster, and <strong>MoE</strong> represents the margin of error. The <strong>Candidates</strong> columns display the backing for each presidential contender (note that percentages may not sum to 100% due to checking for blank or null votes). Finally, <strong>Spread</strong> estimates the lead one candidate holds over the other.</p>
<p>If, on the other hand, we ask ourselves why larger surveys are not done, for example 50,000 people, the reason is that:</p>
<p>You might wonder why we rely on smaller samples instead of surveying, say, 50,000 people. The primary constraint is cost; reaching such a large audience is prohibitively expensive. Furthermore, parameter estimation is inherently theoretical—providing a razor-thin margin of error implies a false sense of absolute certainty. In reality, voter opinions are fluid, no survey is perfectly random (often missing rural populations), and respondents who claim they will vote might ultimately stay home.</p>
</div>
<div id="confidence-intervals" class="section level3 hasAnchor" number="9.6.2">
<h3><span class="header-section-number">9.6.2</span> Confidence Intervals<a href="statistical-inference.html#confidence-intervals" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Confidence intervals are a very useful concept widely used by Data Scientists. However, it is nothing more than another way of expressing what we have already learned so far.</p>
<p>And it is that a <strong>confidence interval</strong> of 95% tells us that there is a 95% probability that the interval we generate includes the parameter <span class="math inline">\(p\)</span> that we want to estimate. This is nothing more than another way of indicating that we have to build an interval considering the margin of error, that is two standard errors around our sample mean.</p>
<p>For the sample of 1100 people we saw in the previous section, we reported an estimate of 56% with a margin of error of <span class="math inline">\(+- 2.99\%\)</span>.</p>
<p>If we now want to use <strong>confidence intervals</strong> in our language we would say: We estimate 56% for the Blue party with a confidence interval of 95%. This confidence interval goes from 53% to 59%.</p>
</div>
</div>
<div id="spread-estimation" class="section level2 hasAnchor" number="9.7">
<h2><span class="header-section-number">9.7</span> Spread Estimation<a href="statistical-inference.html#spread-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Although we are interested in estimating the proportion that the Blue party would obtain <span class="math inline">\(p\)</span>, sometimes it is more useful to know the difference (by how much it wins/loses). For example, when we have two parties in the second round of elections not only do we have votes for Blue and Red, but also blank/spoiled. Also, in regular elections we have more than one pollster doing several surveys. So one could give 45% for Blue, 41% for Red. While another can give 41% for Blue and 38% for Red, etc. If we compare surveys, rather than knowing the exact percentage it is more useful to know by how much the blue party wins, since if we see that in all, for example it wins by 4%, with a tiny standard error, then <span class="math inline">\(p\)</span> would not matter much. Only with the difference data we could take get an idea of who will win.</p>
<p>This difference is called <em>spread</em>. We had defined that the voting intention for the Blue party was <span class="math inline">\(p\)</span> and for the red party <span class="math inline">\(1-p\)</span>. So what we would expect to obtain for the difference would be <span class="math inline">\(p - (1-p)\)</span>, that is <span class="math inline">\(2p - 1\)</span>.</p>
<p>Standard error of the <em>spread</em>:</p>
<p><span class="math inline">\(SE[spread]=2\sqrt{\frac{p(1-p) }{n}}\)</span></p>
<p>We see that the standard error is twice the standard error of the average, which depends on <span class="math inline">\(p\)</span>, and we have already found previously an estimation to not depend on <span class="math inline">\(p\)</span> but on the mean of our sample. So we will use:</p>
<p><span class="math inline">\(\hat{SE[spread]}=2\sqrt{\frac{\overline{X}(1-\overline{X}) }{n}}\)</span></p>
<p>Let’s see with an example these concepts. Let’s study the 2016 US elections. In this case we have multiple pollsters, conducting multiple surveys months prior to elections, and mainly two parties competing for president.</p>
<p>We are going to use the <code>polls_us_election_2016</code> data frame included in the <code>dslabs</code> library which includes data from multiple surveys conducted for the 2016 US elections between Hillary Clinton and Donald Trump. The first thing we will do is explore the data:</p>
<div class="sourceCode" id="cb358"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb358-1"><a href="statistical-inference.html#cb358-1" tabindex="-1"></a><span class="fu">library</span>(dslabs)</span>
<span id="cb358-2"><a href="statistical-inference.html#cb358-2" tabindex="-1"></a><span class="fu">head</span>(polls_us_election_2016)</span></code></pre></div>
<p>As we see, we do not have the standard error, nor the confidence interval. So we will proceed to make some mutations applying the formulas learned so far focusing on the voting intention for Hillary Clinton.</p>
<div class="sourceCode" id="cb359"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb359-1"><a href="statistical-inference.html#cb359-1" tabindex="-1"></a>surveys <span class="ot">&lt;-</span> polls_us_election_2016 <span class="sc">|&gt;</span></span>
<span id="cb359-2"><a href="statistical-inference.html#cb359-2" tabindex="-1"></a>  <span class="fu">filter</span>(state <span class="sc">==</span> <span class="st">&quot;U.S.&quot;</span>) <span class="sc">|&gt;</span> </span>
<span id="cb359-3"><a href="statistical-inference.html#cb359-3" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">X_avg =</span> rawpoll_clinton<span class="sc">/</span><span class="dv">100</span>) <span class="sc">|&gt;</span></span>
<span id="cb359-4"><a href="statistical-inference.html#cb359-4" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">SE_prom =</span> <span class="fu">sqrt</span>((X_avg<span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span>X_avg))<span class="sc">/</span>samplesize)) <span class="sc">|&gt;</span></span>
<span id="cb359-5"><a href="statistical-inference.html#cb359-5" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">inferior =</span> X_avg <span class="sc">-</span> <span class="dv">2</span><span class="sc">*</span>SE_prom,</span>
<span id="cb359-6"><a href="statistical-inference.html#cb359-6" tabindex="-1"></a>         <span class="at">superior =</span> X_avg <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>SE_prom) <span class="sc">|&gt;</span></span>
<span id="cb359-7"><a href="statistical-inference.html#cb359-7" tabindex="-1"></a>  <span class="fu">select</span>(pollster, enddate, X_avg, SE_prom, inferior, superior)</span>
<span id="cb359-8"><a href="statistical-inference.html#cb359-8" tabindex="-1"></a></span>
<span id="cb359-9"><a href="statistical-inference.html#cb359-9" tabindex="-1"></a><span class="co"># First 5 rows</span></span>
<span id="cb359-10"><a href="statistical-inference.html#cb359-10" tabindex="-1"></a>surveys <span class="sc">|&gt;</span> </span>
<span id="cb359-11"><a href="statistical-inference.html#cb359-11" tabindex="-1"></a>  <span class="fu">head</span>(<span class="dv">5</span>)</span>
<span id="cb359-12"><a href="statistical-inference.html#cb359-12" tabindex="-1"></a><span class="co">#&gt;                   pollster    enddate  X_avg     SE_prom  inferior  superior</span></span>
<span id="cb359-13"><a href="statistical-inference.html#cb359-13" tabindex="-1"></a><span class="co">#&gt; 1 ABC News/Washington Post 2016-11-06 0.4700 0.010592790 0.4488144 0.4911856</span></span>
<span id="cb359-14"><a href="statistical-inference.html#cb359-14" tabindex="-1"></a><span class="co">#&gt; 2  Google Consumer Surveys 2016-11-07 0.3803 0.002978005 0.3743440 0.3862560</span></span>
<span id="cb359-15"><a href="statistical-inference.html#cb359-15" tabindex="-1"></a><span class="co">#&gt; 3                    Ipsos 2016-11-06 0.4200 0.010534681 0.3989306 0.4410694</span></span>
<span id="cb359-16"><a href="statistical-inference.html#cb359-16" tabindex="-1"></a><span class="co">#&gt; 4                   YouGov 2016-11-07 0.4500 0.008204286 0.4335914 0.4664086</span></span>
<span id="cb359-17"><a href="statistical-inference.html#cb359-17" tabindex="-1"></a><span class="co">#&gt; 5         Gravis Marketing 2016-11-06 0.4700 0.003869218 0.4622616 0.4777384</span></span></code></pre></div>
<p>For example, IPSOS in a survey published on 11/06/16 estimated 42% voting intention for Clinton with a 95% confidence interval in a range going from 39.89% to 44.10%.</p>
<p>Does this data mean that they estimated she would lose? No, given that in this case we are using real data the proportion of votes for Clinton with those for Trump will not sum 100%. In fact, on actual election day Clinton obtained 48.2% and Trump 46.1% of total votes cast. That is real <span class="math inline">\(p\)</span> was 48.2%.</p>
<p>What we could calculate is how many of these pollsters guessed right in their estimation. That is, if in their confidence intervals is the <span class="math inline">\(p=48.2\%\)</span> that Clinton finally obtained. To do this, we will add a column <code>guessed_right</code> (guessed_right) with the validation of whether it is in the confidence interval and then use <code>summarize()</code> to calculate the percentage of surveys that guessed right.</p>
<div class="sourceCode" id="cb360"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb360-1"><a href="statistical-inference.html#cb360-1" tabindex="-1"></a>surveys <span class="sc">|&gt;</span> </span>
<span id="cb360-2"><a href="statistical-inference.html#cb360-2" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">guessed_right =</span> inferior <span class="sc">&lt;=</span> <span class="fl">0.482</span> <span class="sc">&amp;</span> <span class="fl">0.482</span> <span class="sc">&lt;=</span> superior) <span class="sc">|&gt;</span> </span>
<span id="cb360-3"><a href="statistical-inference.html#cb360-3" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="fu">mean</span>(guessed_right))</span>
<span id="cb360-4"><a href="statistical-inference.html#cb360-4" tabindex="-1"></a><span class="co">#&gt;   mean(guessed_right)</span></span>
<span id="cb360-5"><a href="statistical-inference.html#cb360-5" tabindex="-1"></a><span class="co">#&gt; 1           0.2802893</span></span></code></pre></div>
<p>Only 28% of published surveys published confidence intervals that included <span class="math inline">\(p\)</span>. This, among many other reasons, because at the beginning there are many more undecided who finally decide in the last weeks.</p>
<p>Let’s analyze now how many guessed right in the spread. It could be that even though they did not estimate exact <span class="math inline">\(p\)</span> the difference did remain over time. To do this, let’s add to our surveys the column <code>spread</code> with the difference of votes:</p>
<div class="sourceCode" id="cb361"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb361-1"><a href="statistical-inference.html#cb361-1" tabindex="-1"></a>surveys_spread <span class="ot">&lt;-</span> polls_us_election_2016 <span class="sc">|&gt;</span></span>
<span id="cb361-2"><a href="statistical-inference.html#cb361-2" tabindex="-1"></a>  <span class="fu">filter</span>(state <span class="sc">==</span> <span class="st">&quot;U.S.&quot;</span>) <span class="sc">|&gt;</span> </span>
<span id="cb361-3"><a href="statistical-inference.html#cb361-3" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">spread =</span> (rawpoll_clinton <span class="sc">-</span> rawpoll_trump)<span class="sc">/</span><span class="dv">100</span>)</span></code></pre></div>
<p>And now we are going to do a trick calculating the spread of our sample:</p>
<p><span class="math inline">\(spread=2*\overline{X}-1\)</span></p>
<p>We can transform this formula:</p>
<p><span class="math inline">\(spread-1=2*\overline{X}\)</span></p>
<p><span class="math inline">\(\frac{spread-1}{2}=\overline{X}\)</span></p>
<p>Or what is the same:</p>
<p><span class="math inline">\(\overline{X}=\frac{spread-1}{2}\)</span></p>
<p>This formula gives us an approximation of how much <span class="math inline">\(\overline{X}\)</span> would be transformed to a scale of 0 to 100%. With it, let’s calculate the standard error and the confidence interval:</p>
<div class="sourceCode" id="cb362"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb362-1"><a href="statistical-inference.html#cb362-1" tabindex="-1"></a>surveys_spread <span class="ot">&lt;-</span> surveys_spread <span class="sc">|&gt;</span> </span>
<span id="cb362-2"><a href="statistical-inference.html#cb362-2" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">X_avg =</span> (spread <span class="sc">+</span> <span class="dv">1</span>)<span class="sc">/</span><span class="dv">2</span>) <span class="sc">|&gt;</span></span>
<span id="cb362-3"><a href="statistical-inference.html#cb362-3" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">SE_spread =</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">sqrt</span>((X_avg<span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span>X_avg))<span class="sc">/</span>samplesize)) <span class="sc">|&gt;</span></span>
<span id="cb362-4"><a href="statistical-inference.html#cb362-4" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">inferior =</span> spread <span class="sc">-</span> <span class="dv">2</span><span class="sc">*</span>SE_spread,</span>
<span id="cb362-5"><a href="statistical-inference.html#cb362-5" tabindex="-1"></a>         <span class="at">superior =</span> spread <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>SE_spread) <span class="sc">|&gt;</span></span>
<span id="cb362-6"><a href="statistical-inference.html#cb362-6" tabindex="-1"></a>  <span class="fu">select</span>(pollster, enddate, spread, SE_spread, inferior, superior)</span>
<span id="cb362-7"><a href="statistical-inference.html#cb362-7" tabindex="-1"></a></span>
<span id="cb362-8"><a href="statistical-inference.html#cb362-8" tabindex="-1"></a><span class="co"># First 5 rows</span></span>
<span id="cb362-9"><a href="statistical-inference.html#cb362-9" tabindex="-1"></a>surveys_spread <span class="sc">|&gt;</span> </span>
<span id="cb362-10"><a href="statistical-inference.html#cb362-10" tabindex="-1"></a>  <span class="fu">head</span>(<span class="dv">5</span>)</span>
<span id="cb362-11"><a href="statistical-inference.html#cb362-11" tabindex="-1"></a><span class="co">#&gt;                   pollster    enddate spread   SE_spread     inferior</span></span>
<span id="cb362-12"><a href="statistical-inference.html#cb362-12" tabindex="-1"></a><span class="co">#&gt; 1 ABC News/Washington Post 2016-11-06 0.0400 0.021206832 -0.002413664</span></span>
<span id="cb362-13"><a href="statistical-inference.html#cb362-13" tabindex="-1"></a><span class="co">#&gt; 2  Google Consumer Surveys 2016-11-07 0.0234 0.006132712  0.011134575</span></span>
<span id="cb362-14"><a href="statistical-inference.html#cb362-14" tabindex="-1"></a><span class="co">#&gt; 3                    Ipsos 2016-11-06 0.0300 0.021334733 -0.012669466</span></span>
<span id="cb362-15"><a href="statistical-inference.html#cb362-15" tabindex="-1"></a><span class="co">#&gt; 4                   YouGov 2016-11-07 0.0400 0.016478037  0.007043926</span></span>
<span id="cb362-16"><a href="statistical-inference.html#cb362-16" tabindex="-1"></a><span class="co">#&gt; 5         Gravis Marketing 2016-11-06 0.0400 0.007746199  0.024507601</span></span>
<span id="cb362-17"><a href="statistical-inference.html#cb362-17" tabindex="-1"></a><span class="co">#&gt;     superior</span></span>
<span id="cb362-18"><a href="statistical-inference.html#cb362-18" tabindex="-1"></a><span class="co">#&gt; 1 0.08241366</span></span>
<span id="cb362-19"><a href="statistical-inference.html#cb362-19" tabindex="-1"></a><span class="co">#&gt; 2 0.03566542</span></span>
<span id="cb362-20"><a href="statistical-inference.html#cb362-20" tabindex="-1"></a><span class="co">#&gt; 3 0.07266947</span></span>
<span id="cb362-21"><a href="statistical-inference.html#cb362-21" tabindex="-1"></a><span class="co">#&gt; 4 0.07295607</span></span>
<span id="cb362-22"><a href="statistical-inference.html#cb362-22" tabindex="-1"></a><span class="co">#&gt; 5 0.05549240</span></span></code></pre></div>
<p>Now let’s calculate how many of these pollsters guessed right in their estimation. That is, if in their confidence intervals is the real value of <span class="math inline">\(spread=48.2\%-46.1\%=2.1\%\)</span> that Clinton finally obtained spread. To do this, we will add the column <code>guessed_right</code> and then <code>summarize()</code>.</p>
<div class="sourceCode" id="cb363"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb363-1"><a href="statistical-inference.html#cb363-1" tabindex="-1"></a>surveys_spread <span class="sc">|&gt;</span> </span>
<span id="cb363-2"><a href="statistical-inference.html#cb363-2" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">guessed_right =</span> inferior <span class="sc">&lt;=</span> <span class="fl">0.021</span> <span class="sc">&amp;</span> <span class="fl">0.021</span> <span class="sc">&lt;=</span> superior) <span class="sc">|&gt;</span> </span>
<span id="cb363-3"><a href="statistical-inference.html#cb363-3" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="fu">mean</span>(guessed_right))</span>
<span id="cb363-4"><a href="statistical-inference.html#cb363-4" tabindex="-1"></a><span class="co">#&gt;   mean(guessed_right)</span></span>
<span id="cb363-5"><a href="statistical-inference.html#cb363-5" tabindex="-1"></a><span class="co">#&gt; 1           0.6735986</span></span></code></pre></div>
<p>In this case we see how 67.3% of the time, surveys correctly estimated the difference in votes favorable to Clinton.</p>
<p>As a clarification, final reminder of this case, even though Clinton obtained more votes she did not win the elections because the US system is different and not necessarily if you win in votes you obtain the presidency.</p>
</div>
<div id="estimates-outside-election-polls" class="section level2 hasAnchor" number="9.8">
<h2><span class="header-section-number">9.8</span> Estimates Outside Election Polls<a href="statistical-inference.html#estimates-outside-election-polls" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We have used election polls to understand statistical inference concepts. However, most Data Scientists are not related to voting intention estimation calculations. That does not mean we will not use those concepts. The central limit theorem not only works in election polls. What it means is that we will use some slightly different formulas that apply to more daily life cases.</p>
<p>From what we have learned so far the main change is the formula to calculate the standard error. We will use instead the standard deviation <span class="math inline">\(\sigma\)</span> of the sample to calculate the standard error:</p>
<p><span class="math inline">\(SE[\overline{X}]=\frac{\sigma}{\sqrt{n}}\)</span></p>
<p>Where <span class="math inline">\(\overline{X}\)</span> is the average of our random sample and <span class="math inline">\(n\)</span> is the sample size.</p>
<div id="example-estimating-average-height" class="section level3 hasAnchor" number="9.8.1">
<h3><span class="header-section-number">9.8.1</span> Example: Estimating Average Height<a href="statistical-inference.html#example-estimating-average-height" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s apply this to a real-world scenario. Suppose we want to estimate the average height of adult males in a population, but we can only measure a sample of 50 people.</p>
<p>Using the <code>heights</code> dataset from <code>dslabs</code>, let’s simulate this process:</p>
<div class="sourceCode" id="cb364"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb364-1"><a href="statistical-inference.html#cb364-1" tabindex="-1"></a><span class="co"># Load the heights data</span></span>
<span id="cb364-2"><a href="statistical-inference.html#cb364-2" tabindex="-1"></a><span class="fu">data</span>(heights)</span>
<span id="cb364-3"><a href="statistical-inference.html#cb364-3" tabindex="-1"></a></span>
<span id="cb364-4"><a href="statistical-inference.html#cb364-4" tabindex="-1"></a><span class="co"># Our &quot;population&quot; (in reality, we wouldn&#39;t have access to this)</span></span>
<span id="cb364-5"><a href="statistical-inference.html#cb364-5" tabindex="-1"></a>population <span class="ot">&lt;-</span> heights <span class="sc">|&gt;</span></span>
<span id="cb364-6"><a href="statistical-inference.html#cb364-6" tabindex="-1"></a>  <span class="fu">filter</span>(sex <span class="sc">==</span> <span class="st">&quot;Male&quot;</span>) <span class="sc">|&gt;</span></span>
<span id="cb364-7"><a href="statistical-inference.html#cb364-7" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">height_cm =</span> height <span class="sc">*</span> <span class="fl">2.54</span>) <span class="sc">|&gt;</span>  <span class="co"># Convert to cm</span></span>
<span id="cb364-8"><a href="statistical-inference.html#cb364-8" tabindex="-1"></a>  <span class="fu">pull</span>(height_cm)</span>
<span id="cb364-9"><a href="statistical-inference.html#cb364-9" tabindex="-1"></a></span>
<span id="cb364-10"><a href="statistical-inference.html#cb364-10" tabindex="-1"></a><span class="co"># True population parameters (unknown in real life)</span></span>
<span id="cb364-11"><a href="statistical-inference.html#cb364-11" tabindex="-1"></a>true_mean <span class="ot">&lt;-</span> <span class="fu">mean</span>(population)</span>
<span id="cb364-12"><a href="statistical-inference.html#cb364-12" tabindex="-1"></a>true_sd <span class="ot">&lt;-</span> <span class="fu">sd</span>(population)</span>
<span id="cb364-13"><a href="statistical-inference.html#cb364-13" tabindex="-1"></a></span>
<span id="cb364-14"><a href="statistical-inference.html#cb364-14" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;True population mean:&quot;</span>, <span class="fu">round</span>(true_mean, <span class="dv">2</span>), <span class="st">&quot;cm</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb364-15"><a href="statistical-inference.html#cb364-15" tabindex="-1"></a><span class="co">#&gt; True population mean: 176.06 cm</span></span>
<span id="cb364-16"><a href="statistical-inference.html#cb364-16" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;True population SD:&quot;</span>, <span class="fu">round</span>(true_sd, <span class="dv">2</span>), <span class="st">&quot;cm</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb364-17"><a href="statistical-inference.html#cb364-17" tabindex="-1"></a><span class="co">#&gt; True population SD: 9.17 cm</span></span></code></pre></div>
<p>Now let’s take a random sample and build a 95% confidence interval:</p>
<div class="sourceCode" id="cb365"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb365-1"><a href="statistical-inference.html#cb365-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)  <span class="co"># For reproducibility</span></span>
<span id="cb365-2"><a href="statistical-inference.html#cb365-2" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">50</span>  <span class="co"># Sample size</span></span>
<span id="cb365-3"><a href="statistical-inference.html#cb365-3" tabindex="-1"></a></span>
<span id="cb365-4"><a href="statistical-inference.html#cb365-4" tabindex="-1"></a><span class="co"># Take a random sample</span></span>
<span id="cb365-5"><a href="statistical-inference.html#cb365-5" tabindex="-1"></a>sample_heights <span class="ot">&lt;-</span> <span class="fu">sample</span>(population, n, <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb365-6"><a href="statistical-inference.html#cb365-6" tabindex="-1"></a></span>
<span id="cb365-7"><a href="statistical-inference.html#cb365-7" tabindex="-1"></a><span class="co"># Sample statistics</span></span>
<span id="cb365-8"><a href="statistical-inference.html#cb365-8" tabindex="-1"></a>sample_mean <span class="ot">&lt;-</span> <span class="fu">mean</span>(sample_heights)</span>
<span id="cb365-9"><a href="statistical-inference.html#cb365-9" tabindex="-1"></a>sample_sd <span class="ot">&lt;-</span> <span class="fu">sd</span>(sample_heights)</span>
<span id="cb365-10"><a href="statistical-inference.html#cb365-10" tabindex="-1"></a></span>
<span id="cb365-11"><a href="statistical-inference.html#cb365-11" tabindex="-1"></a><span class="co"># Standard error of the mean</span></span>
<span id="cb365-12"><a href="statistical-inference.html#cb365-12" tabindex="-1"></a>SE <span class="ot">&lt;-</span> sample_sd <span class="sc">/</span> <span class="fu">sqrt</span>(n)</span>
<span id="cb365-13"><a href="statistical-inference.html#cb365-13" tabindex="-1"></a></span>
<span id="cb365-14"><a href="statistical-inference.html#cb365-14" tabindex="-1"></a><span class="co"># 95% Confidence interval (2 standard errors)</span></span>
<span id="cb365-15"><a href="statistical-inference.html#cb365-15" tabindex="-1"></a>ci_lower <span class="ot">&lt;-</span> sample_mean <span class="sc">-</span> <span class="dv">2</span> <span class="sc">*</span> SE</span>
<span id="cb365-16"><a href="statistical-inference.html#cb365-16" tabindex="-1"></a>ci_upper <span class="ot">&lt;-</span> sample_mean <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> SE</span>
<span id="cb365-17"><a href="statistical-inference.html#cb365-17" tabindex="-1"></a></span>
<span id="cb365-18"><a href="statistical-inference.html#cb365-18" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Sample mean:&quot;</span>, <span class="fu">round</span>(sample_mean, <span class="dv">2</span>), <span class="st">&quot;cm</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb365-19"><a href="statistical-inference.html#cb365-19" tabindex="-1"></a><span class="co">#&gt; Sample mean: 176.97 cm</span></span>
<span id="cb365-20"><a href="statistical-inference.html#cb365-20" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Standard error:&quot;</span>, <span class="fu">round</span>(SE, <span class="dv">2</span>), <span class="st">&quot;cm</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb365-21"><a href="statistical-inference.html#cb365-21" tabindex="-1"></a><span class="co">#&gt; Standard error: 1.44 cm</span></span>
<span id="cb365-22"><a href="statistical-inference.html#cb365-22" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;95% CI: [&quot;</span>, <span class="fu">round</span>(ci_lower, <span class="dv">2</span>), <span class="st">&quot;,&quot;</span>, <span class="fu">round</span>(ci_upper, <span class="dv">2</span>), <span class="st">&quot;] cm</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb365-23"><a href="statistical-inference.html#cb365-23" tabindex="-1"></a><span class="co">#&gt; 95% CI: [ 174.09 , 179.84 ] cm</span></span>
<span id="cb365-24"><a href="statistical-inference.html#cb365-24" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Does CI contain true mean?&quot;</span>, ci_lower <span class="sc">&lt;=</span> true_mean <span class="sc">&amp;</span> true_mean <span class="sc">&lt;=</span> ci_upper, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb365-25"><a href="statistical-inference.html#cb365-25" tabindex="-1"></a><span class="co">#&gt; Does CI contain true mean? TRUE</span></span></code></pre></div>
<p>Let’s visualize this with a Monte Carlo simulation to verify that our confidence intervals work as expected:</p>
<div class="sourceCode" id="cb366"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb366-1"><a href="statistical-inference.html#cb366-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb366-2"><a href="statistical-inference.html#cb366-2" tabindex="-1"></a>n_simulations <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb366-3"><a href="statistical-inference.html#cb366-3" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">50</span></span>
<span id="cb366-4"><a href="statistical-inference.html#cb366-4" tabindex="-1"></a></span>
<span id="cb366-5"><a href="statistical-inference.html#cb366-5" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb366-6"><a href="statistical-inference.html#cb366-6" tabindex="-1"></a>  <span class="at">sim_id =</span> <span class="dv">1</span><span class="sc">:</span>n_simulations</span>
<span id="cb366-7"><a href="statistical-inference.html#cb366-7" tabindex="-1"></a>) <span class="sc">|&gt;</span></span>
<span id="cb366-8"><a href="statistical-inference.html#cb366-8" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb366-9"><a href="statistical-inference.html#cb366-9" tabindex="-1"></a>    <span class="at">sample_data =</span> <span class="fu">map</span>(sim_id, <span class="sc">~</span> <span class="fu">sample</span>(population, n, <span class="at">replace =</span> <span class="cn">FALSE</span>)),</span>
<span id="cb366-10"><a href="statistical-inference.html#cb366-10" tabindex="-1"></a>    <span class="at">sample_mean =</span> <span class="fu">map_dbl</span>(sample_data, mean),</span>
<span id="cb366-11"><a href="statistical-inference.html#cb366-11" tabindex="-1"></a>    <span class="at">sample_sd =</span> <span class="fu">map_dbl</span>(sample_data, sd),</span>
<span id="cb366-12"><a href="statistical-inference.html#cb366-12" tabindex="-1"></a>    <span class="at">SE =</span> sample_sd <span class="sc">/</span> <span class="fu">sqrt</span>(n),</span>
<span id="cb366-13"><a href="statistical-inference.html#cb366-13" tabindex="-1"></a>    <span class="at">ci_lower =</span> sample_mean <span class="sc">-</span> <span class="dv">2</span> <span class="sc">*</span> SE,</span>
<span id="cb366-14"><a href="statistical-inference.html#cb366-14" tabindex="-1"></a>    <span class="at">ci_upper =</span> sample_mean <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> SE,</span>
<span id="cb366-15"><a href="statistical-inference.html#cb366-15" tabindex="-1"></a>    <span class="at">contains_true =</span> ci_lower <span class="sc">&lt;=</span> true_mean <span class="sc">&amp;</span> true_mean <span class="sc">&lt;=</span> ci_upper</span>
<span id="cb366-16"><a href="statistical-inference.html#cb366-16" tabindex="-1"></a>  )</span>
<span id="cb366-17"><a href="statistical-inference.html#cb366-17" tabindex="-1"></a></span>
<span id="cb366-18"><a href="statistical-inference.html#cb366-18" tabindex="-1"></a><span class="co"># Proportion of CIs that contain the true mean</span></span>
<span id="cb366-19"><a href="statistical-inference.html#cb366-19" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Proportion of 95% CIs containing true mean:&quot;</span>, </span>
<span id="cb366-20"><a href="statistical-inference.html#cb366-20" tabindex="-1"></a>    <span class="fu">round</span>(<span class="fu">mean</span>(results<span class="sc">$</span>contains_true), <span class="dv">3</span>), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb366-21"><a href="statistical-inference.html#cb366-21" tabindex="-1"></a><span class="co">#&gt; Proportion of 95% CIs containing true mean: 0.956</span></span></code></pre></div>
<p>This confirms that approximately 95% of our confidence intervals capture the true population mean—exactly as the theory predicts!</p>
</div>
</div>
<div id="exercises-13" class="section level2 hasAnchor" number="9.9">
<h2><span class="header-section-number">9.9</span> Exercises<a href="statistical-inference.html#exercises-13" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The most common data a Data Scientist manages comes from people, some attribute/characteristic of them. In these exercises we are going to use the heights data frame that we already used for other purposes in previous chapters.</p>
<div class="sourceCode" id="cb367"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb367-1"><a href="statistical-inference.html#cb367-1" tabindex="-1"></a><span class="fu">library</span>(dslabs)</span>
<span id="cb367-2"><a href="statistical-inference.html#cb367-2" tabindex="-1"></a><span class="fu">data</span>(heights)</span></code></pre></div>
<ol start="82" style="list-style-type: decimal">
<li>Create a vector named <code>x</code> to extract the height data for all individuals in the dataset. Then, report the population’s average and standard deviation, ensuring you convert the heights to meters first.</li>
</ol>
<details>
<summary type="button">
Solution
</summary>
<div class="sourceCode" id="cb368"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb368-1"><a href="statistical-inference.html#cb368-1" tabindex="-1"></a>x <span class="ot">&lt;-</span> heights <span class="sc">|&gt;</span> </span>
<span id="cb368-2"><a href="statistical-inference.html#cb368-2" tabindex="-1"></a>  <span class="fu">filter</span>(sex <span class="sc">==</span> <span class="st">&quot;Male&quot;</span>) <span class="sc">|&gt;</span></span>
<span id="cb368-3"><a href="statistical-inference.html#cb368-3" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">height_m =</span> height<span class="sc">/</span><span class="fl">39.37</span>) <span class="sc">|&gt;</span> </span>
<span id="cb368-4"><a href="statistical-inference.html#cb368-4" tabindex="-1"></a>  <span class="fu">pull</span>(height_m)</span>
<span id="cb368-5"><a href="statistical-inference.html#cb368-5" tabindex="-1"></a></span>
<span id="cb368-6"><a href="statistical-inference.html#cb368-6" tabindex="-1"></a><span class="co"># Average of the population</span></span>
<span id="cb368-7"><a href="statistical-inference.html#cb368-7" tabindex="-1"></a><span class="fu">mean</span>(x)</span>
<span id="cb368-8"><a href="statistical-inference.html#cb368-8" tabindex="-1"></a></span>
<span id="cb368-9"><a href="statistical-inference.html#cb368-9" tabindex="-1"></a><span class="co"># Standard deviation</span></span>
<span id="cb368-10"><a href="statistical-inference.html#cb368-10" tabindex="-1"></a><span class="fu">sd</span>(x)</span></code></pre></div>
</details>
<blockquote>
<p>Mathematically we use <code>x</code> in lowercase to refer to our total population and <code>X</code> to refer to a random sample. We will denote the population mean as <span class="math inline">\(\mu\)</span> and the population standard deviation as <span class="math inline">\(\sigma\)</span></p>
</blockquote>
<blockquote>
<p>Most of the time we will not have access to the mean and standard deviation of the population because it is very large and highly expensive.</p>
</blockquote>
<ol start="83" style="list-style-type: decimal">
<li>Assume we cannot access the entire population and can only obtain a random sample of 100 people. Simulate this by creating a random sample with replacement from <code>x</code>, storing the values in a vector called <code>X</code>. Using this sample data, construct a 95% confidence interval to estimate the population average.</li>
</ol>
<details>
<summary type="button">
Solution
</summary>
<div class="sourceCode" id="cb369"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb369-1"><a href="statistical-inference.html#cb369-1" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb369-2"><a href="statistical-inference.html#cb369-2" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">sample</span>(x, N, <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb369-3"><a href="statistical-inference.html#cb369-3" tabindex="-1"></a></span>
<span id="cb369-4"><a href="statistical-inference.html#cb369-4" tabindex="-1"></a><span class="co"># Expected value</span></span>
<span id="cb369-5"><a href="statistical-inference.html#cb369-5" tabindex="-1"></a><span class="fu">mean</span>(X)</span>
<span id="cb369-6"><a href="statistical-inference.html#cb369-6" tabindex="-1"></a></span>
<span id="cb369-7"><a href="statistical-inference.html#cb369-7" tabindex="-1"></a><span class="co"># Standard error</span></span>
<span id="cb369-8"><a href="statistical-inference.html#cb369-8" tabindex="-1"></a>se <span class="ot">&lt;-</span> <span class="fu">sd</span>(X)<span class="sc">/</span><span class="fu">sqrt</span>(N)</span>
<span id="cb369-9"><a href="statistical-inference.html#cb369-9" tabindex="-1"></a></span>
<span id="cb369-10"><a href="statistical-inference.html#cb369-10" tabindex="-1"></a><span class="co"># 95% confidence interval is approx. 2 se</span></span>
<span id="cb369-11"><a href="statistical-inference.html#cb369-11" tabindex="-1"></a>ic <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">mean</span>(X) <span class="sc">-</span> <span class="dv">2</span><span class="sc">*</span>se, <span class="fu">mean</span>(X) <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>se)</span></code></pre></div>
</details>
<ol start="84" style="list-style-type: decimal">
<li>Validate your estimation method using a Monte Carlo simulation. Repeat the sampling process 10,000 times and calculate the percentage of expected confidence intervals that successfully capture the true population mean.</li>
</ol>
<details>
<summary type="button">
Solution
</summary>
<div class="sourceCode" id="cb370"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb370-1"><a href="statistical-inference.html#cb370-1" tabindex="-1"></a>true_mean <span class="ot">&lt;-</span> <span class="fu">mean</span>(x)</span>
<span id="cb370-2"><a href="statistical-inference.html#cb370-2" tabindex="-1"></a></span>
<span id="cb370-3"><a href="statistical-inference.html#cb370-3" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb370-4"><a href="statistical-inference.html#cb370-4" tabindex="-1"></a>n_times <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb370-5"><a href="statistical-inference.html#cb370-5" tabindex="-1"></a></span>
<span id="cb370-6"><a href="statistical-inference.html#cb370-6" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2025</span>)</span>
<span id="cb370-7"><a href="statistical-inference.html#cb370-7" tabindex="-1"></a>simulation <span class="ot">&lt;-</span> <span class="fu">replicate</span>(n_times, {</span>
<span id="cb370-8"><a href="statistical-inference.html#cb370-8" tabindex="-1"></a>  X <span class="ot">&lt;-</span> <span class="fu">sample</span>(x, N, <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb370-9"><a href="statistical-inference.html#cb370-9" tabindex="-1"></a>  se <span class="ot">&lt;-</span> <span class="fu">sd</span>(X)<span class="sc">/</span><span class="fu">sqrt</span>(N)</span>
<span id="cb370-10"><a href="statistical-inference.html#cb370-10" tabindex="-1"></a>  lower <span class="ot">&lt;-</span> <span class="fu">mean</span>(X) <span class="sc">-</span> <span class="dv">2</span><span class="sc">*</span>se</span>
<span id="cb370-11"><a href="statistical-inference.html#cb370-11" tabindex="-1"></a>  upper <span class="ot">&lt;-</span> <span class="fu">mean</span>(X) <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>se</span>
<span id="cb370-12"><a href="statistical-inference.html#cb370-12" tabindex="-1"></a>  <span class="fu">between</span>(true_mean, lower, upper)</span>
<span id="cb370-13"><a href="statistical-inference.html#cb370-13" tabindex="-1"></a>})</span>
<span id="cb370-14"><a href="statistical-inference.html#cb370-14" tabindex="-1"></a></span>
<span id="cb370-15"><a href="statistical-inference.html#cb370-15" tabindex="-1"></a><span class="fu">mean</span>(simulation)</span></code></pre></div>
</details>

</div>
</div>



            </section>

          </div>
        </div>
      </div>
<a href="continuous-probabilities.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="introduction-1.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": true,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": null
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": null,
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "section"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
