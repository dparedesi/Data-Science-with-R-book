[["index.html", "Data Science with R Data Analysis and prediction algorithms with R. Third Edition Preface Support This Work Stay Connected", " Data Science with R Data Analysis and prediction algorithms with R. Third Edition Author: Mg. Daniel Paredes Inilupu 2025-12-25 Preface Welcome to the third edition of Data Science with R! In an era where data-driven decisions shape industries from healthcare to finance, mastering R gives you the power to extract insights, build predictive models, and communicate findings effectively. This book has evolved from personal learning notes into a comprehensive resource that takes you from fundamentals to advanced data science techniques using practical, hands-on exercises. This book is designed for beginners with no prior R experience who want a structured path into data science, as well as analysts looking to upgrade from spreadsheets to reproducible R workflows. It also serves students, professionals, and practitioners seeking to modernize their machine learning skills with tidymodels. Basic familiarity with statistics concepts is helpful but not required, as all code examples are self-contained and explained step-by-step. The third edition reflects the latest developments in the R ecosystem. We have updated everything to run on R 4.5.2 and RStudio 2025.09.2. A major shift in this edition is the full migration to tidymodels for machine learning and the adoption of modern tidyverse patterns, including the native pipe operator. We have also introduced entirely new topics such as Generative AI and LLM integration with R, AI-assisted coding workflows, ethics in data science, and enhanced text mining. You will also find expanded content on interactive visualization, deep learning with Keras and TensorFlow, big data processing with Sparklyr, and reproducible workflows using Git and GitHub. Each chapter builds on previous concepts, but you can also jump to topics of interest. If you are learning R from scratch, start with the Fundamentals in the first two chapters. Chapter 3 covers visualization with ggplot2, while Chapter 12 dives into building machine learning models. For those interested in working with Large Language Models, Chapter 14 covers Generative AI. Throughout the book, you will find hands-on exercises to test your understanding. Solutions are provided, but we encourage you to try them yourself first. Just like previous editions, many exercises are inspired by practical classroom experiences and activities from the Professional Certificate in Data Science1 by HarvardX. The code used to generate this book is available on GitHub, encouraging transparency and reproducibility. Support This Work Over 700 hours went into creating this resource. If you find it valuable, consider purchasing the PDF on Leanpub. Your purchase includes: Future updates at no extra cost Three months of direct Q&amp;A access with the author Support for keeping the web version free for everyone The web version available at bookdown2 seeks to democratize data science knowledge. Share it and let’s contribute together to freeing knowledge. Stay Connected This book has reached readers in Mexico, Colombia, Spain, Peru, Chile, and many other countries. I deeply thank readers of previous editions for their comments and suggestions, which have been fundamental to improving each version. If you have questions or suggestions, write to me at dparedesi@uni.pe. I usually respond within 48 hours. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. https://online-learning.harvard.edu/series/professional-certificate-data-science↩︎ https://bookdown.org/dparedesi/data-science-con-r/↩︎ "],["acknowledgments.html", "Acknowledgments Family Mentors and Inspirations Contributors", " Acknowledgments Family First and foremost, I want to express my profound gratitude to my wife, Desislava, for her invaluable emotional support during the countless hours I dedicated to this project. Her knowledge in R was also key—contributing solutions and perspectives that enriched this work. Mentors and Inspirations A special thanks to Rafael Irizarry3, a true benchmark in the R world, whose didactic approach to teaching advanced techniques significantly advanced my learning. I also extend my gratitude to the developers who, with their dedication, continue to create and maintain this wonderful language. My gratitude also goes to Briguit Reinaldo, CEO of Cedhinfo, whose tireless work brings the teaching of computer technologies to more people in Peru, inspiring many to explore new opportunities. Contributors Finally, I want to acknowledge the valuable contribution of engineering students from UNI who participated in grammatical review, paraphrasing, exercise creation, and topic proposals. Special thanks to Josep Agama4 and Andrés Espinoza5 for their fundamental contributions to the first three chapters. https://hsph.harvard.edu/profile/rafael-a-irizarry/↩︎ https://www.linkedin.com/in/josep-agama-749a61190/↩︎ https://www.linkedin.com/in/aespinozacontreras/↩︎ "],["introduction.html", "Introduction Why R? Installing R Installing RStudio RStudio Sections Testing Your Installation What’s Next?", " Introduction What You’ll Learn in This Chapter: Understand why R is an excellent choice for data science Install R and RStudio on your computer Navigate the RStudio interface confidently Write and execute your first R code Data science requires a multidisciplinary approach that combines statistics, programming, data mining, and domain expertise. This book is designed to help you develop those skills through practical, hands-on examples in R. Why R? R is a language created by statisticians for data analysis, making it an excellent choice for your data science journey. It is free and open-source, meaning there are no licensing costs and you can inspect how any function works under the hood. Its rich ecosystem includes over 19,000 packages on CRAN covering virtually any analytical task, from machine learning to bioinformatics. One of R’s strongest features is reproducibility, allowing you to write scripts once and share them with colleagues to obtain consistent results anywhere. It also offers best-in-class visualization through ggplot2 for publication-quality graphics and Shiny for interactive dashboards. Furthermore, you will find an active community with extensive documentation, tutorials, and a welcoming presence on social media. While Python is also popular in data science, R excels particularly in statistical analysis and data visualization. These are the core skills we will develop throughout this book, leveraging the tidyverse ecosystem to make data manipulation intuitive and readable. Installing R You can download R from the Comprehensive R Archive Network (CRAN). Search for CRAN on Google: On the CRAN page, select the version for your operating system—Linux, Mac OS X, or Windows: The following steps show the Windows installation process. The steps for Mac and Linux are similar—simply select your operating system and follow the corresponding download link. On the CRAN website, click on base to download the core R installation. This includes all the basic packages you need to get started. Later chapters will show you how to install additional packages. Click on the download link to get the latest stable version: Once downloaded, run the installer and follow the on-screen instructions. Installation Tip: When the installer asks about options, the default settings work perfectly for beginners. You can always customize your installation later. Installing RStudio Although you could start using R directly in the console, we recommend installing RStudio—an integrated development environment (IDE) that makes working with R significantly more productive and enjoyable. Search for RStudio on Google: You’ll see the Posit website (RStudio’s parent company). Click on DOWNLOAD in the upper right menu: Scroll down until you find the download options. Select the Free RStudio Desktop option: The page will display a download button optimized for your operating system. You can also choose from the list of all available installers below: Once the installer is downloaded, run it and follow the on-screen instructions. Having trouble? Installation issues are usually straightforward to resolve. Check the CRAN FAQ or search “R installation [your OS]” for community solutions. The RStudio Community is also an excellent resource. RStudio Sections When you start RStudio for the first time, you’ll see three main sections: One of the great advantages of R over point-and-click analysis software is that we can save our work as Scripts—text files containing R code that can be shared, version-controlled, and re-run at any time. To create a new Script, click on File → New File → R Script: This opens the fourth panel, giving you the complete RStudio layout: Let’s understand the layout. The Source Editor in the top-left is where you write and edit your R scripts. Think of it as your code notebook where you can save, organize, and run code. Below it is the Console (bottom-left), where code executes and results appear. You can type commands directly here for quick experiments. On the right side, the Environment/History panel (top-right) shows all variables and functions you have created in your current session, while the history tab tracks your commands. Finally, the Files/Plots/Help panel (bottom-right) serves multiple purposes, allowing you to browse files, view generated plots, and access package documentation. Essential Keyboard Shortcuts Mastering keyboard shortcuts will significantly speed up your workflow. The most commonly used command is running the current line or selection, which you can do by pressing Ctrl + Enter (or Cmd + Enter on Mac). To run the entire script, use Ctrl + Shift + S (Cmd + Shift + S). Creating a new script is as easy as pressing Ctrl + Shift + N (Cmd + Shift + N), and you should save your work frequently with Ctrl + S (Cmd + S). A shortcut specific to R is inserting the assignment operator (&lt;-), which is done with Alt + - (Option + -). Finally, you can quickly comment or uncomment lines using Ctrl + Shift + C (Cmd + Shift + C). Pro Tip: Press Alt + Shift + K (Windows/Linux) or Option + Shift + K (Mac) in RStudio to see the complete list of keyboard shortcuts. Testing Your Installation Let’s verify everything is working correctly. Go to the Console panel and calculate how much 13 multiplied by 265 is. Click on the console, type the following, and press Enter: 13 * 265 #&gt; [1] 3445 You should see 3445 as the result. Let’s understand the output format: Understanding R Output: The [1] before the result indicates this is the first element of the output. R can return multiple values, and the bracketed number helps you track which element you’re looking at. In this book, we use ## to distinguish R output from R code. Lines starting with ## show what you’ll see in your console after running the code. Now let’s try something more interesting. R isn’t just a calculator—it’s a powerful tool for working with data. Try this in your console: # Create a vector of values values &lt;- c(10, 20, 30, 40, 50) # Calculate the mean mean(values) #&gt; [1] 30 # How many elements are there? length(values) #&gt; [1] 5 You’ve just created your first data structure (a vector) and applied functions to it—fundamental concepts we’ll explore in depth in the next chapter. Writing Scripts While the console is great for quick experiments, scripts are essential for reproducible work. Try this in the Source Editor (not the console): # My First R Script # Calculating basic statistics # Create some data temperatures &lt;- c(22, 25, 23, 28, 30, 27, 24) # Calculate statistics mean(temperatures) # Average temperature max(temperatures) # Highest temperature min(temperatures) # Lowest temperature To execute code from the script: Run a single line: Place your cursor on the line and press Ctrl + Enter (or Cmd + Enter on Mac) Run selected lines: Highlight the lines you want to run and press Ctrl + Enter Run the entire script: Press Ctrl + Shift + S (or Cmd + Shift + S on Mac) Notice that lines starting with # are comments—R ignores them, but they’re invaluable for explaining your code to others (and to your future self!). Challenge: Test Your Setup Try these exercises to confirm everything works: Calculate sqrt(144) in the console (the square root of 144) Create a new script with three different calculations and save it as my_first_script.R Use the assignment operator (&lt;-) to store a value: my_number &lt;- 42 Print your stored value by typing my_number and pressing Enter What’s Next? Congratulations! You have successfully set up your R environment and written your first R code. You now understand why R is a powerful choice for data science and how to navigate the four panels of the RStudio interface. You have also learned to use the console, write scripts, and apply essential keyboard shortcuts. In the next chapter, we will dive into R Fundamentals, learning about objects, data types, vectors, and functions that form the foundation of all R programming. You will discover how R stores and manipulates data, setting the stage for the data analysis and visualization techniques to come. Let’s continue the journey! "],["objects.html", "Chapter 1 Objects 1.1 What are objects in R? 1.2 Variables: The first objects on your journey 1.3 Object types for complex data 1.4 The Universe of Objects in R 1.5 Exercises", " Chapter 1 Objects In the world of programming, an object is like a container that holds information. This information can be of different types: numbers, text, complex data, and even code. The important thing is that an object groups everything necessary to represent an entity or concept. In R, practically everything is an object. The variables we will use to store data, the functions we will use to process that data, and even the data itself, are objects. 1.1 What are objects in R? Imagine you are organizing your move to the United States. Each item you pack in a box (clothes, books, appliances) can be considered an object. Each object has characteristics that define it: a name, a type, a size, a weight, etc. In R, objects also have characteristics that define them. These characteristics are called attributes. For instance, every object has a Name so we can refer to it, and a Type that indicates what kind of data it contains (numeric, character, logical, etc.). Objects also have a Class defining their structure and behavior (such as vector, list, or data frame) and a Length indicating the number of elements they contain. 1.1.1 R as an object-oriented language R is an object-oriented programming language, meaning it relies on the concept of objects to organize and process information. This approach offers several advantages, such as Modularity, allowing us to divide a program into smaller, manageable parts. It also promotes Reusability, as objects can be used in different parts of the program or even in other projects. Furthermore, objects provide Encapsulation, hiding implementation details to facilitate their use and maintenance. 1.1.2 The power of abstraction The concept of an object allows us to abstract the complexity of the real world. Instead of thinking about the details of how data is stored and processed in computer memory, we can think in terms of objects representing real-world entities. For example, instead of thinking of a series of numbers representing the temperatures of different cities, we can think of a “temperatures” object containing all that information. This abstraction facilitates understanding and handling information, allowing us to focus on the logic of the problem we want to solve. 1.2 Variables: The first objects on your journey Before we start packing for our move to the United States, we need to know what things we will take. Each object we decide to take is represented in R as a variable. Think of variables as labels we put on each object. For example, we could use the variable state to save the name of the state we are moving to, or the variable num_suitcases to save the number of suitcases we will take. 1.2.1 Creating variables in R In R, we don’t need to declare a variable before using it. We simply assign it a value using the &lt;- symbol. Note: You might see the = symbol used for assignment in other programming languages or even in some R code. While = works in R, the &lt;- operator is the standard and idiomatic way to assign values to variables. It helps distinguish between assigning a value to a variable and passing arguments to a function (where = is always used). Example: # Assign the value &quot;California&quot; to the variable &quot;state&quot; state &lt;- &quot;California&quot; # Assign the value 5 to the variable &quot;num_suitcases&quot; num_suitcases &lt;- 5 To see the value we have saved in a variable, we simply type its name in the RStudio console and press Enter. Example: state &lt;- &quot;California&quot; state #&gt; [1] &quot;California&quot; When executing this code, you will see the value \"California\" appear in the console. 1.2.2 Operations with variables We can also use variables to perform operations. For example, if we want to calculate the total cost of our plane trip, we could use the variables ticket_price and num_people. Example: ticket_price &lt;- 300 num_people &lt;- 4 total_cost &lt;- ticket_price * num_people total_cost #&gt; [1] 1200 In this example, we first assign values to the variables ticket_price and num_people. Then, we multiply these variables to calculate the total_cost and display its value in the console. 1.2.3 Best practices for naming variables Watch out for capitalization! R is case-sensitive. If you create a variable called state and then try to access it as State, R will not find it. Descriptive names It is important to use descriptive names for variables, clearly indicating what information they contain. Instead of using variables like x or y, it is better to use names like ticket_price or num_suitcases. Rules for naming variables When naming your variables, remember that they can contain letters, numbers, and underscores (_), but they cannot start with a number or contain spaces. Also, keep in mind that R is case-sensitive, so capitalization matters. 1.2.4 Data types Variables in R can contain different types of data. Numeric variables are used for numbers, such as the population of a city or the cost of a plane ticket. Character variables store text, like the name of a state (“California”) or a city (“Los Angeles”). Logical variables represent binary truth values, TRUE or FALSE, which are useful for conditions, such as indicating whether we want to visit a specific city. 1.3 Object types for complex data The variables we have seen so far are very useful for storing individual information, such as the name of a city or the number of suitcases we will carry on our move. However, in the real world, we often need to work with more complex datasets. Imagine you want to save the names of all the cities you plan to visit on your trip to the United States. Would you have to create a variable for each city? That would be very tedious! Fortunately, R offers other types of objects that allow us to organize and manipulate information more efficiently. Let’s look at some of them: 1.3.1 Vectors: organizing information of the same type Vectors are like trains transporting a series of objects of the same type. They can be numbers, text, or logical values, but all elements of a vector must be of the same type. For example, we could use a vector to save the name of each state in the United States, or a vector to save the population of each state. Creating vectors: To create a vector, we can use the c() function (which stands for “combine”) and list the elements we want to include, separated by commas. # Create a vector with the names of some states states &lt;- c(&quot;California&quot;, &quot;Texas&quot;, &quot;Florida&quot;, &quot;New York&quot;) # Create a vector with the population of each state (in millions) population &lt;- c(39.2, 29.0, 21.4, 19.4) If we want to know the amount of data our vector has, its length, we will use the length() function. The class() function tells us the class of the object, that is, what type of data it contains. length(population) #&gt; [1] 4 class(states) #&gt; [1] &quot;character&quot; class(population) #&gt; [1] &quot;numeric&quot; We can use the names() function to assign names to the elements of a vector. This can be useful for identifying each element. names(population) &lt;- states population #&gt; California Texas Florida New York #&gt; 39.2 29.0 21.4 19.4 In addition to c(), there are other useful functions for creating vectors. The seq() function creates a sequence of numbers, allowing us to specify the start value, the end value, and the increment. # Create a vector with numbers from 1 to 10 numbers &lt;- seq(1, 10) # Create a vector with numbers from 2 to 20, by 2 even_numbers &lt;- seq(2, 20, by = 2) Another useful function is rep(), which repeats a value or a vector a specified number of times. # Create a vector with the value 1 repeated 5 times ones &lt;- rep(1, 5) # Create a vector with the sequence &quot;A&quot;, &quot;B&quot; repeated 3 times letters &lt;- rep(c(&quot;A&quot;, &quot;B&quot;), 3) # Output: &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; Accessing vector elements: Each element of a vector has a position, indicated by a number in brackets. The first element is at position 1, the second at position 2, and so on. # Show the first element of the &quot;states&quot; vector states[1] # Output: &quot;California&quot; # Show the third element of the &quot;population&quot; vector population[3] # Output: 21.4 We can also access multiple elements at once using the : operator. For example, to access elements from the second to the fourth of the states vector: states[2:4] #&gt; [1] &quot;Texas&quot; &quot;Florida&quot; &quot;New York&quot; Operations with vectors: We can perform mathematical operations with numeric vectors. For example, if we want to calculate the total population of the four states, we can use the + operator to sum the elements of the population vector. population &lt;- c(39.2, 29.0, 21.4, 19.4) population[1] + population[2] + population[3] + population[4] #&gt; [1] 109 If we want to perform the same operation more concisely, R allows us to sum all elements of a vector directly: population &lt;- c(39.2, 29.0, 21.4, 19.4) sum(population) #&gt; [1] 109 R also offers other tools for performing operations with vectors. For example, if we want to calculate the square root of the population of each state: sqrt(population) #&gt; [1] 6.260990 5.385165 4.626013 4.404543 In this case, the sqrt() function calculates the square root of each element of the population vector individually. This is possible because many functions in R are vectorized, meaning they can operate directly on vectors, element by element. Vectorized functions are very efficient as they avoid the need to write loops to process each element of the vector separately. We will explore functions in R and how to use them for more complex data analysis in greater depth later. Vector coercion: Unlike other programming languages, R tries to interpret or change a value when it encounters an error. For example, if we try to convert a character vector to numeric, R will convert the elements it can and replace the ones it cannot with NA. example &lt;- c(&quot;3&quot;, &quot;b&quot;, &quot;6&quot;, &quot;a&quot;, &quot;bridge&quot;, &quot;4&quot;) as.numeric(example) #&gt; Warning: NAs introduced by coercion #&gt; [1] 3 NA 6 NA NA 4 Sorting vectors: We can sort the elements of a vector using the sort() function. districts &lt;- c(&quot;Comas&quot;, &quot;Lince&quot;, &quot;Miraflores&quot;, &quot;Lurigancho&quot;, &quot;Chorrillos&quot;) sort(districts) #&gt; [1] &quot;Chorrillos&quot; &quot;Comas&quot; &quot;Lince&quot; &quot;Lurigancho&quot; &quot;Miraflores&quot; We can also order a vector using its indices with the order() function. This way, we get a vector with the positions the elements of the original vector would occupy if they were sorted. This can be useful when we want to sort a vector based on another vector or when we want to preserve the original vector without modifying it. indices &lt;- order(districts) # Output: 5 1 2 4 3 districts[indices] #&gt; [1] &quot;Chorrillos&quot; &quot;Comas&quot; &quot;Lince&quot; &quot;Lurigancho&quot; &quot;Miraflores&quot; NA in vectors: If a vector contains NA values, some operations may return NA. We can use the is.na() function to identify NA values and filter them. example_na &lt;- c(28, 3, 19, NA, 89, 45, NA, 86, 5, 18, 28, NA) example_no_na &lt;- example_na[!is.na(example_na)] mean(example_no_na) # Output: 38.66667 #&gt; [1] 35.66667 1.3.2 Lists: grouping objects of different types Lists are like containers that can hold different types of objects. Imagine a box where you can put clothes, books, tools, and any other object you need. In R, lists allow you to group diverse information into a single object. Creating lists: To create a list, we use the list() function and specify the elements we want to include, separated by commas. Each element can have a name, indicated with the = symbol. # Create a list with information about a city city_info &lt;- list(name = &quot;San Francisco&quot;, population = 880000, cost_of_living = 3.8, climate = &quot;Temperate&quot;) Accessing list elements: To access the elements of a list, we can use their names or their positions. # Access the &quot;name&quot; element of the &quot;city_info&quot; list city_info$name # Output: &quot;San Francisco&quot; # Access the second element of the &quot;city_info&quot; list city_info[[2]] # Output: 880000 1.3.3 Matrices: organizing data in rows and columns Matrices are like tables that organize information in rows and columns. All elements of a matrix must be of the same type. Creating matrices: To create a matrix, we use the matrix() function. We must specify the data we want to include, the number of rows (nrow), and the number of columns (ncol). # Create a matrix with distances between cities (in miles) city_distances &lt;- matrix(c(0, 2600, 2100, 950, 2600, 0, 1100, 2700, 2100, 1100, 0, 2100, 950, 2700, 2100, 0), nrow = 4, ncol = 4) city_distances #&gt; [,1] [,2] [,3] [,4] #&gt; [1,] 0 2600 2100 950 #&gt; [2,] 2600 0 1100 2700 #&gt; [3,] 2100 1100 0 2100 #&gt; [4,] 950 2700 2100 0 Accessing matrix elements: To access the elements of a matrix, we use brackets and specify the row and column of the element we want. # Access the element in row 1, column 3 of the &quot;city_distances&quot; matrix city_distances[1, 3] #&gt; [1] 2100 1.3.4 Arrays: multidimensional matrices Arrays are like matrices that have more than two dimensions. Imagine a matrix that, in addition to rows and columns, has depth. In R, arrays allow you to organize data in more complex structures. Creating arrays: To create an array, we use the array() function. # Create an array with maximum and minimum temperatures of # three cities during the summer months (June, July, August) temperatures &lt;- array(c(25, 28, 30, 22, 25, 28, # City 1 28, 20, 32, 25, 18, 30, # City 2 22, 25, 28, 18, 23, 25), # City 3 dim = c(3, 2, 3)) # 3 cities, 2 temperatures (max/min), 3 months temperatures #&gt; , , 1 #&gt; #&gt; [,1] [,2] #&gt; [1,] 25 22 #&gt; [2,] 28 25 #&gt; [3,] 30 28 #&gt; #&gt; , , 2 #&gt; #&gt; [,1] [,2] #&gt; [1,] 28 25 #&gt; [2,] 20 18 #&gt; [3,] 32 30 #&gt; #&gt; , , 3 #&gt; #&gt; [,1] [,2] #&gt; [1,] 22 18 #&gt; [2,] 25 23 #&gt; [3,] 28 25 Accessing array elements: To access the elements of an array, we use brackets and specify the position of the element in each dimension. # Access the maximum temperature of city 2 in July temperatures[2, 1, 2] #&gt; [1] 20 1.3.5 Factors: representing categorical data Factors are a special type of object used to represent categorical data, that is, data that can be classified into groups. For example, the type of climate (“warm”, “temperate”, “cold”), the region of a country (“north”, “south”, “east”, “west”), or the type of housing (“house”, “apartment”). Creating factors: To create a factor, we use the factor() function. # Create a factor with climate types of different cities climate_types &lt;- factor(c(&quot;Temperate&quot;, &quot;Warm&quot;, &quot;Cold&quot;)) Levels of a factor: The different values a factor can take are called levels. In the previous example, the levels of the climate_types factor are “Temperate”, “Warm”, and “Cold”. Utility of factors: Factors are very useful for data analysis, as they allow grouping and comparing information efficiently. For example, we could use the climate_types factor to analyze how the cost of living varies in cities with different climates. 1.4 The Universe of Objects in R Throughout this chapter, we have explored the different types of objects inhabiting the R universe. From the simplest variables to multidimensional arrays, each object plays an important role in building our data analyses. 1.4.1 Philosophy of objects in R In R, everything is an object. This philosophy has profound implications for how code is written and executed. By treating everything as an object, R promotes consistency, modularity, and reuse. Objects allow us to encapsulate information and behavior, facilitating code organization and maintenance. Furthermore, the ability to create our own objects gives us great power to model and solve complex problems. By understanding the philosophy of objects in R, we can make the most of the language’s capabilities for data analysis. 1.4.2 Comparison with other languages While many modern programming languages use the object-oriented paradigm, R has a particular approach. In languages like Python or Java, creating classes and objects is a fundamental part of the language. In R, while it is possible to create classes and objects, the language focuses more on the use of functions to manipulate and transform data. This difference is due in part to R’s history as a language for statistical analysis. In this context, functions are a natural tool for performing calculations and analyses. 1.5 Exercises Now that you know the different types of objects in R, it’s time to put your knowledge to the test. Create four variables to plan your move. Define city_name with the city you would like to move to, population with its number of inhabitants, and distance with the kilometers from your current location. Also, create a logical variable want_to_live_there indicating if you truly want to live there. Solution city_name &lt;- &quot;Seattle&quot; population &lt;- 724745 distance &lt;- 8340 # Approximate distance from Lima, Peru want_to_live_there &lt;- TRUE Create a vector called nearby_cities containing the names of three cities near the city you chose in the previous exercise. Solution nearby_cities &lt;- c(&quot;Tacoma&quot;, &quot;Bellevue&quot;, &quot;Everett&quot;) Construct a list called my_list that groups different types of information about yourself. It should include your name, your age, a vector with your three favorite colors, and a logical value indicating if you simplify like chocolate. Solution my_list &lt;- list(name = &quot;Ana&quot;, age = 30, favorite_colors = c(&quot;blue&quot;, &quot;green&quot;, &quot;red&quot;), likes_chocolate = TRUE) Create a matrix called monthly_expenses containing your estimated monthly expenses in the following categories: Category January February March Housing Transport Food Entertainment Complete the matrix with numerical values. Solution monthly_expenses &lt;- matrix(c(1500, 1500, 1500, # Housing 300, 250, 350, # Transport 500, 400, 550, # Food 200, 150, 250), # Entertainment nrow = 4, ncol = 3, dimnames = list(c(&quot;Housing&quot;, &quot;Transport&quot;, &quot;Food&quot;, &quot;Entertainment&quot;), c(&quot;January&quot;, &quot;February&quot;, &quot;March&quot;))) Create a factor called climate_types containing the names of the different climate types in the United States (you can use “Temperate”, “Warm”, “Cold”, etc.). Assign labels to the factor levels to make them more descriptive (for example, “Cold climate”, “Temperate climate”, etc.). Solution climate_types &lt;- factor(c(&quot;Temperate&quot;, &quot;Warm&quot;, &quot;Cold&quot;, &quot;Warm&quot;, &quot;Temperate&quot;), levels = c(&quot;Cold&quot;, &quot;Temperate&quot;, &quot;Warm&quot;), labels = c(&quot;Cold climate&quot;, &quot;Temperate climate&quot;, &quot;Warm climate&quot;)) climate_types Create a vector called cities_to_visit with the names of 5 cities you would like to visit in the United States. Then, create another vector called days_per_city with the number of days you would like to spend in each city. Finally, create a third vector called daily_cost with the estimated daily cost in each city (in dollars). Solution cities_to_visit &lt;- c(&quot;New York&quot;, &quot;Los Angeles&quot;, &quot;Chicago&quot;, &quot;San Francisco&quot;, &quot;Miami&quot;) days_per_city &lt;- c(5, 4, 3, 6, 2) daily_cost &lt;- c(200, 180, 150, 220, 170) Create a vector called max_temperatures with the average maximum temperatures (in Celsius) of the cities you want to visit during the month of July. Then, create a vector called min_temperatures with the average minimum temperatures. Finally, create a matrix containing these two vectors as columns, and name the rows with the names of the cities. Solution max_temperatures &lt;- c(29, 28, 27, 22, 31) # Max temperatures in July min_temperatures &lt;- c(21, 18, 19, 15, 25) # Min temperatures in July # Create the matrix temperatures &lt;- matrix(c(max_temperatures, min_temperatures), nrow = 5, ncol = 2, dimnames = list(cities_to_visit, c(&quot;Maximum&quot;, &quot;Minimum&quot;))) temperatures #&gt; Maximum Minimum #&gt; New York 29 21 #&gt; Los Angeles 28 18 #&gt; Chicago 27 19 #&gt; San Francisco 22 15 #&gt; Miami 31 25 Create a three-dimensional array containing information about the climate of the cities you want to visit. The first dimension should represent the cities, the second dimension should represent the months of the year (“January”, “February”, …, “December”), and the third dimension should represent two variables: “Temperature” and “Precipitation”. You can use dummy values to fill the array. Solution # Create an array with dimensions 5 cities x 12 months x 2 variables climate &lt;- array(dim = c(5, 12, 2), dimnames = list(cities_to_visit, month.name, c(&quot;Temperature&quot;, &quot;Precipitation&quot;))) # Fill the array with dummy values (example) climate[,, &quot;Temperature&quot;] &lt;- sample(10:35, 60, replace = TRUE) # Temperatures between 10 and 35 degrees climate[,, &quot;Precipitation&quot;] &lt;- sample(0:100, 60, replace = TRUE) # Precipitation between 0 and 100 mm climate #&gt; , , Temperature #&gt; #&gt; January February March April May June July August September #&gt; New York 11 35 13 25 28 14 13 26 23 #&gt; Los Angeles 29 24 16 18 26 23 23 13 29 #&gt; Chicago 35 33 19 29 14 13 12 32 35 #&gt; San Francisco 23 29 25 16 19 16 33 20 29 #&gt; Miami 18 34 22 12 14 29 30 25 34 #&gt; October November December #&gt; New York 19 26 28 #&gt; Los Angeles 31 13 19 #&gt; Chicago 32 28 29 #&gt; San Francisco 31 35 14 #&gt; Miami 18 26 21 #&gt; #&gt; , , Precipitation #&gt; #&gt; January February March April May June July August September #&gt; New York 43 8 70 44 55 100 29 95 89 #&gt; Los Angeles 40 26 30 55 10 82 92 46 40 #&gt; Chicago 33 85 51 69 38 15 4 84 91 #&gt; San Francisco 74 30 62 1 20 76 90 20 29 #&gt; Miami 42 29 5 87 14 25 37 12 58 #&gt; October November December #&gt; New York 38 100 11 #&gt; Los Angeles 71 60 85 #&gt; Chicago 37 42 73 #&gt; San Francisco 54 95 86 #&gt; Miami 62 15 27 Imagine you have a vector with the daily maximum temperatures of a US city for a year. Create a program that, using only the concepts learned in this chapter (variables, vectors, matrices, arrays, and factors), identifies the longest streak of consecutive days with maximum temperatures above a given threshold (for example, 25 degrees Celsius). Solution This exercise requires efficient vector handling and algorithmic logic to identify the longest streak. Here is a possible solution: # Create a vector with dummy maximum temperatures for a year temperatures &lt;- sample(10:35, 365, replace = TRUE) # Define the temperature threshold threshold &lt;- 25 # Create a logical vector indicating if the temperature exceeds the threshold hot_days &lt;- temperatures &gt; threshold # Initialize variables to track the longest streak current_streak &lt;- 0 longest_streak &lt;- 0 start_longest_streak &lt;- 0 # Iterate through the hot days vector for (i in 1:length(hot_days)) { if (hot_days[i]) { current_streak &lt;- current_streak + 1 } else { if (current_streak &gt; longest_streak) { longest_streak &lt;- current_streak start_longest_streak &lt;- i - current_streak } current_streak &lt;- 0 } } # Show the longest streak and its position cat(&quot;The longest streak of hot days is:&quot;, longest_streak, &quot;\\n&quot;) #&gt; The longest streak of hot days is: 5 cat(&quot;Starts on day:&quot;, start_longest_streak, &quot;\\n&quot;) #&gt; Starts on day: 41 This code uses a for loop to traverse the hot days vector and two variables (current_streak and longest_streak) to track the longest streak. Imagine you have a vector with the daily stock prices of a company for a year. Create a program that, using only the concepts learned in this chapter, determines the time period in which you could have bought and sold the shares to obtain the maximum profit. Assume you can only buy and sell once. Solution This exercise is a variant of the classic “maximize stock profit” problem. Solving it optimally can be complex, but with the concepts from this chapter, we can create an algorithm that finds a solution (though not necessarily the optimal one). # Create a vector with dummy stock prices for a year prices &lt;- sample(50:150, 365, replace = TRUE) # Initialize variables to track max profit max_profit &lt;- 0 buy_day &lt;- 1 sell_day &lt;- 1 # Iterate through the prices vector for (i in 1:(length(prices) - 1)) { for (j in (i + 1):length(prices)) { profit &lt;- prices[j] - prices[i] if (profit &gt; max_profit) { max_profit &lt;- profit buy_day &lt;- i sell_day &lt;- j } } } # Show max profit and buy/sell days cat(&quot;Maximum profit:&quot;, max_profit, &quot;\\n&quot;) #&gt; Maximum profit: 100 cat(&quot;Buy day:&quot;, buy_day, &quot;\\n&quot;) #&gt; Buy day: 193 cat(&quot;Sell day:&quot;, sell_day, &quot;\\n&quot;) #&gt; Sell day: 309 This code uses two nested for loops to compare all possible pairs of buy and sell days. "],["functions.html", "Chapter 2 Functions 2.1 Introduction to the world of functions 2.2 Anatomy of a function 2.3 Mastering the use of functions 2.4 Higher-order functions 2.5 Closures: functions with memory 2.6 Debugging and error handling: solving the mysteries of your code 2.7 Exercises", " Chapter 2 Functions 2.1 Introduction to the world of functions In the previous chapter, we explored the different types of objects we can use to store and organize information in R. We learned to create variables, vectors, lists, matrices, and arrays, and saw how to access their elements and perform operations with them. Now, in this chapter, we will go a step further and delve into the world of functions. Functions are one of the fundamental pillars of programming in R, allowing us to perform more complex tasks and automate our work. 2.1.1 What are functions? Imagine a coffee machine. You provide the ingredients (water, coffee, sugar), and the machine performs a series of steps to produce a cup of coffee. Similarly, a function in R is a set of instructions that receives input data (the arguments) and performs a series of operations to produce a result (the return value). Functions allow us to encapsulate a set of instructions into a single block of code, facilitating reuse and code organization. Instead of writing the same instructions over and over again, we can create a function that performs them for us. 2.1.2 Why use functions? Functions offer several advantages, starting with Reusability, which allows us to use the same logic in different parts of our code or across projects. They also improve Organization by breaking code into logical blocks, and enhance Readability by keeping scripts concise. Finally, functions provide Abstraction, hiding complex implementation details so we can focus on the problem logic. 2.1.3 First functions: exploring basic R functions R includes a large number of predefined functions. For instance, sum() calculates the total of a vector’s elements, while mean() computes their arithmetic average. numbers &lt;- c(1, 2, 3, 4, 5) sum(numbers) # Output: 15 #&gt; [1] 15 temperatures &lt;- c(25, 28, 26, 29, 27) mean(temperatures) # Output: 27 #&gt; [1] 27 Other common functions include round(), which limits the number of decimal places, and length(), which tells us how many elements a vector contains. pi # Output: 3.141593 #&gt; [1] 3.141593 round(pi, 2) # Output: 3.14 #&gt; [1] 3.14 cities &lt;- c(&quot;New York&quot;, &quot;Los Angeles&quot;, &quot;Chicago&quot;) length(cities) # Output: 3 #&gt; [1] 3 These are just a few of the many predefined functions that R offers. As we progress through the book, we will explore more functions and learn how to use them to perform more complex data analysis. 2.2 Anatomy of a function In the previous section, we saw what functions are and why they are so useful in programming. Now, we are going to delve into the structure of a function, so you can create your own functions and automate tasks in your data analysis. 2.2.1 Arguments: the ingredients of the function To make a cup of coffee, you need ingredients: water, coffee, and maybe sugar or milk. Similarly, functions in R need arguments to do their job. Arguments are the input data that the function uses to perform its operations. For example, the sum() function needs a vector of numbers as an argument to calculate the sum of its elements. numbers &lt;- c(1, 2, 3, 4, 5) sum(numbers) # Output: 15 #&gt; [1] 15 A function’s arguments are specified in parentheses after the function name. If a function requires multiple arguments, they are separated by commas. For example, imagine we want to create a function to calculate the total cost of a plane trip. This function might need the ticket_price, the num_people traveling, and an optional discount (such as a reduction for students or senior citizens). The function could be called calculate_vacation_cost and would be used as follows: calculate_vacation_cost(ticket_price = 300, num_people = 2, discount = 0.1) In this case, we are passing three arguments to the function: ticket_price with value 300, num_people with value 2, and discount with value 0.1 (representing a 10% discount). 2.2.2 Body: the instructions of the function The body of a function is the set of instructions that are executed when the function is called. These instructions can be any valid R code: variable assignments, mathematical operations, conditionals, loops, calls to other functions, etc. The body of a function is defined within curly braces {}. For example, the body of the function calculate_trip_cost could be: calculate_vacation_cost &lt;- function(ticket_price, num_people, discount = 0) { total_cost &lt;- ticket_price * num_people * (1 - discount) return(total_cost) } In this body, first the total cost of the trip is calculated by multiplying the ticket price by the number of people and by (1 minus the discount). Then, return() is used to return the total_cost. Note that in the function definition, the argument discount has a default value of 0. This means that if we do not specify a value for discount when calling the function, the value 0 will be used. For example, if we do not specify a value for discount, the function uses the default value 0, and the total cost is 600: # Call the function without specifying the discount calculate_vacation_cost(ticket_price = 300, num_people = 2) #&gt; [1] 600 If we want to apply a discount, we can specify it when calling the function: calculate_vacation_cost(ticket_price = 300, num_people = 2, discount = 0.1) #&gt; [1] 540 In this case, the total cost is 540, since a 10% discount is applied. 2.2.3 Return value: the result of the function The return value is the result the function produces after executing its instructions. It can be a simple value (a number, text, a logical value) or a more complex object (a vector, a list, a data frame). In R, the return value is specified with the return() function. If return() is not used, the function will return the result of the last expression evaluated in the body. In the calculate_vacation_cost example, the return value is the total_cost of the trip, which is a number. 2.2.4 Examples: creating simple functions step by step Let’s see an example of how to create a simple function that converts degrees Celsius to Fahrenheit: celsius_to_fahrenheit &lt;- function(celsius) { fahrenheit &lt;- (celsius * 9 / 5) + 32 return(fahrenheit) } In this example, celsius_to_fahrenheit is the name of the function, and celsius is its argument representing the input temperature. Inside the body, the function calculates the equivalent Fahrenheit value using the formula (celsius * 9 / 5) + 32 and stores it in the variable fahrenheit, which is then sent back as the result using return(). Now we can use our function to convert temperatures: celsius_to_fahrenheit(0) # Output: 32 #&gt; [1] 32 celsius_to_fahrenheit(100) # Output: 212 #&gt; [1] 212 Congratulations! You just created your first function in R. As we progress through the chapter, you will learn to create more complex functions and use them to solve real-world problems. 2.3 Mastering the use of functions We have already seen how to create simple functions with basic arguments, including the possibility of assigning default values. Now, we will explore even more advanced techniques to master the use of functions and write more flexible and efficient code. 2.3.1 Functions with a variable number of arguments (...): Adapting to different situations Sometimes, we don’t know beforehand how many arguments a function will receive. For these cases, R offers us the possibility of defining functions with a variable number of arguments using the three dots (...). For example, the sum() function can receive any number of arguments: sum(1, 2, 3) #&gt; [1] 6 sum(1, 2, 3, 4, 5) # Output: 15 #&gt; [1] 15 We can use the three dots (...) to create our own functions that accept a variable number of arguments. For example, a function that calculates the average of several numbers: calculate_average &lt;- function(...) { numbers &lt;- c(...) average &lt;- mean(numbers) return(average) } calculate_average(1, 2, 3) #&gt; [1] 2 calculate_average(1, 2, 3, 4, 5) #&gt; [1] 3 In this example, the three dots (...) capture all the arguments passed to the function and store them in the numbers vector. Then, the function calculates the average of the numbers in the vector and returns it as a result. It is important to note that when using ..., we lose the ability to name the arguments individually. However, we gain flexibility by being able to pass a variable number of arguments to the function. 2.3.2 Variable scope: local and global variables The scope of a variable refers to the part of the code where the variable is accessible. In R, variables defined inside a function have a local scope, meaning they are only accessible within the function. Variables defined outside any function have a global scope, meaning they are accessible from anywhere in the code. For example, in the function calculate_average, the variable numbers has a local scope: calculate_average &lt;- function(...) { numbers &lt;- c(...) average &lt;- mean(numbers) return(average) } If we try to access the variable numbers outside the function, we will get an error: numbers # Error: object &#39;numbers&#39; not found This is because numbers only exists inside the calculate_average function. When the function finishes executing, the local variables defined inside it cease to exist. On the other hand, if we define a variable outside any function, it will be a global variable: conversion_rate &lt;- 0.621371 # Conversion rate from kilometers to miles We can access the conversion_rate variable from anywhere in the code, even inside a function: kilometers_to_miles &lt;- function(kilometers) { miles &lt;- kilometers * conversion_rate return(miles) } kilometers_to_miles(100) #&gt; [1] 62.1371 It is important to keep variable scope in mind when writing functions to avoid errors and confusion. If a variable is not defined in the current scope (local), R will look in the global scope. If the variable is not found in any scope, an error will occur. For example, imagine we want to calculate the total cost of a trip, including the cost of the plane ticket, accommodation, and other expenses. We can create a function that receives these expenses as arguments and calculates the total cost: calculate_trip_cost &lt;- function(ticket, accommodation, other_expenses) { total_cost &lt;- ticket + accommodation + other_expenses return(total_cost) } If we call this function with expense values, we get the total cost: calculate_trip_cost(ticket = 300, accommodation = 500, other_expenses = 100) #&gt; [1] 900 Now, imagine we want to apply a tax to the total cost. We could define a global variable tax_rate: tax_rate &lt;- 0.16 Warning: Relying on global variables inside a function (like tax_rate in the example below) is generally considered bad practice. It makes the function dependent on the external environment, which can lead to unexpected errors if the global variable changes or doesn’t exist. It is better to pass all necessary values as arguments to the function. And then modify the function to include the tax: calculate_trip_cost &lt;- function(ticket, accommodation, other_expenses) { total_cost &lt;- ticket + accommodation + other_expenses total_cost &lt;- total_cost * (1 + tax_rate) return(total_cost) } When calling the function again, the total cost will include the tax: calculate_trip_cost(ticket = 300, accommodation = 500, other_expenses = 100) #&gt; [1] 1044 In this case, the calculate_trip_cost function can access the global variable tax_rate because it is not defined locally within the function. If we try to use a variable that is not defined in any scope, we will get an error: calculate_trip_cost &lt;- function(ticket, accommodation, other_expenses) { total_cost &lt;- ticket + accommodation + other_expenses + tip return(total_cost) } calculate_trip_cost(ticket = 300, accommodation = 500, other_expenses = 100) # Error: object &#39;tip&#39; not found In this case, the variable tip is not defined either locally or globally, so the function cannot access it. It is important to understand the concept of variable scope to write functions that work correctly and avoid errors. 2.3.3 Examples: functions to calculate taxes, discounts, etc. Functions are very useful for automating repetitive tasks, such as calculating taxes, discounts, or converting units. Let’s look at some examples with different levels of difficulty: Calculating shipping cost for a package calculate_shipping_cost &lt;- function(weight, destination) { if (destination == &quot;local&quot;) { cost &lt;- 5 + 0.1 * weight } else if (destination == &quot;national&quot;) { cost &lt;- 10 + 0.2 * weight } else { # destination == &quot;international&quot; cost &lt;- 20 + 0.5 * weight } return(cost) } # Usage example package_weight &lt;- 2.5 # Weight in kilograms destination &lt;- &quot;national&quot; shipping_cost &lt;- calculate_shipping_cost(package_weight, destination) shipping_cost #&gt; [1] 10.5 In this example, the calculate_shipping_cost() function calculates the shipping cost of a package based on its weight and destination. The function uses a conditional structure (if-else if-else) to apply different shipping rates depending on the destination. Calculating income tax with brackets calculate_income_tax &lt;- function(income) { if (income &lt;= 10000) { rate &lt;- 0.10 } else if (income &lt;= 20000) { rate &lt;- 0.15 } else { rate &lt;- 0.20 } tax &lt;- income * rate return(tax) } # Usage example income &lt;- 15000 tax &lt;- calculate_income_tax(income) tax #&gt; [1] 2250 In this example, the calculate_income_tax() function calculates a person’s income tax based on their income. The function uses a conditional structure (if-else if-else) to apply different tax rates according to the income bracket. Calculating trip cost with multiple options calculate_trip_cost &lt;- function(origin_city, destination_city, transport_type = &quot;plane&quot;, num_people = 1, hotel = NULL, daily_expenses = 100, trip_duration = 7) { # Calculate transport cost if (transport_type == &quot;plane&quot;) { transport_cost &lt;- 300 * num_people # Base price per person } else if (transport_type == &quot;train&quot;) { transport_cost &lt;- 150 * num_people # Base price per person } else { transport_cost &lt;- 0 # Assuming transport is by own car } # Calculate accommodation cost if (!is.null(hotel)) { accommodation_cost &lt;- hotel$price * trip_duration } else { accommodation_cost &lt;- 0 # Assuming staying not at a hotel } # Calculate other expenses other_expenses &lt;- daily_expenses * num_people * trip_duration # Calculate total cost total_cost &lt;- transport_cost + accommodation_cost + other_expenses return(total_cost) } # Usage example trip_cost_1 &lt;- calculate_trip_cost(origin_city = &quot;Lima&quot;, destination_city = &quot;New York&quot;, transport_type = &quot;plane&quot;, num_people = 2) trip_cost_2 &lt;- calculate_trip_cost(origin_city = &quot;Lima&quot;, destination_city = &quot;Los Angeles&quot;, transport_type = &quot;train&quot;, num_people = 3, hotel = list(price = 150), daily_expenses = 120, trip_duration = 10) trip_cost_1 #&gt; [1] 2000 trip_cost_2 #&gt; [1] 5550 2.4 Higher-order functions In previous sections, we explored how to create and use functions in R. Now, let’s delve into a more advanced concept: higher-order functions. Higher-order functions are those that can receive other functions as arguments or return a function as a result. This type of function allows us to write more flexible and expressive code, and they are a powerful tool for data analysis. 2.4.1 lapply() and sapply(): applying a function to each element Imagine you have a list with information about several US cities, and you want to calculate the population density of each city. You could write a for loop to iterate through the list and calculate the density of each city separately. However, R offers a more efficient and elegant way to do this: the lapply() function. lapply() (which stands for “list apply”) takes two arguments: A list (or a vector). A function to be applied to each element of the list. lapply() applies the function to each element of the list and returns a new list with the results. # Create a list with information about cities cities &lt;- list( New_York = list(population = 8.4e6, area = 783.8), Los_Angeles = list(population = 3.9e6, area = 1302.0), Chicago = list(population = 2.7e6, area = 606.1) ) # Function to calculate population density calculate_density &lt;- function(city) { density &lt;- city$population / city$area return(density) } # Calculate population density of each city densities &lt;- lapply(cities, calculate_density) densities #&gt; $New_York #&gt; [1] 10717.02 #&gt; #&gt; $Los_Angeles #&gt; [1] 2995.392 #&gt; #&gt; $Chicago #&gt; [1] 4454.71 In this example, lapply() applies the calculate_density function to each element of the cities list and returns a new list densities with the population density of each city. The sapply() function is similar to lapply(), but tries to simplify the result. If the result is a list of vectors of the same type and length, sapply() returns a vector or a matrix. # Calculate population density of each city with sapply() densities &lt;- sapply(cities, calculate_density) densities #&gt; New_York Los_Angeles Chicago #&gt; 10717.020 2995.392 4454.710 In this case, sapply() returns a vector with population densities. 2.4.2 apply(): applying a function to rows or columns The apply() function allows us to apply a function to the rows or columns of a matrix or array. It’s like having a tool that allows us to go through each row or column of our data table and perform a specific calculation on each one. For example, if we have a matrix with the maximum and minimum temperatures of different cities, we can use apply() to calculate the average temperature of each city. # Create a matrix with temperatures temperatures &lt;- matrix(c(25, 18, 30, 22, 35, 28), nrow = 3, ncol = 2, dimnames = list(c(&quot;New York&quot;, &quot;Los Angeles&quot;, &quot;Chicago&quot;), c(&quot;Maximum&quot;, &quot;Minimum&quot;))) # Calculate average temperature of each city average_temperatures &lt;- apply(temperatures, 1, mean) average_temperatures #&gt; New York Los Angeles Chicago #&gt; 23.5 26.5 29.0 In this example, apply() applies the mean() function to each row of the temperatures matrix (the argument 1 indicates that the function should be applied to rows) and returns a vector with the average temperatures of each city. If we wanted to calculate the maximum or minimum temperature among all cities, we could use apply() with the max() or min() function, respectively, and apply it to columns (using argument 2). # Calculate maximum temperature among all cities maximum_temperature &lt;- apply(temperatures, 2, max) maximum_temperature #&gt; Maximum Minimum #&gt; 30 35 2.4.3 mapply(): applying a function to multiple arguments The mapply() function allows us to apply a function to multiple arguments in parallel. It’s like having a tool that allows us to take several sets of data and apply the same operation to each corresponding set. For example, imagine we have two vectors: one with the names of different US cities and another with their respective populations. We want to create a new vector containing the phrase “The city of [city name] has a population of [population] inhabitants”. We could use mapply() to apply a function combining the city name and its population to each pair of elements from the vectors. # Create vectors with city names and populations cities &lt;- c(&quot;New York&quot;, &quot;Los Angeles&quot;, &quot;Chicago&quot;) populations &lt;- c(8.4e6, 3.9e6, 2.7e6) # Function to create the phrase create_phrase &lt;- function(city, population) { phrase &lt;- paste(&quot;The city of&quot;, city, &quot;has a population of&quot;, population, &quot;inhabitants.&quot;) return(phrase) } # Create vector with phrases city_phrases &lt;- mapply(create_phrase, cities, populations) city_phrases #&gt; New York #&gt; &quot;The city of New York has a population of 8400000 inhabitants.&quot; #&gt; Los Angeles #&gt; &quot;The city of Los Angeles has a population of 3900000 inhabitants.&quot; #&gt; Chicago #&gt; &quot;The city of Chicago has a population of 2700000 inhabitants.&quot; In this example, mapply() applies the create_phrase function to the cities and populations vectors in parallel, taking one element from each vector at a time, and returns a vector with the resulting phrases. Note that the create_phrase function receives two arguments: city and population. mapply() is responsible for taking one element from each vector and passing them as arguments to the function. In the first iteration, it passes “New York” as city and 8.4e6 as population. In the second iteration, it passes “Los Angeles” and 3.9e6, and so on. Another example of using mapply() would be if we have two vectors with maximum and minimum temperatures of different cities, and we want to calculate the temperature difference between maximum and minimum for each city. # Create vectors with maximum and minimum temperatures maxs &lt;- c(25, 30, 35) mins &lt;- c(18, 22, 28) # Calculate temperature difference for each city temp_difference &lt;- mapply(function(max, min) max - min, maxs, mins) temp_difference #&gt; [1] 7 8 7 In this example, mapply() applies the anonymous function function(max, min) max - min to the maxs and mins vectors in parallel, taking the first element of maxs and the first element of mins, then the second element of each vector, and so on. For each pair of elements, the anonymous function calculates the difference and returns a vector with the results. 2.4.4 Examples: data analysis with higher-order functions Higher-order functions are a powerful tool for data analysis. They allow us to perform complex operations concisely and efficiently. Imagine you have a matrix with information about different states, where each row represents a state and each column a numeric variable, such as population or per capita income. You could use apply() to calculate the mean of each column. # Create a matrix with information about states states &lt;- matrix(c(39.2e6, 29.0e6, 21.4e6, 64500, 56100, 50800), nrow = 3, ncol = 2, dimnames = list(c(&quot;California&quot;, &quot;Texas&quot;, &quot;Florida&quot;), c(&quot;population&quot;, &quot;per_capita_income&quot;))) # Calculate mean of each column means &lt;- apply(states, 2, mean) means #&gt; population per_capita_income #&gt; 29866666.67 57133.33 In this example, apply() applies the mean() function to each column of the states matrix and returns a vector with the means. Another one would be if we have a list with prices of different hotels in several US cities. You could use sapply() to apply a function calculating the tax of each price, or lapply() to convert prices from dollars to euros. You could also use apply() to calculate the average price of hotels in each city, or to find the most expensive and cheapest hotel in each city. As we progress through the book, we will see more examples of how to use higher-order functions to solve real-world problems. The possibilities are endless, and higher-order functions give you great flexibility to manipulate and analyze your data. 2.5 Closures: functions with memory Until now, we have seen that functions in R receive arguments, execute a set of instructions, and return a result. However, functions can also have “memory”, that is, they can remember information between calls. This is possible thanks to a concept called closures. 2.5.1 Concept: functions that “remember” A closure is a function that “remembers” the environment in which it was created. This means the function has access to variables that were defined at the time of its creation, even if those variables are no longer in the current scope. To better understand this concept, let’s see an example. Imagine we want to create a function that counts how many times it has been called. We can do this using a closure: create_counter &lt;- function() { counter &lt;- 0 # Initialize the counter # Define the function that increments the counter increment_counter &lt;- function() { counter &lt;&lt;- counter + 1 return(counter) } return(increment_counter) # Return the function } # Create a counter my_counter &lt;- create_counter() # Call the counter several times my_counter() #&gt; [1] 1 my_counter() #&gt; [1] 2 my_counter() #&gt; [1] 3 In this example, the create_counter() function creates a counter variable and an increment_counter() function. The increment_counter() function has access to the counter variable and increments it by 1 each time it is called. The create_counter() function returns the increment_counter() function. When we call my_counter(), we are calling the increment_counter() function that was created inside create_counter(). This function “remembers” the value of the counter variable and increments it on each call. It is important to note that the counter variable is not a global variable. It is only accessible within the increment_counter() function. This is because counter was defined inside the create_counter() function, so its scope is local to that function. However, the increment_counter() function “captures” the counter variable in its environment, allowing it to access it even after the create_counter() function has finished executing. 2.5.2 Applications: creating counters, functions with internal state Closures have many applications in programming. They are commonly used for creating counters that maintain an internal state between calls, configuring parameters where a generated function remembers specific settings (like a temperature scale), and encapsulating data to hide sensitive information or internal logic within the function scope. 2.5.3 Examples: simulating a game, creating an operation history Let’s see some more concrete examples of using closures: Simulating a game: We can use a closure to simulate a game where the player has to guess a secret number. The closure can “remember” the secret number and keep track of the player’s attempts. Creating an operation history: We can use a closure to create a function that records operations performed on a variable. The closure can “remember” the operation history and show it when requested. Closures are a powerful tool that allows us to write more flexible and expressive code. As you become familiar with them, you will discover new ways to apply them in your data analysis. 2.6 Debugging and error handling: solving the mysteries of your code So far, we have explored the fascinating world of functions in R. We have learned to create, use, and combine them to perform complex tasks. However, on the programming journey, encountering errors is inevitable. Sometimes, our code doesn’t work as we expect, and we encounter cryptic error messages that leave us perplexed. In this section, we will learn to identify, understand, and fix errors in our R code. We will also see how to handle errors gracefully, so our code is more robust and reliable. 2.6.1 Identifying errors: common error messages in R When our code contains an error, R will show us an error message in the console. These messages can seem intimidating at first, but with a little practice, we will learn to interpret them and use them to find the cause of the error. Some common error messages in R include Error: object 'object_name' not found, which happens when you interpret a non-existent variable or function. Another is invalid argument when function inputs don’t match the expected type, such as passing text to a numeric function. You might also encounter argument is of length zero in if conditions, often due to NULL or empty vectors, or invalid 'for' loop sequence when the loop iterator definition is flawed. It is important to read error messages carefully and try to understand what they are telling us. Often, the error message will give us a clue about the cause of the problem. 2.6.2 Debugging tools: debug(), traceback() R offers several tools to debug our code and find the cause of errors. Two of the most useful tools are debug() and traceback(). debug(): This function allows us to execute a function step by step, allowing us to inspect the value of variables at each step and understand how the code is executing. To use debug(), we simply call the function with the name of the function we want to debug as an argument. debug(my_function) Then, when we call my_function(), R will enter debug mode and allow us to execute the code line by line. traceback(): This function shows us the sequence of function calls that led to the error. This can be useful for understanding how the error was reached and which functions are involved. To use traceback(), simply call the function after an error has occurred. traceback() R will show a list of the functions that were called, starting with the function where the error occurred and ending with the function that started the code execution. 2.6.3 Error handling: tryCatch() Sometimes, we want our code to continue executing even if an error occurs. For this, we can use the tryCatch() function. tryCatch() allows us to specify a block of code that will be executed if an error occurs. We can also specify a block of code that will be executed if no error occurs. tryCatch( { # Code that might produce an error }, error = function(e) { # Code to be executed if an error occurs }, finally = { # Code to be executed always, whether or not there is an error } ) For example, if we are reading data from a file and the file does not exist, we can use tryCatch() to show an error message and continue with code execution. tryCatch( { data &lt;- read.csv(&quot;my_file.csv&quot;) }, error = function(e) { print(&quot;Error reading file. Please verify the file exists.&quot;) } ) 2.6.4 Examples: debugging functions with errors, handling exceptions Let’s see some examples of how to use debugging tools and error handling in R: Debugging a function with debug(): Imagine we create a function to calculate a person’s Body Mass Index (BMI), but when using it, we get an error. We can use debug() to analyze what happens inside the function. calculate_bmi &lt;- function(weight, height) { bmi &lt;- weight / (height ^ 2) return(bmi) } debug(calculate_bmi) calculate_bmi(weight = 70, height = 1.75) # We call the function to start debugging When executing this code, R will enter debug mode. In the console, we will see a new prompt Browse[1]&gt;. We can use commands like n (next) to execute the next line of code, c (continue) to continue normal execution, or Q to exit debug mode. We can also print the value of variables using their name (e.g. weight, height, bmi). Handling an exception with tryCatch(): Suppose we are creating a function to calculate the annual population growth rate of a city. If the initial population is 0, the division will produce an error. We can use tryCatch() to handle this situation: calculate_growth_rate &lt;- function(initial_population, final_population, years) { tryCatch( { rate &lt;- ((final_population / initial_population)^(1 / years) - 1) * 100 return(rate) }, error = function(e) { message(&quot;Error: Initial population cannot be zero.&quot;) return(NA) } ) } calculate_growth_rate(10000, 12000, 5) # Output: 3.7137... #&gt; [1] 3.713729 calculate_growth_rate(0, 12000, 5) # Output: &quot;Error: Initial population cannot be zero.&quot; #&gt; [1] Inf In this example, if initial_population is 0, tryCatch() captures the error and displays a message. Then, it returns NA to indicate that calculation could not be performed. With practice, you will learn to use these tools to debug your code, handle errors, and write more robust and reliable programs. 2.7 Exercises It’s time to test your skills with functions! Below, you will find a series of exercises with different levels of difficulty. Create a function called miles_to_kilometers() converting miles to kilometers. The function should receive a miles argument and return the equivalent in kilometers. (Remember that 1 mile equals 1.60934 kilometers). Solution miles_to_kilometers &lt;- function(miles) { kilometers &lt;- miles * 1.60934 return(kilometers) } Create a function called triangle_area() calculating the area of a triangle. The function should receive two arguments: base and height, and return the triangle’s area. (Remember that the area of a triangle is equal to (base * height) / 2). Solution triangle_area &lt;- function(base, height) { area &lt;- (base * height) / 2 return(area) } Create a function called price_with_vat() calculating the price of a product including VAT. The function should receive two arguments: price_without_vat and vat_rate (default, 0.16), and return the price with VAT. Solution price_with_vat &lt;- function(price_without_vat, vat_rate = 0.16) { price_with_vat &lt;- price_without_vat * (1 + vat_rate) return(price_with_vat) } Create a function called is_even() determining if a number is even. The function should receive a number argument and return TRUE if the number is even and FALSE if not. (Hint: use the modulo operator %%). Solution is_even &lt;- function(number) { return(number %% 2 == 0) } Create a function called my_factorial() calculating the factorial of a number. The factorial of a positive integer n, denoted by n!, is the product of all positive integers less than or equal to n. For example, 5! = 5 * 4 * 3 * 2 * 1 = 120. (Hint: use a recursive function). Note: We name it my_factorial() to avoid shadowing R’s built-in factorial() function. Solution my_factorial &lt;- function(n) { if (n &lt; 0) { # We use message() and return(NA) because we haven&#39;t covered stop() yet message(&quot;Factorial is not defined for negative numbers&quot;) return(NA) } if (n == 0) { return(1) } else { return(n * my_factorial(n - 1)) } } Create a function called fibonacci() generating a Fibonacci sequence of a given length. The Fibonacci sequence is a series of numbers where each number is the sum of the two preceding ones. The sequence typically starts with 0 and 1. For example, a Fibonacci sequence of length 10 would be: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34. Solution fibonacci &lt;- function(n) { if (n &lt;= 0) { return(numeric(0)) } else if (n == 1) { return(0) } else if (n == 2) { return(c(0, 1)) } else { fib_seq &lt;- numeric(n) fib_seq[1] &lt;- 0 fib_seq[2] &lt;- 1 for (i in 3:n) { fib_seq[i] &lt;- fib_seq[i - 1] + fib_seq[i - 2] } return(fib_seq) } } fibonacci(10) #&gt; [1] 0 1 1 2 3 5 8 13 21 34 Create a function called gcd() calculating the greatest common divisor (GCD) of two numbers. The GCD of two or more non-zero integers is the largest positive integer that divides them without a remainder. For example, the GCD of 12 and 18 is 6. (Hint: use the Euclidean algorithm). Solution gcd &lt;- function(a, b) { while (b != 0) { temp &lt;- b b &lt;- a %% b a &lt;- temp } return(a) } Create a function called validate_password() validating a password. The function should receive a password argument and return TRUE if the password meets the following conditions, and FALSE otherwise: Has at least 8 characters. Contains at least one uppercase letter. Contains at least one lowercase letter. Contains at least one number. Contains at least one special character (!@#$%^&amp;*). Solution validate_password &lt;- function(password) { if (nchar(password) &lt; 8) { return(FALSE) } if (!grepl(&quot;[A-Z]&quot;, password)) { return(FALSE) } if (!grepl(&quot;[a-z]&quot;, password)) { return(FALSE) } if (!grepl(&quot;[0-9]&quot;, password)) { return(FALSE) } if (!grepl(&quot;[!@#$%^&amp;*]&quot;, password)) { return(FALSE) } return(TRUE) } Create a function called apply_discount() receiving a price calculation function and a discount as arguments. The apply_discount() function should return a new function calculating the price with the discount applied. Solution apply_discount &lt;- function(price_function, discount) { function(original_price) { discounted_price &lt;- price_function(original_price) * (1 - discount) return(discounted_price) } } Create a function called create_temperature_converter() receiving a temperature scale as argument (“Celsius”, “Fahrenheit” or “Kelvin”). The function should return a function converting temperatures to the specified scale. Solution create_temperature_converter &lt;- function(scale) { if (scale == &quot;Celsius&quot;) { return(function(temp) (temp - 32) * 5 / 9) # Fahrenheit to Celsius } else if (scale == &quot;Fahrenheit&quot;) { return(function(temp) (temp * 9 / 5) + 32) # Celsius to Fahrenheit } else if (scale == &quot;Kelvin&quot;) { return(function(temp) temp + 273.15) # Celsius to Kelvin } else { # We use message() and return(NA) because we haven&#39;t covered stop() yet message(&quot;Invalid temperature scale.&quot;) return(NA) } } Create a function called guess_number() simulating a guess the number game. The function should generate a random number between 1 and 100 and ask the user to guess it. The function should give hints to the user (higher or lower) and count the number of attempts. (Hint: use a closure to store the secret number and the number of attempts). Solution guess_number &lt;- function() { secret_number &lt;- sample(1:100, 1) attempts &lt;- 0 guess &lt;- function() { attempts &lt;&lt;- attempts + 1 cat(&quot;Attempt&quot;, attempts, &quot;: &quot;) number &lt;- as.numeric(readline()) if (is.na(number)) { cat(&quot;Please enter a valid number.\\n&quot;) } else if (number &lt; secret_number) { cat(&quot;The secret number is higher.\\n&quot;) } else if (number &gt; secret_number) { cat(&quot;The secret number is lower.\\n&quot;) } else { cat(&quot;You guessed it! The secret number was&quot;, secret_number, &quot;\\n&quot;) cat(&quot;It took you&quot;, attempts, &quot;attempts.\\n&quot;) } } return(guess) } game &lt;- guess_number() game() #&gt; Attempt 1 : #&gt; Please enter a valid number. Create a function that, given a vector of integers, finds the contiguous subsequence with the maximum sum. For example, for the vector c(-2, 1, -3, 4, -1, 2, 1, -5, 4), the contiguous subsequence with the maximum sum is c(4, -1, 2, 1), with a sum of 6. Solution max_subsequence &lt;- function(x) { current_max &lt;- 0 global_max &lt;- 0 start &lt;- 1 end &lt;- 1 temp_start &lt;- 1 for (i in 1:length(x)) { current_max &lt;- current_max + x[i] if (current_max &gt; global_max) { global_max &lt;- current_max start &lt;- temp_start end &lt;- i } if (current_max &lt; 0) { current_max &lt;- 0 temp_start &lt;- i + 1 } } return(list(subsequence = x[start:end], sum = global_max)) } test &lt;- c(-2, 1, -3, 4, -1, 2, 1, -5, 4) max_subsequence(test) #&gt; $subsequence #&gt; [1] 4 -1 2 1 #&gt; #&gt; $sum #&gt; [1] 6 Create a function that, given a character vector, determines if it is possible to obtain a palindrome by rearranging its letters. A palindrome is a word or phrase that reads the same left to right as right to left (e.g. “radar”). Solution is_palindrome_possible &lt;- function(text) { letters &lt;- strsplit(tolower(text), &quot;&quot;)[[1]] frequencies &lt;- table(letters) odds &lt;- sum(frequencies %% 2) return(odds &lt;= 1) } test &lt;- c(&quot;radar&quot;, &quot;hello&quot;, &quot;abb&quot;) result &lt;- sapply(test, is_palindrome_possible) result #&gt; radar hello abb #&gt; TRUE FALSE TRUE Create a function that, given a positive integer, determines if it is a prime number. A prime number is a natural number greater than 1 that has no divisors other than 1 and itself. Solution is_prime &lt;- function(n) { if (n &lt;= 1) { return(FALSE) } if (n &lt;= 3) { return(TRUE) } if (n %% 2 == 0 || n %% 3 == 0) { return(FALSE) } i &lt;- 5 while (i * i &lt;= n) { if (n %% i == 0 || n %% (i + 2) == 0) { return(FALSE) } i &lt;- i + 6 } return(TRUE) } The condition i * i &lt;= n in the while loop limits iterations to the square root of n. This optimizes the algorithm, as it is not necessary to check divisors greater than the square root of n. The increment i &lt;- i + 6 is based on the observation that all prime numbers greater than 3 can be expressed in the form 6k ± 1. Therefore, only numbers of the form 6k ± 1 need to be checked as possible divisors. "],["data-frames.html", "Chapter 3 Data Frames 3.1 Introduction to Data Frames 3.2 Creating Data Frames: Building your database for the move 3.3 Exploring Data Frames: Discovering the secrets of your data 3.4 Manipulating Data Frames: Transforming your data 3.5 Exercises 3.6 Data frames in plots 3.7 Data interpretation 3.8 Exercises", " Chapter 3 Data Frames 3.1 Introduction to Data Frames In previous chapters, we explored different types of objects in R, such as variables, vectors, lists, and matrices. These objects allow us to store information in more efficient ways. Now, in this chapter, we will delve into the world of data frames, an essential tool for organizing and analyzing information that will help you make the best decision about your move to the United States. 3.1.1 What are data frames? Imagine a spreadsheet, with rows and columns organizing information in a tabular way. In R, a data frame is precisely that: a data structure that stores information in a tabular format, with rows representing observations (for example, every US city) and columns representing variables (such as population, cost of living, crime rate). Each column of a data frame can contain a different data type: numeric, character, logical, factor, etc. This makes data frames very versatile for storing diverse information. For example, a data frame about US cities could serve as a comprehensive record. It might contain a character column for the city name and another for the state it belongs to. Numeric columns could store the population and the area in square kilometers, while a logical column like has_beach could indicate whether the city is coastal. 3.1.2 Why data frames? In R, there are various structures for organizing data, such as vectors, lists, and matrices. However, data frames stand out as a fundamental tool in data analysis. Why? Data frames offer a unique combination of features that make them ideal for representing and manipulating complex information: Data frames are uniquely suited for data analysis because of their specific features. Their tabular structure organizes data into rows and columns, similar to a spreadsheet, making it intuitive to visualize. They offer flexibility by allowing each column to hold a different data type, such as numbers, text, or dates. this structure also ensures efficiency, as most R analysis packages are optimized to work directly with data frames. In summary, data frames are a versatile and powerful data structure that adapts to the needs of modern data analysis. 3.1.3 Data Frames in action: exploring information about the United States In the context of your move to the United States, data frames will be essential for organizing and analyzing the information you need to make the best decision. We can use data frames to store information about: We can use data frames to store and correlate various aspects of your potential new home. You might track crime rates across different states, compare the cost of living (housing, food, transportation) in target cities, analyze climate data like temperature and precipitation, or study demographics such as population age and education levels. With this information organized in data frames, you will be able to perform deeper analyses and make more informed decisions about your move. 3.2 Creating Data Frames: Building your database for the move Now that you know what data frames are and why they are so important in data analysis, it’s time to learn how to create them. In R, we can create data frames in different ways: importing data from external files or creating them manually. 3.2.1 Importing data from files: CSV, Excel A common way to create data frames is by importing data from external files, such as CSV (Comma Separated Values) files or Excel files. R offers us functions to read data from different formats. One of the most common ways to create data frames is by importing data from external files. For CSV (Comma Separated Values) files, we rely on the read_csv() function from the readr package (part of the tidyverse), which is faster and more robust than the base R equivalent. To import a file, you simply provide its URL or file path: ``` r library(readr) url &lt;- &quot;https://dparedesi.github.io/DS-with-R-datasets/student-grades.csv&quot; # Import data from a CSV file called &quot;student-grades.csv&quot; grades &lt;- read_csv(url) #&gt; Rows: 21 Columns: 9 #&gt; ── Column specification ───────────────────────────── #&gt; Delimiter: &quot;,&quot; #&gt; chr (3): start_date, gender, type #&gt; dbl (6): P1, P2, P3, P4, P5, P6 #&gt; #&gt; ℹ Use `spec()` to retrieve the full column specification for this data. #&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. grades #&gt; # A tibble: 21 × 9 #&gt; start_date gender type P1 P2 P3 P4 P5 P6 #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 03/05/2020 female Individual Work 1 5 5 5 5 5 5 #&gt; 2 03/05/2020 male Individual Work 1 5 5 5 5 4 5 #&gt; 3 03/05/2020 female Individual Work 1 5 5 4 5 5 5 #&gt; 4 03/05/2020 male Individual Work 1 5 5 5 5 5 5 #&gt; 5 03/05/2020 male Individual Work 1 2 5 5 5 5 5 #&gt; 6 03/05/2020 male Individual Work 1 5 4 5 1 5 5 #&gt; 7 03/05/2020 male Individual Work 1 2 1 5 5 2 5 #&gt; 8 03/05/2020 male Individual Work 1 5 5 5 5 5 5 #&gt; 9 03/05/2020 male Individual Work 1 4 5 5 5 5 5 #&gt; 10 03/05/2020 male Individual Work 1 3 4 5 5 5 5 #&gt; # ℹ 11 more rows ``` The read_csv() function offers several arguments to customize how files are read. The header argument allows you to specify if the first row contains column names, while sep defines the column separator (defaulting to a comma). You can also use dec to set the character used for decimal points. For Excel files, we use the read_excel() function from the readxl package. This function works similarly but includes specific arguments like sheet to specify which spreadsheet tab to import. ``` r # Install the readxl package (if you don&#39;t have it installed) install.packages(&quot;readxl&quot;) # Load the readxl package library(readxl) # Import data from an Excel file called &quot;states.xlsx&quot; states &lt;- read_excel(&quot;states.xlsx&quot;) ``` 3.2.2 Creating data frames manually We can also create data frames manually, combining vectors with the data.frame() function. # Create vectors with information about cities cities &lt;- c(&quot;New York&quot;, &quot;Los Angeles&quot;, &quot;Chicago&quot;) states &lt;- c(&quot;New York&quot;, &quot;California&quot;, &quot;Illinois&quot;) population &lt;- c(8.4e6, 3.9e6, 2.7e6) # Create a data frame with city information df_cities_simple &lt;- data.frame(city = cities, state = states, population = population) df_cities_simple #&gt; city state population #&gt; 1 New York New York 8400000 #&gt; 2 Los Angeles California 3900000 #&gt; 3 Chicago Illinois 2700000 In this example, we create a data frame called df_cities_simple with three columns: city, state, and population. Each column is created from a vector. Note that the vectors must have the same length to be combined into a data frame. 3.2.3 Examples We can use data frames to organize diverse information about our move to the United States. For example, we could create a data frame with information about different cities, including their cost of living, crime rate, and climate. We could also create a data frame with information about the different states, including their population, gross domestic product (GDP), and education system. # Create a data frame with information about cities df_cities &lt;- data.frame( city = c(&quot;New York&quot;, &quot;Los Angeles&quot;, &quot;Chicago&quot;, &quot;Houston&quot;), state = c(&quot;New York&quot;, &quot;California&quot;, &quot;Illinois&quot;, &quot;Texas&quot;), cost_of_living = c(3.5, 2.8, 2.5, 2.0), # In thousands of dollars crime_rate = c(400, 350, 500, 450), # Per 100,000 inhabitants climate = c(&quot;Temperate&quot;, &quot;Mediterranean&quot;, &quot;Continental&quot;, &quot;Subtropical&quot;) ) df_cities #&gt; city state cost_of_living crime_rate climate #&gt; 1 New York New York 3.5 400 Temperate #&gt; 2 Los Angeles California 2.8 350 Mediterranean #&gt; 3 Chicago Illinois 2.5 500 Continental #&gt; 4 Houston Texas 2.0 450 Subtropical # Create a data frame with information about states df_states &lt;- data.frame( state = c(&quot;California&quot;, &quot;Texas&quot;, &quot;Florida&quot;, &quot;New York&quot;), population = c(39.2e6, 29.0e6, 21.4e6, 19.4e6), gdp = c(3.2e12, 1.8e12, 1.1e12, 1.7e12), # In dollars education_system = c(&quot;Good&quot;, &quot;Regular&quot;, &quot;Good&quot;, &quot;Excellent&quot;) ) df_states #&gt; state population gdp education_system #&gt; 1 California 39200000 3.2e+12 Good #&gt; 2 Texas 29000000 1.8e+12 Regular #&gt; 3 Florida 21400000 1.1e+12 Good #&gt; 4 New York 19400000 1.7e+12 Excellent These data frames will allow us to analyze the information more efficiently and make more informed decisions about our move. 3.3 Exploring Data Frames: Discovering the secrets of your data We have already learned to create data frames, now it is time to explore their content and discover the information they hide. R offers us various tools to examine and understand our data. 3.3.1 Accessing rows, columns, and cells A data frame is like a map organized in rows and columns. To access the information we need, we must know how to navigate this map. R provides us with different ways to access rows, columns, and cells of a data frame. There are several ways to access specific data within a dataframe. To retrieve a column, you can use the $ operator (e.g., df_cities$state) or bracket notation with the column name in quotes (e.g., df_states[\"population\"]). To access a specific row, use brackets with the row number (e.g., df_cities[3, ]). For a precise cell at the intersection of a row and column, specify both indices (e.g., df_states[2, 3]). You can also filter rows based on conditions, such as extracting all cities where the cost of living is less than 3 using a logical expression inside the brackets. 3.3.2 Functions for exploring data frames R offers several useful functions for exploring data frames: R provides useful functions for a quick overview of your data. head() displays the first six rows, while tail() shows the last six. To understand the structure—such as column names and data types—you can use str(). For a statistical overview including mean, median, and quartiles, summary() is the go-to function. Additionally, View() opens an interactive spreadsheet-style window to browse the data. 3.3.3 Examples: exploring data frames with move information By exploring the data frames we created in the previous section, we can obtain valuable information about US cities and states. For example, we could use summary() to get descriptive statistics of the cost of living in different cities, or View() to examine information about each state in detail. # Get descriptive statistics of cost of living in different cities summary(df_cities$cost_of_living) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 2.000 2.375 2.650 2.700 2.975 3.500 # Examine detailed information about each state View(df_states) In addition to the mentioned functions, we can use other tools to explore our data frames. For example, we can use the table() function to get the frequency of each value in a categorical column, such as the climate column in the df_cities data frame. table(df_cities$climate) #&gt; #&gt; Continental Mediterranean Subtropical Temperate #&gt; 1 1 1 1 We can also use the hist() function to create a histogram of a numeric column, such as the population column in the df_states data frame. hist(df_states$population) These are just some ideas of how we can explore our data frames. As you become familiar with R, you will discover new functions and techniques for analyzing and visualizing your data. 3.4 Manipulating Data Frames: Transforming your data In the previous section, we learned to explore data frames and access the information they contain. Now, we will go a step further and learn to manipulate data frames, transforming data to answer specific questions and obtain relevant information for our move. 3.4.1 Introduction to the pipeline operator (|&gt;) Before modifying data frames, we will introduce a tool to write more readable and efficient code: the native pipeline operator (|&gt;). This operator was introduced in R 4.1 (2021) as a built-in language feature, meaning it works without any additional packages. Note: You may also encounter the %&gt;% pipe operator from the magrittr package (part of the tidyverse). Both |&gt; and %&gt;% work similarly for most data analysis tasks. We use the native |&gt; operator throughout this book as it is built into R, but %&gt;% is still widely used in older codebases. The pipeline operator allows us to chain several operations sequentially. Instead of writing nested code, we can use the pipeline operator to “pass” the result of one operation to the next. To use additional data manipulation functions, we’ll load the tidyverse package, which includes dplyr - a package with many useful functions for working with data frames. A package in R is like a toolbox with additional functions and data for performing specific tasks. To use a package’s functions, we must first install it and then load it into our working environment. To install the tidyverse package, we can use the following instruction in the R console: install.packages(&quot;tidyverse&quot;) This will install tidyverse and all the packages it contains, including dplyr. Once the package is installed, we can load it with the library() function: library(tidyverse) Now we can use the pipeline operator (|&gt;) and functions from dplyr. For example, we’ll use the murders dataset from the dslabs package. This dataset contains gun murder data by US state in 2010, including variables like state name, abbreviation, region, population, and total murders. Let’s use a pipeline to view selected columns: install.packages(&quot;dslabs&quot;) # Load library and dataset library(dslabs) data(murders) # Pipeline murders |&gt; select(state, population, total) #&gt; state population total #&gt; 1 Alabama 4779736 135 #&gt; 2 Alaska 710231 19 #&gt; 3 Arizona 6392017 232 #&gt; 4 Arkansas 2915918 93 #&gt; 5 California 37253956 1257 #&gt; 6 Colorado 5029196 65 #&gt; 7 Connecticut 3574097 97 #&gt; 8 Delaware 897934 38 #&gt; 9 District of Columbia 601723 99 #&gt; 10 Florida 19687653 669 #&gt; 11 Georgia 9920000 376 #&gt; 12 Hawaii 1360301 7 #&gt; 13 Idaho 1567582 12 #&gt; 14 Illinois 12830632 364 #&gt; 15 Indiana 6483802 142 #&gt; 16 Iowa 3046355 21 #&gt; 17 Kansas 2853118 63 #&gt; 18 Kentucky 4339367 116 #&gt; 19 Louisiana 4533372 351 #&gt; 20 Maine 1328361 11 #&gt; 21 Maryland 5773552 293 #&gt; 22 Massachusetts 6547629 118 #&gt; 23 Michigan 9883640 413 #&gt; 24 Minnesota 5303925 53 #&gt; 25 Mississippi 2967297 120 #&gt; 26 Missouri 5988927 321 #&gt; 27 Montana 989415 12 #&gt; 28 Nebraska 1826341 32 #&gt; 29 Nevada 2700551 84 #&gt; 30 New Hampshire 1316470 5 #&gt; 31 New Jersey 8791894 246 #&gt; 32 New Mexico 2059179 67 #&gt; 33 New York 19378102 517 #&gt; 34 North Carolina 9535483 286 #&gt; 35 North Dakota 672591 4 #&gt; 36 Ohio 11536504 310 #&gt; 37 Oklahoma 3751351 111 #&gt; 38 Oregon 3831074 36 #&gt; 39 Pennsylvania 12702379 457 #&gt; 40 Rhode Island 1052567 16 #&gt; 41 South Carolina 4625364 207 #&gt; 42 South Dakota 814180 8 #&gt; 43 Tennessee 6346105 219 #&gt; 44 Texas 25145561 805 #&gt; 45 Utah 2763885 22 #&gt; 46 Vermont 625741 2 #&gt; 47 Virginia 8001024 250 #&gt; 48 Washington 6724540 93 #&gt; 49 West Virginia 1852994 27 #&gt; 50 Wisconsin 5686986 97 #&gt; 51 Wyoming 563626 5 Code with pipeline is easier to read and understand, as it follows the natural flow of operations. Pipeline creates a view; we are not editing the murders data frame. We can show the first rows using the head() function: head(murders |&gt; select(state, population, total)) #&gt; state population total #&gt; 1 Alabama 4779736 135 #&gt; 2 Alaska 710231 19 #&gt; 3 Arizona 6392017 232 #&gt; 4 Arkansas 2915918 93 #&gt; 5 California 37253956 1257 #&gt; 6 Colorado 5029196 65 We can also use the pipeline operator to show the first rows: murders |&gt; select(state, population, total) |&gt; head() #&gt; state population total #&gt; 1 Alabama 4779736 135 #&gt; 2 Alaska 710231 19 #&gt; 3 Arizona 6392017 232 #&gt; 4 Arkansas 2915918 93 #&gt; 5 California 37253956 1257 #&gt; 6 Colorado 5029196 65 For better readability, we will use one function per line, obtaining the same result: murders |&gt; select(state, population, total) |&gt; # Select columns head() # Show first 6 rows #&gt; state population total #&gt; 1 Alabama 4779736 135 #&gt; 2 Alaska 710231 19 #&gt; 3 Arizona 6392017 232 #&gt; 4 Arkansas 2915918 93 #&gt; 5 California 37253956 1257 #&gt; 6 Colorado 5029196 65 3.4.2 Transforming a table with mutate() We can create new columns or modify existing ones using the mutate() function. For example, to add a column with the homicide rate per 100,000 inhabitants to the murders data frame: murders |&gt; mutate(ratio = total / population * 100000) |&gt; head() #&gt; state abb region population total ratio #&gt; 1 Alabama AL South 4779736 135 2.824424 #&gt; 2 Alaska AK West 710231 19 2.675186 #&gt; 3 Arizona AZ West 6392017 232 3.629527 #&gt; 4 Arkansas AR South 2915918 93 3.189390 #&gt; 5 California CA West 37253956 1257 3.374138 #&gt; 6 Colorado CO West 5029196 65 1.292453 This creates a view with the additional ratio column. If we want to modify the murders data frame directly, we use the assignment operator &lt;-: murders &lt;- murders |&gt; mutate(ratio = total / population * 100000) 3.4.3 Filtering data: selecting cities that interest you We can filter rows meeting a condition using the filter() function. For example, to get states with less than 1 homicide per 100,000 inhabitants: # Load dataset data(murders) murders |&gt; mutate(ratio = total / population * 100000) |&gt; filter(ratio &lt; 1) #&gt; state abb region population total ratio #&gt; 1 Hawaii HI West 1360301 7 0.5145920 #&gt; 2 Idaho ID West 1567582 12 0.7655102 #&gt; 3 Iowa IA North Central 3046355 21 0.6893484 #&gt; 4 Maine ME Northeast 1328361 11 0.8280881 #&gt; 5 Minnesota MN North Central 5303925 53 0.9992600 #&gt; 6 New Hampshire NH Northeast 1316470 5 0.3798036 #&gt; 7 North Dakota ND North Central 672591 4 0.5947151 #&gt; 8 Oregon OR West 3831074 36 0.9396843 #&gt; 9 South Dakota SD North Central 814180 8 0.9825837 #&gt; 10 Utah UT West 2763885 22 0.7959810 #&gt; 11 Vermont VT Northeast 625741 2 0.3196211 #&gt; 12 Wyoming WY West 563626 5 0.8871131 We can use different operators to create our conditions: R supports standard comparison operators to create conditions: greater than (&gt;), less than (&lt;), greater than or equal to (&gt;=), less than or equal to (&lt;=), equal to (==), and different from (!=). You can combine multiple conditions using logical operators: &amp; for AND, | for OR, and ! for NOT. For example, to filter by ratio less than 1 and West region: murders |&gt; mutate(ratio = total / population * 100000) |&gt; filter(ratio &lt; 1 &amp; region == &quot;West&quot;) #&gt; state abb region population total ratio #&gt; 1 Hawaii HI West 1360301 7 0.5145920 #&gt; 2 Idaho ID West 1567582 12 0.7655102 #&gt; 3 Oregon OR West 3831074 36 0.9396843 #&gt; 4 Utah UT West 2763885 22 0.7959810 #&gt; 5 Wyoming WY West 563626 5 0.8871131 3.4.4 Sorting data: finding the safest cities The arrange() function from the dplyr package allows us to order the rows of a data frame based on one or more columns. Imagine you have a data frame with information about different cities, and you want to order them from safest to least safe, based on their crime rate. Or perhaps you want to order them by cost of living, from cheapest to most expensive. arrange() allows you to do this easily. For example, to order states by homicide rate (from lowest to highest): murders |&gt; mutate(ratio = total / population * 100000) |&gt; arrange(ratio) |&gt; head() #&gt; state abb region population total ratio #&gt; 1 Vermont VT Northeast 625741 2 0.3196211 #&gt; 2 New Hampshire NH Northeast 1316470 5 0.3798036 #&gt; 3 Hawaii HI West 1360301 7 0.5145920 #&gt; 4 North Dakota ND North Central 672591 4 0.5947151 #&gt; 5 Iowa IA North Central 3046355 21 0.6893484 #&gt; 6 Idaho ID West 1567582 12 0.7655102 If we want to sort in descending order, we use the desc() function: murders |&gt; mutate(ratio = total / population * 100000) |&gt; arrange(desc(ratio)) |&gt; head() #&gt; state abb region population total ratio #&gt; 1 District of Columbia DC South 601723 99 16.452753 #&gt; 2 Louisiana LA South 4533372 351 7.742581 #&gt; 3 Missouri MO North Central 5988927 321 5.359892 #&gt; 4 Maryland MD South 5773552 293 5.074866 #&gt; 5 South Carolina SC South 4625364 207 4.475323 #&gt; 6 Delaware DE South 897934 38 4.231937 We can also sort by multiple columns. For example, if we want to sort first by region and then by state (in alphabetical order): murders |&gt; arrange(region, state) |&gt; head() #&gt; state abb region population total #&gt; 1 Connecticut CT Northeast 3574097 97 #&gt; 2 Maine ME Northeast 1328361 11 #&gt; 3 Massachusetts MA Northeast 6547629 118 #&gt; 4 New Hampshire NH Northeast 1316470 5 #&gt; 5 New Jersey NJ Northeast 8791894 246 #&gt; 6 New York NY Northeast 19378102 517 3.4.5 Aggregating and summarizing data: obtaining general overview The summarize() function from the dplyr package allows us to calculate descriptive statistics for one or more columns of a data frame. It’s like summarizing information from our data frame into a single number or a set of numbers. For example, to calculate the mean population of states: murders |&gt; summarize(mean_population = mean(population)) #&gt; mean_population #&gt; 1 6075769 We can combine summarize() with group_by() to calculate statistics by groups. For example, to calculate average population by region: murders |&gt; group_by(region) |&gt; summarize(mean_population = mean(population)) #&gt; # A tibble: 4 × 2 #&gt; region mean_population #&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 Northeast 6146360 #&gt; 2 South 6804378. #&gt; 3 North Central 5577250. #&gt; 4 West 5534273. 3.4.6 Joining data frames: combining information Imagine you have two data frames: one with information about cities (name, population, etc.) and another with information about the states those cities belong to (state name, governor, etc.). If you want to combine information from both data frames to have a single data frame with all information about cities and their states, you can use dplyr join functions. dplyr offers several functions for joining data frames, such as left_join(), right_join(), inner_join(), and full_join(). Each function performs a different type of join, depending on how data frame rows are combined. The left_join() function joins two data frames keeping all rows from the first data frame (the one on the left) and adding columns from the second data frame that match the first data frame’s rows. If a row from the first data frame has no match in the second data frame, new columns will have NA values. For example, if we have a data frame with city information and another with state information, we can join them by the state column: df_cities_states &lt;- left_join(df_cities, df_states, by = &quot;state&quot;) The resulting data frame df_cities_states will contain information from both data frames combined. If a city in df_cities does not have a corresponding state in df_states, columns from df_states will have NA values for that city. Let’s see a concrete example. Suppose we have the following data frames: df_cities &lt;- data.frame( city = c(&quot;New York&quot;, &quot;Los Angeles&quot;, &quot;Chicago&quot;, &quot;Houston&quot;), state = c(&quot;New York&quot;, &quot;California&quot;, &quot;Illinois&quot;, &quot;Texas&quot;) ) df_states &lt;- data.frame( state = c(&quot;California&quot;, &quot;Texas&quot;, &quot;Florida&quot;), governor = c(&quot;Gavin Newsom&quot;, &quot;Greg Abbott&quot;, &quot;Ron DeSantis&quot;) ) # Join data frames by &quot;state&quot; column df_cities_states &lt;- left_join(df_cities, df_states, by = &quot;state&quot;) df_cities_states #&gt; city state governor #&gt; 1 New York New York &lt;NA&gt; #&gt; 2 Los Angeles California Gavin Newsom #&gt; 3 Chicago Illinois &lt;NA&gt; #&gt; 4 Houston Texas Greg Abbott In this example, left_join() combines df_cities and df_states data frames by the state column. Note that “New York” and “Chicago” cities have NA values in the governor column, since their states (“New York” and “Illinois”) are not present in the df_states data frame. The other join functions (right_join(), inner_join(), and full_join()) work similarly, but with different criteria for combining data frame rows. The other join functions work similarly but with different inclusion criteria. right_join() does the opposite of left_join(), keeping all rows from the right data frame and only matching rows from the left. inner_join() is more restrictive, keeping only rows that have matches in both tables, while full_join() is the most inclusive, retaining all rows from both data frames and filling in NA where no match exists. You can consult dplyr documentation for more information about these functions. 3.4.7 Examples The dplyr functions we have seen allow us to perform complex data transformations to answer specific questions about our move to the United States. Let’s see some examples with R code: Examples of analysis questions We can combine these tools to answer specific questions. To find suitable locations, we might filtered for cities with a “Good” education system and a cost of living index below 2.5. Alternatively, to study economic prosperity, we could sort states by their GDP per capita (calculated as GDP divided by population) in descending order. For a more comprehensive climate analysis, we could join our city data with a separate climate table. With these tools, you will be able to explore and analyze information about the United States to make the best decision about your move. 3.5 Exercises Report the state abbreviation abb and population population columns from the murders data frame Solution murders |&gt; select(abb, population) Report all data frame data that are not from the South region. Solution murders |&gt; filter(region != &quot;South&quot;) If we want to filter all records that are from the South and West region we will use %in% instead of == to compare versus a vector Create the vector south_and_west containing values “South” and “West”. Then filter records that are from those two regions. Solution south_and_west &lt;- c(&quot;South&quot;, &quot;West&quot;) murders |&gt; filter(region %in% south_and_west) Add the ratio column to the murders data frame with the murder ratio per 100,000 inhabitants. Then, filter those with a ratio less than 0.5 and are from “South” and “West” regions. Report state, abb, and ratio columns. Solution data(murders) south_and_west &lt;- c(&quot;South&quot;, &quot;West&quot;) murders &lt;- murders |&gt; mutate(ratio = total/population*100000) |&gt; filter(ratio &lt; 0.5 &amp; region %in% south_and_west) |&gt; select(state, abb, ratio) murders To sort using pipeline we use the arrange(x) function, where x is the name of the column we want to take as reference which will sort in ascending order or arrange(desc(x)) to sort in descending order. Modify the code generated in the previous exercise to sort the result by the ratio field. Solution data(murders) south_and_west &lt;- c(&quot;South&quot;, &quot;West&quot;) murders &lt;- murders |&gt; mutate(ratio = total/population*100000) |&gt; filter(ratio &lt; 0.5 &amp; region %in% south_and_west) |&gt; select(state, abb, ratio) |&gt; arrange(ratio) murders So, finally we can know what state options we have to be able to move and solve the presented case. 3.6 Data frames in plots Now we will see some functions that allow us to visualize our data. Little by little we will build more complex and visually more aesthetic graphs to present. First let’s see the most basic functions R presents us. In the next chapter we will see in more detail graph types and in which situations it is recommended to use one or another graph. 3.6.1 Scatter plots One of the most used plots in R is the scatter plot, which is a type of mathematical diagram using Cartesian coordinates to show values for two variables for a set of data (Jarrell 1994, 492). By default we assume the variables to analyze are independent. Thus, the scatter plot will show the degree of correlation (not causality) between the two variables. The simplest way to plot a scatter plot is with the plot(x,y) function, where x and y are vectors indicating the x-axis coordinates and y-axis coordinates of each point we want to plot. For example, let’s see the relationship between population size and total murders. # Let&#39;s store population data in the x_axis object x_axis &lt;- murders$population # Let&#39;s store total murders data in the y_axis object y_axis &lt;- murders$total # With this code we create the scatter plot plot(x_axis, y_axis) We can see a correlation between population and number of cases. Let’s transform the x_axis dividing by one million (\\({10}^6\\)). Thus we will have the x axis expressed in millions. x_axis &lt;- murders$population/10^6 y_axis &lt;- murders$total plot(x_axis, y_axis) 3.6.2 Histograms We can also create histograms from a vector with the hist function. data(murders) murders &lt;- murders |&gt; mutate(ratio = total/population*100000) hist(murders$ratio) The ease R gives us to create graphs will save us time for analysis. From here we can quickly see that most states have a ratio &lt; 5. 3.6.3 Box plot Finally, R allows us to create box plots easily with the boxplot function. So, if we wanted to analyze the distribution of ratio we would use the following code: boxplot(murders$ratio) 3.7 Data interpretation We have seen graphs that can be generated with a line of code, but we need to interpret them. To do so, we need to learn or remember some statistics. Throughout this book we will learn statistical concepts not going deep into the math part, but from the practical part and leveraging that functions already exist in R. Let’s remember our case/problem. We have a list of murders in each of the 51 states. If we order them by the total column we would have: murders |&gt; arrange(total) |&gt; head() #&gt; state abb region population total ratio #&gt; 1 Vermont VT Northeast 625741 2 0.3196211 #&gt; 2 North Dakota ND North Central 672591 4 0.5947151 #&gt; 3 New Hampshire NH Northeast 1316470 5 0.3798036 #&gt; 4 Wyoming WY West 563626 5 0.8871131 #&gt; 5 Hawaii HI West 1360301 7 0.5145920 #&gt; 6 South Dakota SD North Central 814180 8 0.9825837 R provides us with the summary() function, which gives us a summary of a vector’s data. summary(murders$total) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 2.0 24.5 97.0 184.4 268.0 1257.0 The summary provides key insights: the Min and Max show the range of the data; the 1st Qu (first quartile) and 3rd Qu (third quartile) indicate the 25th and 75th percentiles; the Median marks the exact middle of the distribution; and the Mean gives the arithmetic average. 3.7.1 Quartiles To understand quartiles let’s visualize total data in an ordered way. To only obtain a single column in pipeline we will use .$ before the variable name: murders |&gt; arrange(total) |&gt; pull(total) #&gt; [1] 2 4 5 5 7 8 11 12 12 16 19 21 22 27 32 #&gt; [16] 36 38 53 63 65 67 84 93 93 97 97 99 111 116 118 #&gt; [31] 120 135 142 207 219 232 246 250 286 293 310 321 351 364 376 #&gt; [46] 413 457 517 669 805 1257 Quartiles divide our vector into 4 parts with the same amount of data. Given we have 51 values, we would have groups of 51/4 = 12.75. We would have groups of 13 values (3 groups of 13 elements and one of 12 elements). For example, the first group would be composed of these numbers: #&gt; [1] 2 4 5 5 7 8 11 12 12 16 19 21 22 The second group would be composed of these numbers: #&gt; [1] 27 32 36 38 53 63 65 67 84 93 93 97 97 And so on. In total 4 groups made up of 25% of data each. 3.7.1.1 First quartile Therefore, when we see the 1st quartile, 1st Qu., let’s think that is the cut indicating up to where I can find 25% of the data. summary(murders$total) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 2.0 24.5 97.0 184.4 268.0 1257.0 In our example 24.5 indicates that every number less than or equal to that number will be within the first 25% of data (25% of 51 data points = 12.75, rounded to 13 data points). If we list numbers less than or equal to 24.5 we will have this list: murders |&gt; arrange(total) |&gt; filter(total &lt;= 24.5) |&gt; pull(total) #&gt; [1] 2 4 5 5 7 8 11 12 12 16 19 21 22 Which is exactly the same list we obtained previously for the first group. 3.7.1.2 Second quartile or median The second quartile, also called the median (Median), indicates the cut of the second group. The first group contains the first 25% of data, the second group has additional 25%. So this cut would give us exactly the value found in the middle. summary(murders$total) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 2.0 24.5 97.0 184.4 268.0 1257.0 In our example 97 indicates that below that number we will find 50% of total data (50% of 51 data points = 25.5, rounded to 26 data points). murders |&gt; arrange(total) |&gt; filter(total &lt;= 97) |&gt; pull(total) #&gt; [1] 2 4 5 5 7 8 11 12 12 16 19 21 22 27 32 36 38 53 63 65 67 84 93 93 97 #&gt; [26] 97 3.7.1.3 Third quartile The third quartile is the cut of the third group. Up to the median we already had 50%, if we add another 25% of data we would have 75%. summary(murders$total) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 2.0 24.5 97.0 184.4 268.0 1257.0 In our example 268 indicates that below that number we will find 75% of total data (75% of 51 data points = 38.25, rounded to 38 data points). 3.7.2 Interpretation of box plot We are now ready to create a box plot with total murders and interpret results. boxplot(murders$total) The box starts at value 24.5 (first quartile) and ends at value 268 (third quartile). The thick line represents the median (second quartile), 97 in our example. Between the first quartile and third quartile (between 24.5 and 97 for our example) we will find 50% of the data, also called interquartile range or IQR. Outside the box we see a vertical line upwards and another downwards, showing the range of our data. Outside those lines we see dots which are atypical data very far from the mean, known as outliers. We can quickly find to which states these extreme data belong if we sort the table descendingly using the desc function: murders |&gt; arrange(desc(total)) |&gt; head() #&gt; state abb region population total ratio #&gt; 1 California CA West 37253956 1257 3.374138 #&gt; 2 Texas TX South 25145561 805 3.201360 #&gt; 3 Florida FL South 19687653 669 3.398069 #&gt; 4 New York NY Northeast 19378102 517 2.667960 #&gt; 5 Pennsylvania PA Northeast 12702379 457 3.597751 #&gt; 6 Michigan MI North Central 9883640 413 4.178622 We see that in California 1257 cases were reported. That is one of the extreme data points we see in the box plot. 3.7.3 Examples Create variable pop_log10 and store log base 10 data of population (log10() function). Perform the same log base 10 transformation for total murders and store it in variable tot_log10. Generate a scatter plot of these two variables. pop_log10 &lt;- log10(murders$population) tot_log10 &lt;- log10(murders$total) plot(pop_log10, tot_log10) Create a histogram of population in millions (divided by \\({10}^6\\)). hist(murders$population/10^6) Create a box plot of population. boxplot(murders$population) 3.8 Exercises Below, you will find a series of exercises with different levels of difficulty. It is time to put into practice what you have learned in this chapter. Remember you can use dplyr functions like filter(), arrange(), mutate(), summarize(), group_by() and left_join() to manipulate data frames. Create a data frame called my_expenses. It should contain a category factor with levels “Housing”, “Transport”, “Food”, and “Entertainment”, along with three numeric columns (january, february, march) recording expenses for each category. Solution my_expenses &lt;- data.frame( category = factor(c(&quot;Housing&quot;, &quot;Transport&quot;, &quot;Food&quot;, &quot;Entertainment&quot;)), january = c(1500, 300, 500, 200), february = c(1500, 250, 400, 150), march = c(1500, 350, 550, 250) ) my_expenses #&gt; category january february march #&gt; 1 Housing 1500 1500 1500 #&gt; 2 Transport 300 250 350 #&gt; 3 Food 500 400 550 #&gt; 4 Entertainment 200 150 250 Use head(), tail(), str() and summary() functions to explore my_expenses data frame. Solution head(my_expenses) #&gt; category january february march #&gt; 1 Housing 1500 1500 1500 #&gt; 2 Transport 300 250 350 #&gt; 3 Food 500 400 550 #&gt; 4 Entertainment 200 150 250 tail(my_expenses) #&gt; category january february march #&gt; 1 Housing 1500 1500 1500 #&gt; 2 Transport 300 250 350 #&gt; 3 Food 500 400 550 #&gt; 4 Entertainment 200 150 250 str(my_expenses) #&gt; &#39;data.frame&#39;: 4 obs. of 4 variables: #&gt; $ category: Factor w/ 4 levels &quot;Entertainment&quot;,..: 3 4 2 1 #&gt; $ january : num 1500 300 500 200 #&gt; $ february: num 1500 250 400 150 #&gt; $ march : num 1500 350 550 250 summary(my_expenses) #&gt; category january february march #&gt; Entertainment:1 Min. : 200 Min. : 150 Min. : 250.0 #&gt; Food :1 1st Qu.: 275 1st Qu.: 225 1st Qu.: 325.0 #&gt; Housing :1 Median : 400 Median : 325 Median : 450.0 #&gt; Transport :1 Mean : 625 Mean : 575 Mean : 662.5 #&gt; 3rd Qu.: 750 3rd Qu.: 675 3rd Qu.: 787.5 #&gt; Max. :1500 Max. :1500 Max. :1500.0 Access february column of my_expenses data frame using $ operator. Then, access the second row of the data frame using brackets. Solution my_expenses$february #&gt; [1] 1500 250 400 150 my_expenses[2, ] #&gt; category january february march #&gt; 2 Transport 300 250 350 Filter my_expenses data frame to get only rows where expenses in january are greater than 400. Solution my_expenses |&gt; filter(january &gt; 400) #&gt; category january february march #&gt; 1 Housing 1500 1500 1500 #&gt; 2 Food 500 400 550 Sort my_expenses data frame descendingly by expenses in march. Solution my_expenses |&gt; arrange(desc(march)) #&gt; category january february march #&gt; 1 Housing 1500 1500 1500 #&gt; 2 Food 500 400 550 #&gt; 3 Transport 300 250 350 #&gt; 4 Entertainment 200 150 250 Add a column called total to my_expenses data frame containing the sum of January, February, and March expenses for each category. Solution my_expenses &lt;- my_expenses |&gt; mutate(total = january + february + march) Calculate mean and standard deviation of total expenses for each category in my_expenses data frame. Solution my_expenses |&gt; summarize(mean_total = mean(total), std_total = sd(total)) #&gt; mean_total std_total #&gt; 1 1862.5 1793.216 Group my_expenses data frame by category and calculate sum of expenses for each month. Solution my_expenses |&gt; group_by(category) |&gt; summarize(sum_january = sum(january), sum_february = sum(february), sum_march = sum(march)) #&gt; # A tibble: 4 × 4 #&gt; category sum_january sum_february sum_march #&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Entertainment 200 150 250 #&gt; 2 Food 500 400 550 #&gt; 3 Housing 1500 1500 1500 #&gt; 4 Transport 300 250 350 Visually analyze the following chart describing total murder distribution by regions. Just by visualizing it, could you point out which region has the smallest data range, ignoring outliers? Which region has the highest median? Solution West has the smallest data range and has two outliers. South has the highest median among all regions. Analyzing solely by seeing a chart allows us to put ourselves in the final observer’s shoes and understand if decisions can be made just with presented information. Create south vector where you store filtered data of total murders occurred in South region. Then, create a histogram of south vector. Solution south &lt;- murders |&gt; filter(region == &quot;South&quot;) |&gt; pull(total) hist(south) Create a new data frame called df_cities_climate combining information from df_cities and df_climate (you must create df_climate data frame with city climate information). Ensure resulting data frame contains all cities from df_cities, even if they don’t have climate information in df_climate. Solution df_climate &lt;- data.frame( city = c(&quot;New York&quot;, &quot;Los Angeles&quot;, &quot;Chicago&quot;), average_temperature = c(12.8, 17.7, 10.7), # In degrees Celsius annual_precipitation = c(1269, 373, 965) # In millimeters ) df_cities_climate &lt;- left_join(df_cities, df_climate, by = &quot;city&quot;) Create a data frame with some missing values (NA). Then, replace missing values with mean of non-missing values in same column. Solution # Create data frame with missing values df_with_na &lt;- data.frame( x = c(1, 2, NA, 4, 5), y = c(NA, 7, 8, NA, 10) ) df_with_na #&gt; x y #&gt; 1 1 NA #&gt; 2 2 7 #&gt; 3 NA 8 #&gt; 4 4 NA #&gt; 5 5 10 # Replace missing values with mean df_with_na &lt;- df_with_na |&gt; mutate(x = ifelse(is.na(x), mean(x, na.rm = TRUE), x), y = ifelse(is.na(y), mean(y, na.rm = TRUE), y)) df_with_na #&gt; x y #&gt; 1 1 8.333333 #&gt; 2 2 7.000000 #&gt; 3 3 8.000000 #&gt; 4 4 8.333333 #&gt; 5 5 10.000000 Create a function called clean_data_frame() receiving a data frame as argument and replacing missing values with mean of non-missing values in each column. Solution clean_data_frame &lt;- function(df) { for (col in names(df)) { if (is.numeric(df[[col]])) { df[[col]] &lt;- ifelse(is.na(df[[col]]), mean(df[[col]], na.rm = TRUE), df[[col]]) } } return(df) } ## Test created function # Create data frame with missing values to test function df_test &lt;- data.frame( age = c(25, 30, NA, 28, 35), height = c(1.75, 1.80, 1.65, NA, 1.70), weight = c(70, 80, 75, 65, NA) ) df_test #&gt; age height weight #&gt; 1 25 1.75 70 #&gt; 2 30 1.80 80 #&gt; 3 NA 1.65 75 #&gt; 4 28 NA 65 #&gt; 5 35 1.70 NA # Apply function to test data frame df_clean &lt;- clean_data_frame(df_test) # Show clean data frame df_clean #&gt; age height weight #&gt; 1 25.0 1.750 70.0 #&gt; 2 30.0 1.800 80.0 #&gt; 3 29.5 1.650 75.0 #&gt; 4 28.0 1.725 65.0 #&gt; 5 35.0 1.700 72.5 References "],["advanced-techniques.html", "Chapter 4 Advanced Techniques 4.1 Metaprogramming: writing code that writes code 4.2 Functional programming: a new paradigm 4.3 R6: The future of OOP in R 4.4 Exercises", " Chapter 4 Advanced Techniques 4.1 Metaprogramming: writing code that writes code In previous chapters, we explored different object types in R and how to use functions to manipulate them. Now, we are going to delve into a more advanced concept: metaprogramming. Metaprogramming is a technique that allows us to write code that generates other code. It’s like having a code factory where we can create new functions and expressions dynamically. Why is this useful? Metaprogramming can be very useful for: Metaprogramming is incredibly useful for automating repetitive tasks, allowing us to generate boilerplate code dynamically. It also enables us to create more flexible functions that adapt to different data structures, and facilitates writing concise, expressive code that captures complex logic simply. In R, metaprogramming is based on the manipulation of expressions. An expression is a representation of R code as an object. We can create expressions, modify them, and evaluate them to generate new code. 4.1.1 Manipulating expressions: The art of sculpting code In R, metaprogramming relies on manipulating expressions. An expression is a representation of R code as an object. Instead of simply executing the code, we can manipulate it as if it were a block of clay, shaping and modifying it to create new expressions and functions. Think of an expression like a cooking recipe. The recipe contains a set of instructions (ingredients and steps to follow) to create a dish. Similarly, an expression in R contains instructions to perform a task. R offers us several tools to manipulate expressions, as if they were the hands of a sculptor shaping clay: R provides a toolkit for sculpting expressions. The quote() function captures code as an expression without running it, like saving a recipe for later. substitute() allows you to inject values into an expression, replacing placeholders with actual variables. To execute these stored expressions, we use eval(), which runs the code and returns the result. Finally, parse() can turn text strings directly into executable expressions. With these tools, we can manipulate expressions to create new functions, modify the behavior of existing functions, and generate code dynamically. 4.1.2 Examples Metaprogramming might seem like an abstract concept at first, but its applications are very concrete and powerful. Let’s look at some examples of how we can use metaprogramming in R to create dynamic functions and generate code automatically. Example 1: Creating a function that generates other functions Imagine you need to create several functions performing similar operations, but with some different parameters. For example, functions adding different constants to a number. Instead of writing each function separately, you can use metaprogramming to create a function that generates these functions dynamically. create_sum_function &lt;- function(n) { expression &lt;- substitute(function(x) x + n) eval(expression) } sum_5 &lt;- create_sum_function(5) sum_10 &lt;- create_sum_function(10) sum_5(10) #&gt; [1] 15 sum_10(10) #&gt; [1] 20 In this example, the create_sum_function() function receives a number n as an argument and generates a new function adding n to its argument. The substitute() function is used to create an expression representing the function we want to generate, and the eval() function is used to evaluate the expression and create the function. Example 2: Generating code for data analysis Suppose you want to perform a data analysis involving several steps, such as filtering data, calculating statistics, and generating a plot. You can use metaprogramming to generate the code for this analysis dynamically, based on specified parameters. analyze_data &lt;- function(data, filter_cond, column_to_analyze, statistic, plot_type) { # Filter data filtered_data &lt;- substitute(data[filter_cond, ][[column_to_analyze]]) filtered_data &lt;- eval(filtered_data) # Calculate statistic calculated_statistic &lt;- substitute(statistic(filtered_data)) calculated_statistic &lt;- eval(calculated_statistic) # Generate plot plot_expression &lt;- substitute(plot_type(filtered_data)) eval(plot_expression) # Return calculated statistic return(calculated_statistic) } # Usage example df &lt;- data.frame( x = c(1, 3, 2, 5.5, 4, 3.5, 8, 7, 9, 10), y = c(10, 8, 9, 6, 7, 5, 3.6, 4, 2, 1) ) # We want to filter data where x &gt; 5, calculate mean of y and generate a histogram result &lt;- analyze_data(df, df$x &gt; 5, &quot;y&quot;, mean, hist) result #&gt; [1] 3.32 Example 3: Creating a function to generate plots with dynamic variable names and advanced options Imagine you need to create a function generating different types of plots (scatter, histograms, boxplots) with custom options like titles, labels, colors, and legends, and that can also handle different datasets and variables. In this case, metaprogramming can be very useful to create a flexible function adapting to these needs. create_plot &lt;- function(data, plot_type, var_x, var_y = NULL, title = NULL, color = &quot;blue&quot;, labels_x = NULL, labels_y = NULL, legend = NULL) { # Create base plot expression if (plot_type == &quot;scatter&quot;) { expression &lt;- substitute(plot(data[[var_x]], data[[var_y]], xlab = labels_x, ylab = labels_y, main = title, col = color)) } else if (plot_type == &quot;histogram&quot;) { expression &lt;- substitute(hist(data[[var_x]], main = title, xlab = labels_x, col = color)) } else if (plot_type == &quot;boxplot&quot;) { expression &lt;- substitute(boxplot(data[[var_x]], main = title, ylab = labels_y, col = color)) } else { stop(&quot;Invalid plot type.&quot;) } # Evaluate base expression eval(expression) # Add legend if specified if (!is.null(legend)) { legend(&quot;topright&quot;, legend = legend, fill = color) } } # Usage example df &lt;- data.frame( x = c(1, 3, 2, 5.5, 4, 3.5, 8, 7, 9, 10), y = c(10, 8, 9, 6, 7, 5, 3.6, 4, 2, 1) ) create_plot(df, &quot;scatter&quot;, &quot;x&quot;, &quot;y&quot;, title = &quot;Scatter Plot&quot;, color = &quot;red&quot;, labels_x = &quot;Variable X&quot;, labels_y = &quot;Variable Y&quot;) create_plot(df, &quot;histogram&quot;, &quot;x&quot;, title = &quot;Histogram of X&quot;, color = &quot;green&quot;, labels_x = &quot;Variable X&quot;) create_plot(df, &quot;boxplot&quot;, &quot;y&quot;, title = &quot;Boxplot of Y&quot;, color = &quot;blue&quot;, labels_y = &quot;Variable X&quot;, legend = c(&quot;Group A&quot;)) In this example, the create_plot() function can generate different types of plots with custom options. The function uses substitute() to construct the base plot expression, and then eval() to evaluate the expression and generate the plot. Additionally, the function can add a legend to the plot if the legend argument is specified. This example illustrates how metaprogramming can be useful for creating more flexible and complex functions that adapt to different needs. 4.2 Functional programming: a new paradigm In previous chapters, we explored different object types in R and how to use functions to manipulate them. We have also seen how metaprogramming allows us to write code that generates other code. Now, we are going to delve into a different programming paradigm: functional programming. Functional programming is a programming style based on the use of pure functions and data immutability. Functional programming relies on two core concepts: pure functions and immutability. A pure function is consistent and side-effect-free, meaning it always produces the same output for the same input and does not modify any external state. Immutability ensures that data is not changed after creation; instead of modifying an existing object, we create a new one with the desired changes. These principles make functional programming easier to reason about, debug, and maintain. It also facilitates writing concurrent and parallel code, as pure functions have no side effects that can interfere with other processes. 4.2.1 Basic principles of functional programming Functional programming rests on several pillars. First, functions are first-class citizens, meaning they can be assigned to variables and passed as arguments just like data. Second, it relies on pure functions that produce consistent outputs without side effects. Third, it emphasizes immutability, creating new data rather than modifying existing objects. Finally, it typically rejects loops, facilitating data processing through higher-order functions instead. 4.2.2 Higher-order functions in R R offers several higher-order functions that are especially useful for functional programming. These functions allow us to manipulate vectors, lists, and other objects concisely and efficiently, avoiding the use of for and while loops. The purrr package offers variants of map() for different types of results: map_dbl() to get a numeric vector, map_chr() to get a character vector, map_lgl() to get a logical vector, etc. The purrr package provides a robust suite of tools. map() applies a function to each element of a list or vector, returning a new list (or vector with variants like map_dbl). reduce() performs a cumulative operation, combining elements one by one until a single result remains. keep() acts as a filter, retaining only those elements that satisfy a given condition. The ~ symbol in higher-order functions is used to define an anonymous function. This means you are creating a function “on the fly”, without needing to give it an explicit name. The part following ~ is the body of this function, specifying operations to be performed on each element of the vector or list to which the function is applied. The dot . is used as a placeholder to refer to the current element. These functions, along with other higher-order functions like map2(), pmap(), accumulate(), and every(), give us great flexibility for processing data functionally in R. 4.2.3 Examples Let’s see some examples of how to apply functional programming in R: Let’s put these concepts into practice with some concrete examples. First, consider a scenario where we want to calculate the sum of squares of even numbers in a vector. ``` r numbers &lt;- c(1, 2, 3, 4, 5) sum_squares_evens &lt;- numbers |&gt; keep(~. %% 2 == 0) |&gt; map_dbl(~. ^2) |&gt; reduce(`+`) sum_squares_evens #&gt; [1] 20 ``` For a second example, let’s filter a list of cities to find those with a population greater than 5 million. ``` r cities &lt;- list( list(name = &quot;New York&quot;, population = 8.4e6), list(name = &quot;Los Angeles&quot;, population = 3.9e6), list(name = &quot;Chicago&quot;, population = 2.7e6) ) big_cities &lt;- cities |&gt; keep(~.x$population &gt; 5e6) big_cities #&gt; [[1]] #&gt; [[1]]$name #&gt; [1] &quot;New York&quot; #&gt; #&gt; [[1]]$population #&gt; [1] 8400000 ``` In this example, &quot;x&quot; acts as a placeholder to represent each element of the `cities` list as it iterates over it. That is, in each iteration, &quot;x&quot; will take the value of one of the cities in the list. You might wonder why we use .x in these expressions. This placeholder serves three main purposes. First, it allows us to define an anonymous function—a quick, unnamed function (~ .x$population &gt; 5e6) that evaluates whether a city meets our criteria. Second, it provides a way to access elements; the .x represents the current list item, allowing us to grab properties like .x$population. Finally, it promotes conciseness, enabling us to write compact, readable code without formally defining a separate function for a simple operation. You can technically use other variable names, but .x is the standard convention in purrr. Functional programming is a powerful paradigm that can help you write cleaner, more efficient, and maintainable code. As you become familiar with its principles and tools, you will be able to apply them to a wide variety of data analysis problems. 4.3 R6: The future of OOP in R For advanced Object-Oriented Programming (OOP) using the R6 package, please refer to Appendix B. 4.4 Exercises Below, you will find a series of exercises with different levels of difficulty. It is time to put into practice what you have learned in this chapter. Formulate an expression that represents the sum of two variables, a and b. Solution expression &lt;- quote(a + b) Compose an expression for the multiplication of x and y, then execute it to find the result. Solution x &lt;- 5 y &lt;- 10 expression &lt;- quote(x * y) eval(expression) #&gt; [1] 50 Generate a numeric vector and apply the map() function to compute the square of each element. Solution numbers &lt;- c(1, 2, 3, 4, 5) squares &lt;- map(numbers, function(x) x^2) squares #&gt; [[1]] #&gt; [1] 1 #&gt; #&gt; [[2]] #&gt; [1] 4 #&gt; #&gt; [[3]] #&gt; [1] 9 #&gt; #&gt; [[4]] #&gt; [1] 16 #&gt; #&gt; [[5]] #&gt; [1] 25 Define a vector of numbers and utilize keep() from the purrr package to retain only the even values. Solution numbers &lt;- c(1, 2, 3, 4, 5) evens &lt;- keep(numbers, ~ . %% 2 == 0) evens #&gt; [1] 2 4 Build a function named create_power_function() that takes a number n and returns a new function capable of raising its input to the power of n. Solution create_power_function &lt;- function(n) { function(x) x^n } Construct a numeric vector and apply reduce() to calculate the product of all its elements. Solution numbers &lt;- c(1, 2, 3, 4, 5) product &lt;- reduce(numbers, `*`) product #&gt; [1] 120 Design a function create_flexible_sum_function() that accepts a number n and yields a function that adds n to the sum of any arguments passed to it. Solution create_flexible_sum_function &lt;- function(n) { function(...) { sum(c(...)) + n } } # Tests # Create a function adding 5 to any set of numbers sum_5 &lt;- create_flexible_sum_function(5) # Usage examples and verification sum_5(2, 3, 4) #&gt; [1] 14 sum_5(10, 20) #&gt; [1] 35 sum_5() #&gt; [1] 5 Develop a create_dynamic_plot() function that takes a data frame, a plot type (“scatter”, “histogram”, or “boxplot”), and a list of options (like title and color), generating the requested plot dynamically. Solution create_dynamic_plot &lt;- function(data, plot_type, options) { # Create base plot expression if (plot_type == &quot;scatter&quot;) { expression &lt;- quote(plot(data[[options$var_x]], data[[options$var_y]], xlab = options$labels_x, ylab = options$labels_y, main = options$title, col = options$color)) } else if (plot_type == &quot;histogram&quot;) { expression &lt;- quote(hist(data[[options$var_x]], main = options$title, xlab = options$labels_x, col = options$color)) } else if (plot_type == &quot;boxplot&quot;) { expression &lt;- quote(boxplot(data[[options$var_x]], main = options$title, ylab = options$labels_y, col = options$color)) } else { stop(&quot;Invalid plot type.&quot;) } # Evaluate base expression eval(expression) } # Create sample data data &lt;- data.frame(x = rnorm(100), y = rnorm(100)) # Tests # Scatter plot options_scatter &lt;- list(var_x = &quot;x&quot;, var_y = &quot;y&quot;, title = &quot;Scatter Plot&quot;, labels_x = &quot;Variable X&quot;, labels_y = &quot;Variable Y&quot;, color = &quot;blue&quot;) create_dynamic_plot(data, &quot;scatter&quot;, options_scatter) # Histogram options_histogram &lt;- list(var_x = &quot;x&quot;, title = &quot;Histogram&quot;, labels_x = &quot;Values&quot;, color = &quot;green&quot;) create_dynamic_plot(data, &quot;histogram&quot;, options_histogram) # Boxplot options_boxplot &lt;- list(var_x = &quot;y&quot;, title = &quot;Boxplot&quot;, labels_y = &quot;Values&quot;, color = &quot;red&quot;) create_dynamic_plot(data, &quot;boxplot&quot;, options_boxplot) "],["ggplot-and-dplyr.html", "Chapter 5 Ggplot and dplyr 5.1 Creating the ggplot object 5.2 Aesthetic mapping layer 5.3 Geoms layer 5.4 Scale layer 5.5 Label, title and legend layer 5.6 Reference lines 5.7 Changing the plot style 5.8 Saving plots 5.9 Summarizing data with dplyr 5.10 Exercises 5.11 Key Takeaways", " Chapter 5 Ggplot and dplyr Learning Objectives: In this chapter, we will master the art of data storytelling. We will learn to create layered visualizations using ggplot2, customizing every aspect from aesthetics and scales to themes and labels. Additionally, we will gain proficiency in summarizing data with dplyr functions like summarize() and group_by(), all while deepening our understanding of the grammar of graphics approach. Building on the basic plotting skills from previous chapters, we’ll now create more sophisticated and visually polished visualizations. The ggplot2 package provides a powerful and consistent framework for building graphics layer by layer. To get started, we’ll use the ggplot object included in the tidyverse library (the tidyverse package includes ggplot2 among its core packages). Since we’ve already installed tidyverse previously, we only need to load it: library(tidyverse) We’ll learn these visualization techniques using our previous case/problem, allowing us to build up our ggplot skills gradually with familiar data. 5.1 Creating the ggplot object We will start by creating the ggplot object from the murders data using the pipeline operator |&gt;. Let’s also remember to have loaded the murders data from the dslabs library. library(dslabs) data(murders) murders |&gt; ggplot() This code only shows us an empty box. This is because we haven’t specified which variables to take from the data frame nor what type of plot we want. To build our plot, we’ll add components using layers. The ggplot system allows us to add one layer at a time, each specifying a different aspect of the visualization. We connect layers using the + operator. 5.2 Aesthetic mapping layer First we will focus on the basic aesthetics, that is: what goes on the x-axis and what we put on the y-axis. To do this we will use the aesthetic function which in R is aes(). For example, let’s add the population data on the x-axis and the total data on the y-axis. We don’t have to use the $ accessor because the aes function takes the murders table before the pipeline as a reference. murders |&gt; ggplot() + aes(x = population, y = total) Now we have a box with the axes marked, but still without any data inside the box. 5.3 Geoms layer Let’s add one more layer that indicates what type of plot we want. To do this we will use the so-called geoms. There are different types of geoms. For example, a scatter plot is shown with points, therefore we will use the geom_point() function. For more detail we can see the documentation of geom_point() here. murders |&gt; ggplot() + aes(x = population, y = total) + geom_point() In the same way, we can show lines connecting the data instead of points with the geom_line() function. murders |&gt; ggplot() + aes(x = population, y = total) + geom_line() Up to this point we have created the same scatter plot that we saw in the previous chapter. The power of ggplot lies in the ease of adding components. For example, to label each point with the state abbreviation (abb), we simply add it as a label attribute inside aes and include the geom_text() layer. murders |&gt; ggplot() + aes(x = population, y = total, label=abb) + geom_point() + geom_text() In this plot we can already see that the upper right point corresponds to CA which is the abbreviation for the state of California. 5.3.1 Tweaking aes and geoms We can tweak our plots in multiple ways by adding attributes to our functions. For example, if we want to identify which region each point belongs to (if it is from the US North, South, etc.) we would have to edit aes() and make color take into account the region variable as follows: murders |&gt; ggplot() + aes(x = population, y = total, label=abb, color=region) + geom_point() + geom_text() Then, we can also edit the attributes of the geoms. For example, let’s make the size of the points larger. To do this we edit inside geom_points(): murders |&gt; ggplot() + aes(x = population, y = total, label=abb, color=region) + geom_point(size=3) + geom_text() Having increased the size of the points, we can no longer see the text of the state abbreviations well. We can nudge the text on the x-axis or on the y-axis. Since we are talking about several million people, let’s nudge the letters 1.5 million to the right. murders |&gt; ggplot() + aes(x = population, y = total, label=abb, color=region) + geom_point(size=3) + geom_text(nudge_x = 1500000) To avoid entering such large numbers we can transform the population on the x-axis in the aes() function. Thus, once we express the data without counting the millions we would have to nudge the text only 1.5 points to the right: murders |&gt; ggplot() + aes(x = population/10^6, y = total, label=abb, color=region) + geom_point(size=3) + geom_text(nudge_x = 1.5) This transformation gives us the same result as before and the x-axis is now easier to understand now that we can see the numbers. 5.4 Scale layer Visually we can still improve our plot further. We see several data points concentrated in lower values and only a few extremes. In those cases it is better to have a view scaling the axes using logarithms. To do this, we will use the layers scale_x_continuous() and scale_y_continuous(). For example, if we want to transform the scale to base 2 logarithm we would have to add layers, but also change the value of nudge_x, due to the scale change: murders |&gt; ggplot() + aes(x = population/10^6, y = total, label=abb, color=region) + geom_point(size=3) + geom_text(nudge_x = 0.23) + scale_x_continuous(trans = &quot;log2&quot;) + scale_y_continuous(trans = &quot;log2&quot;) In the same way, we could do the transformation to base 10 logarithm: murders |&gt; ggplot() + aes(x = population/10^6, y = total, label=abb, color=region) + geom_point(size=3) + geom_text(nudge_x = 0.075) + scale_x_continuous(trans = &quot;log10&quot;) + scale_y_continuous(trans = &quot;log10&quot;) The transformation of the scale to base 10 logarithm is widely used in statistics and R provides us with a faster function to proceed with this scale transformation, the function scale_x_log10(), which gives us the same result as the previous plot. murders |&gt; ggplot() + aes(x = population/10^6, y = total, label=abb, color=region) + geom_point(size=3) + geom_text(nudge_x = 0.075) + scale_x_log10() + scale_y_log10() 5.5 Label, title and legend layer We can also change the labels (label in English) of the plot. So far on the x-axis we see that population/10^6 appears and we can change it with the function xlab(). In the same way we can change on the y-axis using ylab(). To add a title to the plot we will use the function ggtitle(). To change the name of the legend we will use the function scale_color_discrete(). murders |&gt; ggplot() + aes(x = population/10^6, y = total, label=abb, color=region) + geom_point(size=3) + geom_text(nudge_x = 0.075) + scale_x_log10() + scale_y_log10() + xlab(&quot;Population expressed in millions (logarithmic scale)&quot;) + ylab(&quot;Total number of murders (logarithmic scale)&quot;) + ggtitle(&quot;Gun murders in the US in 2010&quot;) + scale_color_discrete(name = &quot;Regions&quot;) Modern Alternative: You can also use the labs() function to set multiple labels in a single layer: labs(x = \"...\", y = \"...\", title = \"...\", color = \"...\"). This is often cleaner for plots with many labels. 5.6 Reference lines We can add reference lines, whether vertical with geom_vline(xintercept = ), horizontal with geom_hline(yintercept = ...) or diagonal with geom_abline(intercept = ), the latter asks us at what point it cuts the y-axis and draws a line with a default slope of 1. For example, we could calculate the average of total murders and draw a horizontal reference line. #Calculate the average of the total avg_total &lt;- mean(murders$total) #And add the horizontal reference line murders |&gt; ggplot() + aes(x = population/10^6, y = total, label=abb, color=region) + geom_point(size=3) + geom_text(nudge_x = 0.075) + scale_x_log10() + scale_y_log10() + xlab(&quot;Population expressed in millions (logarithmic scale)&quot;) + ylab(&quot;Total number of murders (logarithmic scale)&quot;) + ggtitle(&quot;Gun murders in the US in 2010&quot;) + scale_color_discrete(name = &quot;Regions&quot;) + geom_hline(yintercept = avg_total) Or we could calculate the murder rate per million inhabitants throughout the US and draw a reference diagonal. In the case of the diagonal we have to express it in the same scale of the axis, therefore we have to convert it to log10. # Calculate the average rate ratio &lt;- sum(murders$total)/sum(murders$population) * 10^6 # Calculate base 10 logarithm to obtain the intercept on the &quot;y-axis&quot; ratio_log10 &lt;- log10(ratio) #And add the diagonal reference line murders |&gt; ggplot() + aes(x = population/10^6, y = total, label=abb, color=region) + geom_point(size=3) + geom_text(nudge_x = 0.075) + scale_x_log10() + scale_y_log10() + xlab(&quot;Population expressed in millions (logarithmic scale)&quot;) + ylab(&quot;Total number of murders (logarithmic scale)&quot;) + ggtitle(&quot;Gun murders in the US in 2010&quot;) + scale_color_discrete(name = &quot;Regions&quot;) + geom_abline(intercept = ratio_log10) We can improve this reference line by making it dashed and gray. To do this, simply edit the geom_abline() function as follows: murders |&gt; ggplot() + aes(x = population/10^6, y = total, label=abb, color=region) + geom_point(size=3) + geom_text(nudge_x = 0.075) + scale_x_log10() + scale_y_log10() + xlab(&quot;Population expressed in millions (logarithmic scale)&quot;) + ylab(&quot;Total number of murders (logarithmic scale)&quot;) + ggtitle(&quot;Gun murders in the US in 2010&quot;) + scale_color_discrete(name = &quot;Regions&quot;) + geom_abline(intercept = ratio_log10, lty = 2, color = &quot;darkgrey&quot;) 5.7 Changing the plot style The plot style using ggplot() can be easily changed. There are multiple themes we can use by loading the ggthemes library. We can, for example, use a widely used theme: the economist theme by adding the theme_economist() layer. library(ggthemes) murders |&gt; ggplot() + aes(x = population/10^6, y = total, label=abb, color=region) + geom_point(size=3) + geom_text(nudge_x = 0.075) + scale_x_log10() + scale_y_log10() + xlab(&quot;Population expressed in millions (logarithmic scale)&quot;) + ylab(&quot;Total number of murders (logarithmic scale)&quot;) + ggtitle(&quot;Gun murders in the US in 2010&quot;) + scale_color_discrete(name = &quot;Regions&quot;) + geom_abline(intercept = ratio_log10, lty = 2, color = &quot;darkgrey&quot;) + theme_economist() We still see overlapping abbreviations. We can make the names repel each other using the geom_text_repel() function instead of geom_text() that we are currently using. To use this function we need to call the ggrepel library. library(ggthemes) library(ggrepel) murders |&gt; ggplot() + aes(x = population/10^6, y = total, label=abb, color=region) + geom_point(size=3) + geom_text_repel() + scale_x_log10() + scale_y_log10() + xlab(&quot;Population expressed in millions (logarithmic scale)&quot;) + ylab(&quot;Total number of murders (logarithmic scale)&quot;) + ggtitle(&quot;Gun murders in the US in 2010&quot;) + scale_color_discrete(name = &quot;Regions&quot;) + geom_abline(intercept = ratio_log10, lty = 2, color = &quot;darkgrey&quot;) + theme_economist() #&gt; Warning: ggrepel: 1 unlabeled data points (too many overlaps). Consider #&gt; increasing max.overlaps This plot is visually much easier to understand and aesthetically much better than the default plot we created in previous chapters. We can explore more examples at this link. 5.8 Saving plots Once you’ve created a plot, you’ll often want to save it to a file. The ggsave() function makes this easy: # Save the last plot created ggsave(&quot;my_plot.png&quot;, width = 8, height = 6) # Or save a specific plot object p &lt;- murders |&gt; ggplot() + aes(x = population, y = total) + geom_point() ggsave(&quot;murders_plot.pdf&quot;, plot = p) The function automatically detects the file type from the extension (.png, .pdf, .svg, etc.). 5.9 Summarizing data with dplyr Visualization and data summarization go hand in hand. While plots help us see patterns in our data, summary statistics help us quantify those patterns. The dplyr package provides powerful functions for calculating summaries. The tidyverse package includes several packages, among them dplyr which makes it easier for us to summarize data from a variable. When we call the tidyvere library we are also calling all the functions of dplyr. To start using the functions of dplyr we are going to load the heights data frame which is in the dslabs library. library(tidyverse) library(dslabs) First let’s understand the heights data frame, we can apply pipeline and then use the head() function: heights |&gt; head() #&gt; sex height #&gt; 1 Male 75 #&gt; 2 Male 70 #&gt; 3 Male 68 #&gt; 4 Male 74 #&gt; 5 Male 61 #&gt; 6 Female 65 This data frame corresponds to the list of attributes of students in a university, where the height column indicates the height of each one in inches. Finally, let’s add the column height_m where we will transform the height to meters. Remember that a meter has 39.37 inches. Let’s store the result in the variable heights_m. heights_m &lt;- heights |&gt; mutate(height_m = height/39.37) heights_m |&gt; head() #&gt; sex height height_m #&gt; 1 Male 75 1.905004 #&gt; 2 Male 70 1.778004 #&gt; 3 Male 68 1.727203 #&gt; 4 Male 74 1.879604 #&gt; 5 Male 61 1.549403 #&gt; 6 Female 65 1.651003 The fastest way to summarize a list of data is indicating what the average is and how much its standard deviation6 is. If we wanted to obtain the average we would use the mean() function and sd() to obtain the standard deviation. For example: avg &lt;- mean(heights_m$height_m) std_dev &lt;- sd(heights_m$height_m) However, this summarizes all students for us without considering whether men could be on average taller than women. If we wanted to calculate the average and std. dev. we would have to filter first, then store in a variable and finally calculate the average and standard deviation. This is impractical and for that dplyr grants us the summarize() function. 5.9.1 Summarize function We can use the summarize function using the pipeline operator. Thus, we could calculate the average and std. dev. in this way: heights_m |&gt; filter(sex == &quot;Male&quot;) |&gt; summarize(avg = mean(height_m), std_dev = sd(height_m)) #&gt; avg std_dev #&gt; 1 1.760598 0.09172018 This function also generates a data frame for us. We can validate it if we store the result in the variable heights_m_male and then report the class of the variable: heights_m_male &lt;- heights_m |&gt; filter(sex == &quot;Male&quot;) |&gt; summarize(avg = mean(height_m), std_dev = sd(height_m)) class(heights_m_male) #&gt; [1] &quot;data.frame&quot; We can report the data frame heights_m_male and use it for future analyzes accessing with the accessor $. heights_m_male #&gt; avg std_dev #&gt; 1 1.760598 0.09172018 We see that the average height of men is 1.76 meters with a standard deviation of 0.09 meters. In the same way, we can use summarize to calculate other functions such as: heights_m |&gt; filter(sex == &quot;Male&quot;) |&gt; summarize(min_val = min(height_m), max_val = max(height_m), median_val = median(height_m)) #&gt; min_val max_val median_val #&gt; 1 1.270003 2.100004 1.752604 The tallest student measures more than 2.1 meters. Half of the male students measure more than 1.75 meters. However, we would now have to change to “Female” to calculate the data for women. We need to group and then summarize the data taking into account the grouping. For this there is the function group_by() 5.9.2 Group By Function This function allows us to create grouped data frames which makes it easier for us to summarize the data. We would only have to select based on what we want to group and no longer filter by sex. In this case the grouping would be based on the sex column: heights_m |&gt; group_by(sex) |&gt; summarize(avg = mean(height_m), std_dev = sd(height_m)) #&gt; # A tibble: 2 × 3 #&gt; sex avg std_dev #&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Female 1.65 0.0955 #&gt; 2 Male 1.76 0.0917 On average, women are shorter than men. If we now remember our danger case in the US. We can calculate the ratio of total crimes regarding the population and then compare it by region in this way: murders |&gt; mutate(ratio = total / population * 100000) |&gt; group_by(region) |&gt; summarize(avg_ratio = mean(ratio)) #&gt; # A tibble: 4 × 2 #&gt; region avg_ratio #&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 Northeast 1.85 #&gt; 2 South 4.42 #&gt; 3 North Central 2.18 #&gt; 4 West 1.83 On average, the South region is more dangerous. 5.10 Exercises This time we are going to perform exercises within the field of biology and for this we must remember the parts of a flower. This way we will give more sense to the problem: Load the iris data frame (data(iris)), which details the characteristics of 150 flowers across 3 species. Create a scatter plot visualizing the relationship between sepal length and petal length. Solution iris |&gt; ggplot() + aes(x = Sepal.Length, y = Petal.Length) + geom_point() Enhance the previous visualization by coloring the points to represent the species of each flower. Solution iris |&gt; ggplot() + aes(x = Sepal.Length, y = Petal.Length, color = Species) + geom_point() Polish your plot by setting the title to “Relationship between sepal and petal size of different flowers”, naming the x-axis “Sepal length (in cm)”, the y-axis “Petal length (in cm)”, and labeling the legend simply as “Species”. Solution iris |&gt; ggplot() + aes(x = Sepal.Length, y = Petal.Length, color = Species) + geom_point() + xlab(&quot;Sepal length (in cm)&quot;) + ylab(&quot;Petal length (in cm)&quot;) + ggtitle(&quot;Relationship between sepal and petal size of different flowers&quot;) + scale_color_discrete(name = &quot;Species&quot;) Generate a statistical summary of the ratio between sepal length and petal length, reporting a data frame that includes the average, standard deviation, and median of this ratio. Solution iris |&gt; mutate(ratio = Sepal.Length / Petal.Length) |&gt; summarize(avg = mean(ratio), std_dev = sd(ratio), median_val = median(ratio)) Refine your previous ratio summary to show these statistics (average, standard deviation, and median) calculated separately for each species. Solution iris |&gt; mutate(ratio = Sepal.Length / Petal.Length) |&gt; group_by(Species) |&gt; summarize(avg = mean(ratio), std_dev = sd(ratio), median_val = median(ratio)) 5.11 Key Takeaways This chapter emphasized a layered approach to visualization, building plots incrementally with aesthetics, geoms, and scales. We learned to map data to visual properties using aes, select the appropriate geometry (like geom_point or geom_line), and transform axes to reveal patterns in skewed data. Beyond the visuals, we explored how to customize themes and labels for clarity, and how to quantify insights using dplyr summarization tools before saving our work with ggsave(). What is the standard deviation?↩︎ "],["gapminder.html", "Chapter 6 Gapminder 6.1 Initial gapminder plots 6.2 Facets 6.3 Time series 6.4 Exercises 6.5 Histograms with ggplot 6.6 Box plots with ggplot 6.7 Comparison of distributions 6.8 Exercises 6.9 Key Takeaways", " Chapter 6 Gapminder The Gapminder Foundation7 is a Swedish non-profit organization that promotes global development through the use of statistics that can help reduce common myths and sensationalist stories about global health and economics. An important selection of data is already loaded in the dslabs library in the gapminder data frame. Our case/problem now will be to answer these two questions: Is it still reasonable to divide the world between Western countries* and developing countries? Is it true that every day we are worse off and rich countries get richer while poor countries get poorer? (*): Samuel Huntington in 1993 published an article called Clash of Civilizations8 where he defined Western countries as those located in the regions of North America, Northern/Southern/Western Europe and Australia and New Zealand. To address these questions, we will follow a structured data science workflow. We’ll start by exploring the data to understand its structure and content, then move to in-depth analysis to identify relevant variables. Finally, we will use visualization and summarization techniques to synthesize our findings and provide clear answers. First let’s explore the structure of the data frame with str(): gapminder |&gt; str() #&gt; &#39;data.frame&#39;: 10545 obs. of 9 variables: #&gt; $ country : Factor w/ 185 levels &quot;Albania&quot;,&quot;Algeria&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... #&gt; $ year : int 1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ... #&gt; $ infant_mortality: num 115.4 148.2 208 NA 59.9 ... #&gt; $ life_expectancy : num 62.9 47.5 36 63 65.4 ... #&gt; $ fertility : num 6.19 7.65 7.32 4.43 3.11 4.55 4.82 3.45 2.7 5.57 ... #&gt; $ population : num 1636054 11124892 5270844 54681 20619075 ... #&gt; $ gdp : num NA 1.38e+10 NA NA 1.08e+11 ... #&gt; $ continent : Factor w/ 5 levels &quot;Africa&quot;,&quot;Americas&quot;,..: 4 1 1 2 2 3 2 5 4 3 ... #&gt; $ region : Factor w/ 22 levels &quot;Australia and New Zealand&quot;,..: 19 11 10 2 15 21 2 1 22 21 ... We have a data frame with more than 10 thousand data points and 9 variables. Now let’s take a look at the data with head(): gapminder |&gt; head() #&gt; country year infant_mortality life_expectancy fertility #&gt; 1 Albania 1960 115.40 62.87 6.19 #&gt; 2 Algeria 1960 148.20 47.50 7.65 #&gt; 3 Angola 1960 208.00 35.98 7.32 #&gt; 4 Antigua and Barbuda 1960 NA 62.97 4.43 #&gt; 5 Argentina 1960 59.87 65.39 3.11 #&gt; 6 Armenia 1960 NA 66.86 4.55 #&gt; population gdp continent region #&gt; 1 1636054 NA Europe Southern Europe #&gt; 2 11124892 13828152297 Africa Northern Africa #&gt; 3 5270844 NA Africa Middle Africa #&gt; 4 54681 NA Americas Caribbean #&gt; 5 20619075 108322326649 Americas South America #&gt; 6 1867396 NA Asia Western Asia Remember that for library data frames we can usually find the documentation and understand each attribute faster: ?gapminder Going directly to the questions would be not leaving that curiosity free to see what else is in the data. Thus, we are going to start with other variables such as infant mortality, fertility or population. We can filter all the data that are from Peru and select the column country, year, infant mortality and population: gapminder |&gt; filter(country == &quot;Peru&quot;) |&gt; select(country, year, infant_mortality, population) #&gt; country year infant_mortality population #&gt; 1 Peru 1960 135.9 10061519 #&gt; 2 Peru 1961 132.6 10350239 #&gt; 3 Peru 1962 129.1 10650672 #&gt; 4 Peru 1963 125.4 10961539 #&gt; 5 Peru 1964 121.8 11281015 #&gt; 6 Peru 1965 118.2 11607684 #&gt; 7 Peru 1966 114.8 11941327 #&gt; 8 Peru 1967 111.6 12282081 #&gt; 9 Peru 1968 108.7 12629333 #&gt; 10 Peru 1969 106.0 12982444 #&gt; 11 Peru 1970 103.4 13341071 #&gt; 12 Peru 1971 100.9 13704333 #&gt; 13 Peru 1972 98.3 14072476 #&gt; 14 Peru 1973 95.8 14447649 #&gt; 15 Peru 1974 93.3 14832839 #&gt; 16 Peru 1975 91.0 15229951 #&gt; 17 Peru 1976 88.9 15639898 #&gt; 18 Peru 1977 87.0 16061327 #&gt; 19 Peru 1978 85.5 16491087 #&gt; 20 Peru 1979 83.9 16924758 #&gt; 21 Peru 1980 82.4 17359118 #&gt; 22 Peru 1981 80.7 17792551 #&gt; 23 Peru 1982 78.7 18225727 #&gt; 24 Peru 1983 76.3 18660443 #&gt; 25 Peru 1984 73.7 19099575 #&gt; 26 Peru 1985 70.7 19544950 #&gt; 27 Peru 1986 67.6 19996250 #&gt; 28 Peru 1987 64.6 20451712 #&gt; 29 Peru 1988 61.7 20909897 #&gt; 30 Peru 1989 58.9 21368856 #&gt; 31 Peru 1990 56.3 21826658 #&gt; 32 Peru 1991 53.7 22283130 #&gt; 33 Peru 1992 51.0 22737056 #&gt; 34 Peru 1993 48.2 23184222 #&gt; 35 Peru 1994 45.4 23619358 #&gt; 36 Peru 1995 42.5 24038761 #&gt; 37 Peru 1996 39.7 24441076 #&gt; 38 Peru 1997 36.9 24827409 #&gt; 39 Peru 1998 34.3 25199744 #&gt; 40 Peru 1999 31.8 25561297 #&gt; 41 Peru 2000 29.6 25914875 #&gt; 42 Peru 2001 27.6 26261363 #&gt; 43 Peru 2002 25.7 26601463 #&gt; 44 Peru 2003 24.1 26937737 #&gt; 45 Peru 2004 22.6 27273188 #&gt; 46 Peru 2005 21.3 27610406 #&gt; 47 Peru 2006 20.1 27949958 #&gt; 48 Peru 2007 19.0 28292768 #&gt; 49 Peru 2008 18.0 28642048 #&gt; 50 Peru 2009 17.1 29001563 #&gt; 51 Peru 2010 16.3 29373644 #&gt; 52 Peru 2011 15.6 29759891 #&gt; 53 Peru 2012 14.9 30158768 #&gt; 54 Peru 2013 14.2 30565461 #&gt; 55 Peru 2014 13.6 30973148 #&gt; 56 Peru 2015 13.1 31376670 #&gt; 57 Peru 2016 NA NA Let’s add a filter to obtain only the data from 2015: gapminder |&gt; filter(country == &quot;Peru&quot; &amp; year == 2015) |&gt; select(country, year, infant_mortality, population) #&gt; country year infant_mortality population #&gt; 1 Peru 2015 13.1 31376670 We can make a comparison between Peru and Chile if we create a vector and instead of the == operator we use the %in% operator that allows evaluating that our data are in that vector. vector_countries = c(&quot;Peru&quot;, &quot;Chile&quot;) gapminder |&gt; filter(country %in% vector_countries &amp; year == 2015) |&gt; select(country, year, infant_mortality, population) #&gt; country year infant_mortality population #&gt; 1 Chile 2015 7.0 17948141 #&gt; 2 Peru 2015 13.1 31376670 Infant mortality is measured in number of children who die per 1,000 infants. This means that it already takes into account the population. In 2015 Peru had a higher infant mortality rate than Chile. 6.1 Initial gapminder plots However, if we want to analyze global data, comparing countries one by one would be impractical. Let’s use ggplot to see if there is a relationship in our data. Let’s create a scatter plot with data from the year 1990 of the fertility variable (fertility), which is the average number of children per woman, and the life expectancy variable (life_expectancy). gapminder |&gt; filter(year == 1990) |&gt; ggplot() + aes(x = fertility, y = life_expectancy) + geom_point() From this graph we can see that countries where families have 7.5 children have a lower life expectancy. On the other hand, in countries with high life expectancy the average number of children is less than 2 children per family. As we have done previously, we can color the points according to some other variable. In this case, knowing which continent they belong to could give us a better idea of the data. gapminder |&gt; filter(year == 1990) |&gt; ggplot() + aes(x = fertility, y = life_expectancy, color = continent) + geom_point() In this graph, groups begin to be seen. Several European countries are in the upper left quadrant, while several African countries are in the lower right quadrant. 6.2 Facets Although the previous graph already shows us a correlation of variables, we cannot see how it has changed from one year to another. For this we will use the facet layer (facet_). In the layer facet_grid(row_variable ~ column_variable) we replace “row_variable” with the name of our variable or replace it with a . if we don’t want any of them. For example, from the previous example let’s compare how the distribution changed by comparing the year 1960 with the year 2013. vector_years &lt;- c(1960, 2013) gapminder |&gt; filter(year %in% vector_years) |&gt; ggplot() + aes(x = fertility, y = life_expectancy, color = continent) + geom_point() + facet_grid(year ~ .) We can make it even clearer which continent changed the most if we add the continent variable as a column. vector_years &lt;- c(1960, 2013) gapminder |&gt; filter(year %in% vector_years) |&gt; ggplot() + aes(x = fertility, y = life_expectancy, color = continent) + geom_point() + facet_grid(year ~ continent) Having several columns for each continent makes it harder to understand because the columns become smaller. It is recommended to have few columns. So we invert the order between year and continent. vector_years &lt;- c(1960, 2013) gapminder |&gt; filter(year %in% vector_years) |&gt; ggplot() + aes(x = fertility, y = life_expectancy, color = continent) + geom_point() + facet_grid(continent ~ year) Here the change by regions is much more evident: the majority of countries have reduced fertility per family while increasing life expectancy. We are living longer than in the 1960s and having fewer children per family. These trends have occurred across all continents. We don’t always have to show all variables, in this case continents. We can continue applying filters so that it shows us a subset of continents that we want to compare. For example: vector_years &lt;- c(1960, 2013) vector_continents &lt;- c(&quot;Europe&quot;, &quot;Asia&quot;) gapminder |&gt; filter(year %in% vector_years &amp; continent %in% vector_continents) |&gt; ggplot() + aes(x = fertility, y = life_expectancy, color = continent) + geom_point() + facet_grid(continent ~ year) In this case it would be visually better if the continents were not in separate rows, but could still be appreciated in the graph. To do this, we will use the wrap facet (facet_wrap(~ x)), where x is the variable we want to wrap. In our case it would be the year, instead of appearing in separate rows we can join and transpose them. vector_years &lt;- c(1960, 2013) vector_continents &lt;- c(&quot;Europe&quot;, &quot;Asia&quot;) gapminder |&gt; filter(year %in% vector_years &amp; continent %in% vector_continents) |&gt; ggplot() + aes(x = fertility, y = life_expectancy, color = continent) + geom_point() + facet_wrap( ~ year) We can add more data by adding more data to the vectors. For example, let’s add a cut in the middle between 1960 and 2013. vector_years &lt;- c(1960, 1985, 2013) vector_continents &lt;- c(&quot;Europe&quot;, &quot;Asia&quot;) gapminder |&gt; filter(year %in% vector_years &amp; continent %in% vector_continents) |&gt; ggplot() + aes(x = fertility, y = life_expectancy, color = continent) + geom_point() + facet_wrap( ~ year) 6.3 Time series Time series are sequences of data measured at determined moments and ordered chronologically. R allows us to easily plot time series, we only need our data frames to include some time variable. 6.3.1 Individual time series In an individual time series we only analyze how a single variable has evolved, for example the evolution of the fertility rate in Peru. For this we can use a scatter plot with points or with lines. As we will remember, we use geom_point() for points: gapminder |&gt; filter(country == &quot;Peru&quot;) |&gt; ggplot() + aes(x = year, y = fertility) + geom_point() #&gt; Warning: Removed 1 row containing missing values or values #&gt; outside the scale range (`geom_point()`). We get a “warning” indicating that there are values that cannot be drawn because they are NA and are not available. This does not prevent showing the graph. If we want a line graph, which is the most used in time series, we use geom_line(): gapminder |&gt; filter(country == &quot;Peru&quot;) |&gt; ggplot() + aes(x = year, y = fertility) + geom_line() #&gt; Warning: Removed 1 row containing missing values or values #&gt; outside the scale range (`geom_line()`). 6.3.2 Multiple time series In multiple time series we seek comparison to analyze in a time series how the data evolved. For example, this would be the time series if we compare Peru, Bolivia and Chile: countries &lt;- c(&quot;Peru&quot;, &quot;Bolivia&quot;, &quot;Chile&quot;) gapminder |&gt; filter(country %in% countries) |&gt; ggplot() + aes(x = year, y = fertility, color = country) + geom_line() #&gt; Warning: Removed 3 rows containing missing values or values #&gt; outside the scale range (`geom_line()`). We can also remove the legend and show the name of the countries as labels on the same graph. To do this we will first have to create a data frame using the function data.frame() that indicates the coordinates where we want each label to appear: countries &lt;- c(&quot;Peru&quot;, &quot;Bolivia&quot;, &quot;Chile&quot;) labels &lt;- data.frame(country_names = countries, x = c(1975, 1965, 1962), y = c(6, 7, 4)) labels #&gt; country_names x y #&gt; 1 Peru 1975 6 #&gt; 2 Bolivia 1965 7 #&gt; 3 Chile 1962 4 We will use this to indicate that we want, for example, Bolivia to be written at the intersection of the year 1972 and with a fertility rate of 6.8. To use these labels in ggplot we will edit the arguments in the geom_text layer. We will use the data attributes to indicate that we want to obtain the data from an external source, and we will include the aes layer inside geom_text to correlate the data frame we have created with the graph. We must keep in mind that the column name in both data frames must be the same, in this case country: countries &lt;- c(&quot;Peru&quot;, &quot;Bolivia&quot;, &quot;Chile&quot;) labels &lt;- data.frame(country = countries, x = c(1976, 1972, 1965), y = c(5.2, 6.8, 5.5)) gapminder |&gt; filter(country %in% countries) |&gt; ggplot() + aes(year, fertility, col = country) + geom_line() + geom_text(data = labels, aes(x, y, label = country)) + theme(legend.position = &quot;none&quot;) #&gt; Warning: Removed 3 rows containing missing values or values #&gt; outside the scale range (`geom_line()`). 6.4 Exercises For these exercises we will continue using the gapminder data frame. Generate a scatter plot comparing fertility rates and life expectancy for the Americas in the year 2000. Use color to differentiate between the regions within the continent. Solution gapminder |&gt; filter( continent == &quot;Americas&quot; &amp; year == 2000) |&gt; ggplot() + aes(fertility, life_expectancy, color = region) + geom_point() To create a vector of sequences we can use X:Y. This creates a vector that goes from number X to number Y During the Vietnam War, both the US and Vietnam suffered significant losses. Create a line chart visualizing how life expectancy changed in both countries from 1955 to 1990 to observe the war’s impact. Solution countries &lt;- c(&quot;Vietnam&quot;, &quot;United States&quot;) year_sequence &lt;- 1955:1990 gapminder |&gt; filter(country %in% countries &amp; year %in% year_sequence) |&gt; ggplot() + aes(year, life_expectancy, color = country) + geom_line() Expand the previous chart to include Cambodia, allowing us to visualize the devastating impact of the Khmer Rouge regime (1975-1979) on life expectancy alongside the Vietnam War data. Solution countries &lt;- c(&quot;Vietnam&quot;, &quot;United States&quot;, &quot;Cambodia&quot;) year_sequence &lt;- 1955:1990 gapminder |&gt; filter(country %in% countries &amp; year %in% year_sequence) |&gt; ggplot() + aes(year, life_expectancy, color = country) + geom_line() 6.5 Histograms with ggplot We could continue exploring the data until we understand it much better. Eventually we would get to the GDP (gdp) data and in turn we would understand that comparing only GDP alone makes no sense since there are countries with much more population than others. Data transformation is not something new, but we will see that it is something recurrent in our analyzes. We are going to use a transformation that allows us to obtain how much is the GDP per capita per day in each country in each year gapminder &lt;- gapminder |&gt; mutate(gdp_per_capita_per_day = gdp/population/365) We could visualize this variable first by creating a histogram of it. A histogram in ggplot is nothing more than one of the geoms we have available, in this case it would be geom_histogram(binwidth = x), where x is the width of the bar. For example, let’s calculate the distribution of our created variable in the year 2010: gapminder |&gt; mutate(gdp_per_capita_per_day = gdp/population/365) |&gt; filter(year == 2010) |&gt; ggplot() + aes(gdp_per_capita_per_day) + geom_histogram(binwidth = 5) #&gt; Warning: Removed 9 rows containing non-finite outside the #&gt; scale range (`stat_bin()`). We can filter out the NA so that we no longer get the low “warnings” with the function we saw previously is.na(). In this case since we don’t want the NA we will negate the function by placing the symbol ! at the beginning. gapminder |&gt; mutate(gdp_per_capita_per_day = gdp/population/365) |&gt; filter(year == 2010 &amp; !is.na(gdp_per_capita_per_day)) |&gt; ggplot() + aes(gdp_per_capita_per_day) + geom_histogram(binwidth = 5) At this point it should be quick to detect that there is a concentration of data from countries with low GDP per capita and we could be tempted to apply a scale transformation on the x-axis. Let’s try with logarithm in base 2: gapminder |&gt; mutate(gdp_per_capita_per_day = gdp/population/365) |&gt; filter(year == 2010 &amp; !is.na(gdp_per_capita_per_day)) |&gt; ggplot() + aes(gdp_per_capita_per_day) + geom_histogram(binwidth = 0.5) + #Change the width to 0.5 due to logarithmic scale scale_x_continuous(trans = &quot;log2&quot;) Let’s be careful interpreting this data. We cannot say that it is a symmetric distribution, even when with this scale we are tempted to do so. Remember the scale and use it appropriately. Tip: For smooth distribution curves, you can also use geom_density() instead of geom_histogram(). Density plots are particularly useful when comparing multiple groups on the same plot. 6.6 Box plots with ggplot In the same way, box plots are one more geom within the available ones, for this we will use the geom_boxplot() layer. For example, let’s create a box plot to analyze GDP per capita per day by continent: gapminder |&gt; mutate(gdp_per_capita_per_day = gdp/population/365) |&gt; filter(year == 2010 &amp; !is.na(gdp_per_capita_per_day)) |&gt; ggplot() + aes(continent, gdp_per_capita_per_day) + geom_boxplot() Now let’s zoom in. Within each continent we have regions, for example in the Americas we have South America, Central America, North America, and so on with each continent. Let’s change the continent variable to region. gapminder |&gt; mutate(gdp_per_capita_per_day = gdp/population/365) |&gt; filter(year == 2010 &amp; !is.na(gdp_per_capita_per_day)) |&gt; ggplot() + aes(region, gdp_per_capita_per_day) + geom_boxplot() As we can verify: this visualization allows us to infer very little. Before discarding a graph let’s think if we can change the configuration to improve the visualization. The first thing we can improve is the names of the regions. They are in horizontal form, but we could rotate it 45 degrees using the theme() layer. gapminder |&gt; mutate(gdp_per_capita_per_day = gdp/population/365) |&gt; filter(year == 2010 &amp; !is.na(gdp_per_capita_per_day)) |&gt; ggplot() + aes(region, gdp_per_capita_per_day) + geom_boxplot() + theme(axis.text.x = element_text(angle = 45, hjust = 1) ) The names are understood, but if we want to find the top 3 (either by median or average) we would have to look for them one by one. Let’s reorder it, but first let’s be aware of some previous considerations: The region column is a Factor type variable, it is not a character string. Even when visually we did not find a difference, factors are used to categorize data. For example, bronze, silver, platinum customers, etc. Factors are useful because internally they are replaced by numbers and numbers, at a computational level, are faster to sort. The default sorting is alphabetical, as we can appreciate if we use the levels function. levels(gapminder$region) #&gt; [1] &quot;Australia and New Zealand&quot; &quot;Caribbean&quot; #&gt; [3] &quot;Central America&quot; &quot;Central Asia&quot; #&gt; [5] &quot;Eastern Africa&quot; &quot;Eastern Asia&quot; #&gt; [7] &quot;Eastern Europe&quot; &quot;Melanesia&quot; #&gt; [9] &quot;Micronesia&quot; &quot;Middle Africa&quot; #&gt; [11] &quot;Northern Africa&quot; &quot;Northern America&quot; #&gt; [13] &quot;Northern Europe&quot; &quot;Polynesia&quot; #&gt; [15] &quot;South America&quot; &quot;South-Eastern Asia&quot; #&gt; [17] &quot;Southern Africa&quot; &quot;Southern Asia&quot; #&gt; [19] &quot;Southern Europe&quot; &quot;Western Africa&quot; #&gt; [21] &quot;Western Asia&quot; &quot;Western Europe&quot; We will use the reorder() function to change the order of the factors and since we are altering the dataframe we would have to use it inside the mutate() function. The reorder() function asks us as the first attribute the factor to reorder, then the vector that we will take into account and finally a grouping function. For example, order based on the median of each region (visually remember that it is the thick line inside each box): gapminder |&gt; mutate(gdp_per_capita_per_day = gdp/population/365) |&gt; filter(year == 2010 &amp; !is.na(gdp_per_capita_per_day)) |&gt; mutate(region = reorder(region, gdp_per_capita_per_day, FUN = median)) |&gt; ggplot() + aes(region, gdp_per_capita_per_day) + geom_boxplot() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) Note that a mutate has been placed after filtering the data. This is to guarantee that we are removing the NA. Otherwise, we risk that all values are NA and the reordering is not performed and remains default. We see at the far left some regions in Africa, and at the far right Europe and USA. Remember that we can add color according to some variable. In this case let’s add color based on the continent: gapminder |&gt; mutate(gdp_per_capita_per_day = gdp/population/365) |&gt; filter(year == 2010 &amp; !is.na(gdp_per_capita_per_day)) |&gt; mutate(region = reorder(region, gdp_per_capita_per_day, FUN = median)) |&gt; ggplot() + aes(region, gdp_per_capita_per_day, color = continent) + geom_boxplot() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) Although we can already differentiate it, in a box plot it is usually the fill (fill in English) of the box that is painted. So, let’s change the color attribute to the fill attribute. And let’s remove the legend on the x-axis. It is not necessary in this case where the regions are self-explanatory. gapminder |&gt; mutate(gdp_per_capita_per_day = gdp/population/365) |&gt; filter(year == 2010 &amp; !is.na(gdp_per_capita_per_day)) |&gt; mutate(region = reorder(region, gdp_per_capita_per_day, FUN = median)) |&gt; ggplot() + aes(region, gdp_per_capita_per_day, fill = continent) + geom_boxplot() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + xlab(&quot;&quot;) This graph helps us see the top 5, but since there are several regions concentrated in small values of GDP per capita we visually lose those regions. We need a scale transformation. If you are thinking of adding a logarithmic scale layer for the y-axis you are on the right track. Let’s try: gapminder |&gt; mutate(gdp_per_capita_per_day = gdp/population/365) |&gt; filter(year == 2010 &amp; !is.na(gdp_per_capita_per_day)) |&gt; mutate(region = reorder(region, gdp_per_capita_per_day, FUN = median)) |&gt; ggplot() + aes(region, gdp_per_capita_per_day, fill = continent) + geom_boxplot() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + xlab(&quot;&quot;) + scale_y_continuous(trans = &quot;log2&quot;) Sometimes it is necessary not only to show the boxes, but also where each of the data points is located. For this we can add the geom_point() layer that we had previously used to show the points of each data. gapminder |&gt; mutate(gdp_per_capita_per_day = gdp/population/365) |&gt; filter(year == 2010 &amp; !is.na(gdp_per_capita_per_day)) |&gt; mutate(region = reorder(region, gdp_per_capita_per_day, FUN = median)) |&gt; ggplot() + aes(region, gdp_per_capita_per_day, fill = continent) + geom_boxplot() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + xlab(&quot;&quot;) + scale_y_continuous(trans = &quot;log2&quot;) + geom_point(size = 0.5) 6.7 Comparison of distributions To be able to solve the first question of the case we would have to compare the distributions of the “Western” countries versus the developing countries. For this, since we do not have a column that indicates which are from the West, we are going to create a western_countries with the list of regions that fall into this category: western_countries &lt;- c(&quot;Western Europe&quot;, &quot;Northern Europe&quot;, &quot;Southern Europe&quot;, &quot;Northern America&quot;, &quot;Australia and New Zealand&quot;) We will also use the ifelse(test, yes, no) function to create a new column such that if the region is in the West it stores a value, and if it is not in the West it stores another value. It is recommended to read the documentation in ?ifelse. Let’s add the column for the group each country belongs to: western_countries &lt;- c(&quot;Western Europe&quot;, &quot;Northern Europe&quot;, &quot;Southern Europe&quot;, &quot;Northern America&quot;, &quot;Australia and New Zealand&quot;) gapminder |&gt; mutate(gdp_per_capita_per_day = gdp/population/365) |&gt; filter(year == 2010 &amp; !is.na(gdp_per_capita_per_day)) |&gt; mutate(group = ifelse(region %in% western_countries, &quot;Western&quot;, &quot;Developing&quot;)) |&gt; head() #&gt; country year infant_mortality life_expectancy fertility #&gt; 1 Albania 2010 14.8 77.2 1.74 #&gt; 2 Algeria 2010 23.5 76.0 2.82 #&gt; 3 Angola 2010 109.6 57.6 6.22 #&gt; 4 Antigua and Barbuda 2010 7.7 75.8 2.13 #&gt; 5 Argentina 2010 13.0 75.8 2.22 #&gt; 6 Armenia 2010 16.1 73.0 1.55 #&gt; population gdp continent region gdp_per_capita_per_day #&gt; 1 2901883 6137563946 Europe Southern Europe 5.794597 #&gt; 2 36036159 79164339611 Africa Northern Africa 6.018638 #&gt; 3 21219954 26125663270 Africa Middle Africa 3.373106 #&gt; 4 87233 836686777 Americas Caribbean 26.277814 #&gt; 5 41222875 434405530244 Americas South America 28.871158 #&gt; 6 2963496 4102285513 Asia Western Asia 3.792527 #&gt; group #&gt; 1 Western #&gt; 2 Developing #&gt; 3 Developing #&gt; 4 Developing #&gt; 5 Developing #&gt; 6 Developing Now that we have how to differentiate the countries we can see their distribution until we find how to answer our question. We start by creating a histogram with logarithmic scale in the x-axis and separate it using facet_grid based on the group it belongs to: western_countries &lt;- c(&quot;Western Europe&quot;, &quot;Northern Europe&quot;, &quot;Southern Europe&quot;, &quot;Northern America&quot;, &quot;Australia and New Zealand&quot;) gapminder |&gt; mutate(gdp_per_capita_per_day = gdp/population/365) |&gt; filter(year == 2010 &amp; !is.na(gdp_per_capita_per_day)) |&gt; mutate(group = ifelse(region %in% western_countries, &quot;Western&quot;, &quot;Developing&quot;)) |&gt; ggplot() + aes(gdp_per_capita_per_day) + geom_histogram(binwidth = 1) + scale_x_continuous(trans = &quot;log2&quot;) + facet_grid(. ~ group) We see that the daily GDP per capita has a distribution with higher values compared to developing countries. However, the picture in one year is not everything. We are ready to see if the separation was the same 40 years back from the date in the example (2010). We are also going to add the geom_histogram() layer the color attribute to see the border of the bars which by default are grey. western_countries &lt;- c(&quot;Western Europe&quot;, &quot;Northern Europe&quot;, &quot;Southern Europe&quot;, &quot;Northern America&quot;, &quot;Australia and New Zealand&quot;) gapminder |&gt; mutate(gdp_per_capita_per_day = gdp/population/365) |&gt; filter(year %in% c(1970, 2010) &amp; !is.na(gdp_per_capita_per_day)) |&gt; mutate(group = ifelse(region %in% western_countries, &quot;Western&quot;, &quot;Developing&quot;)) |&gt; ggplot() + aes(gdp_per_capita_per_day) + geom_histogram(binwidth = 1, color = &quot;black&quot;) + scale_x_continuous(trans = &quot;log2&quot;) + facet_grid(year ~ group) Both groups, both “Western” and “Developing” have improved in that 40-year span, but developing countries have advanced more than Western countries. So far we have assumed something: that all countries that reported in 2010 also reported data in 1970. To make the comparison finer we have to look for the distribution of countries that have data reported both in 1970 and in 2010. To do this, we are going to create a vector that lists the countries with data in 1970 and another of those that have data in 2010 and then look for the intersection. Remember that to extract a column we use the pull() function. western_countries &lt;- c(&quot;Western Europe&quot;, &quot;Northern Europe&quot;, &quot;Southern Europe&quot;, &quot;Northern America&quot;, &quot;Australia and New Zealand&quot;) list_1 &lt;- gapminder |&gt; mutate(gdp_per_capita_per_day = gdp/population/365) |&gt; filter(year %in% c(1970) &amp; !is.na(gdp_per_capita_per_day)) |&gt; pull(country) list_2 &lt;- gapminder |&gt; mutate(gdp_per_capita_per_day = gdp/population/365) |&gt; filter(year %in% c(2010) &amp; !is.na(gdp_per_capita_per_day)) |&gt; pull(country) To find the intersection of these two vectors we will use the function intersect(vector_1, vector_2), which will give us the vector we are looking for. intersection_vector &lt;- intersect(list_1, list_2) So, we recreate our histogram including only the countries on this list. gapminder |&gt; mutate(gdp_per_capita_per_day = gdp/population/365) |&gt; filter(year %in% c(1970, 2010) &amp; !is.na(gdp_per_capita_per_day)) |&gt; filter(country %in% intersection_vector) |&gt; mutate(group = ifelse(region %in% western_countries, &quot;Western&quot;, &quot;Developing&quot;)) |&gt; ggplot() + aes(gdp_per_capita_per_day) + geom_histogram(binwidth = 1, color = &quot;black&quot;) + scale_x_continuous(trans = &quot;log2&quot;) + facet_grid(year ~ group) We see now more clearly with comparable data how there are more countries within the developing region that increased per capita GDP, and by a larger margin than Western countries. But this first inference is still visual, we need to compare how the median, range, etc. changed. For this we will use a box plot very similar to the previous one, but this time we will edit geom_boxplot() so that it shows us in a single graph how each region has changed from 1970 to 2010. gapminder |&gt; mutate(gdp_per_capita_per_day = gdp/population/365) |&gt; filter(year %in% c(1970, 2010) &amp; !is.na(gdp_per_capita_per_day)) |&gt; filter(country %in% intersection_vector) |&gt; mutate(region = reorder(region, gdp_per_capita_per_day, FUN = median)) |&gt; ggplot() + aes(region, gdp_per_capita_per_day) + geom_boxplot(aes(region, gdp_per_capita_per_day, fill=factor(year))) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + xlab(&quot;&quot;) + scale_y_continuous(trans = &quot;log2&quot;) We see how there are regions within Asia that have grown substantially. As we know from general culture, some countries in Asia are already powers, but today with these graphs we can understand well how much each region has changed until becoming a power. Therefore, we can now answer both questions of the case: It is not reasonable to continue using the categorization of “Western” and “developing” since there are more and more regions that are poorly represented by those categories, such as East Asia. It is not true that rich countries get richer while poor countries get poorer. We have seen that developing countries have even higher growth than the growth that Western countries have. 6.8 Exercises For this series of exercises we will use the stars data frame from the dslabs library. This dataset contains attributes of stars including their temperature, spectral type, and magnitude. The magnitude column represents absolute magnitude—a measure of intrinsic brightness where more negative values indicate greater luminosity. library(dslabs) data(stars) head(stars) The temperature data is currently in Kelvin. Create a new column temp_celsius using the formula \\(C = K - 273.15\\), then visualize the relationship between temperature and magnitude. Color the points by star type and use a base-10 logarithmic scale for the x-axis to better display the wide range of temperatures. Solution stars |&gt; mutate(temp_celsius = temp - 273.15) |&gt; ggplot() + aes(temp_celsius, magnitude, color = type) + scale_x_log10() + geom_point() Since lower magnitude values correspond to higher brightness, reverse the y-axis scale using scale_y_reverse() to make the plot more intuitive (brighter stars at the top). Solution stars |&gt; mutate(temp_celsius = temp - 273.15) |&gt; ggplot() + aes(temp_celsius, magnitude, color = type) + scale_x_log10() + geom_point() + scale_y_reverse() The Sun is a G-type star. To determine if these are the most luminous, create a box plot comparing the magnitude distributions across different star types. No, G-type stars are not the most luminous. For this we can elaborate this graph: Solution stars |&gt; ggplot() + aes(type, magnitude) + geom_boxplot() + scale_y_reverse() 6.9 Key Takeaways In this chapter, we followed a complete data exploration process, starting with understanding the data structure before diving into visualization. We utilized faceting to reveal trends across categories and time, and used time series plots to track specific country trajectories. We also explored distributions using histograms and box plots, applying scale transformations to handle skewed economic data. Finally, we practiced iterative refinement, adjusting our plots step-by-step to tell a clearer story. https://www.gapminder.org/↩︎ Full text of Huntington’s article (in English)↩︎ "],["introduction-to-probabilities.html", "Introduction to Probabilities", " Introduction to Probabilities Understanding probability theory is indispensable for every Data Scientist. In the following chapters we are going to learn how to apply probability theory in R. It is expected that the reader has basic knowledge in statistics given that we will not go into depth in each of the statistical concepts. For example, we are not going to explain formulas for calculating probabilities of independent or dependent events. Instead, we are going to apply directly probabilities of independent or dependent events in R. "],["discrete-probabilities.html", "Chapter 7 Discrete Probabilities 7.1 Calculation using the mathematical definition 7.2 Monte Carlo Simulation for Discrete Variables 7.3 Exercises 7.4 Combinations and Permutations 7.5 Sufficient Experiments with Monte Carlo Simulation 7.6 Case: Birthdays in Classrooms 7.7 Exercises 7.8 Integrative Exercise", " Chapter 7 Discrete Probabilities We will start with some basic principles of categorical data. Probabilities of this type are called discrete probabilities. Understanding the basic principles of discrete probabilities will help us understand continuous probabilities which are the most common in data science applications. In this chapter, we will master the foundations of discrete probability. We will start by calculating probabilities using mathematical definitions and then learn to estimate them using Monte Carlo simulations. We will leverage R functions like sample(), replicate(), and mean() to model random events, and use the gtools package to solve problems involving permutations and combinations. Finally, we will determine how to choose an adequate sample size to ensure our simulations yield reliable results. Recall that a discrete variable is a variable that cannot take some values within a minimum countable set, that is, it does not accept any value, only those that belong to the set. For example, if we have 4 women and 6 men seated in a room and we were to raffle 1 prize, intuitively we would know that the probability that the winner is a man is 60%. 7.1 Calculation using the mathematical definition The probability we obtained by intuition in the previous example can be expressed as follows: \\(P(A) = probability\\ of\\ event\\ A = \\frac{Times\\ event\\ A\\ can\\ occur}{Total\\ possible\\ outcomes}\\) \\(P(Winner\\ is\\ man) = \\frac{6}{10} = 60\\%\\) 7.2 Monte Carlo Simulation for Discrete Variables Monte Carlo simulation or method is a statistical method used to solve complex mathematical problems through the generation of random variables. In this case the problem is not complex, but Monte Carlo can be used to familiarize ourselves with a method that we will use constantly. We will use Monte Carlo simulation to estimate the proportion we would obtain if we repeated this experiment randomly a determined number of times. That is, the probability of the event using this estimation would be the proportion of times that event occurred in our simulation. In R we can easily create random samples using the sample() function. For example, let’s create a vector of students and then use the sample() function to choose one at random. students &lt;- c(&quot;woman&quot;, &quot;woman&quot;, &quot;woman&quot;, &quot;woman&quot;, &quot;man&quot;, &quot;man&quot;, &quot;man&quot;, &quot;man&quot;, &quot;man&quot;, &quot;man&quot;) sample(students, 1) We could also use the rep() function to create the students vector faster. To do this we would enter as the first argument a vector and as the second another vector indicating how many times we want them to be created. Thus, we would create the students vector faster. students &lt;- rep(c(&quot;woman&quot;, &quot;man&quot;), times = c(4, 6)) students #&gt; [1] &quot;woman&quot; &quot;woman&quot; &quot;woman&quot; &quot;woman&quot; &quot;man&quot; &quot;man&quot; &quot;man&quot; &quot;man&quot; &quot;man&quot; #&gt; [10] &quot;man&quot; Now we have to simulate a determined number of times the experiment of picking a random element. For this we will use the replicate() function. Let’s replicate this experiment 100 times: students &lt;- rep(c(&quot;woman&quot;, &quot;man&quot;), times = c(4, 6)) n_times &lt;- 100 results &lt;- replicate(n_times, { sample(students, 1) }) results #&gt; [1] &quot;man&quot; &quot;man&quot; &quot;woman&quot; &quot;man&quot; &quot;man&quot; &quot;woman&quot; &quot;man&quot; &quot;man&quot; &quot;man&quot; #&gt; [10] &quot;man&quot; &quot;woman&quot; &quot;man&quot; &quot;man&quot; &quot;woman&quot; &quot;woman&quot; &quot;man&quot; &quot;man&quot; &quot;man&quot; #&gt; [19] &quot;woman&quot; &quot;man&quot; &quot;man&quot; &quot;woman&quot; &quot;woman&quot; &quot;man&quot; &quot;man&quot; &quot;man&quot; &quot;woman&quot; #&gt; [28] &quot;man&quot; &quot;man&quot; &quot;man&quot; &quot;woman&quot; &quot;man&quot; &quot;woman&quot; &quot;man&quot; &quot;man&quot; &quot;woman&quot; #&gt; [37] &quot;man&quot; &quot;man&quot; &quot;man&quot; &quot;man&quot; &quot;woman&quot; &quot;man&quot; &quot;man&quot; &quot;man&quot; &quot;man&quot; #&gt; [46] &quot;woman&quot; &quot;man&quot; &quot;woman&quot; &quot;man&quot; &quot;man&quot; &quot;man&quot; &quot;woman&quot; &quot;man&quot; &quot;man&quot; #&gt; [55] &quot;man&quot; &quot;man&quot; &quot;woman&quot; &quot;man&quot; &quot;man&quot; &quot;man&quot; &quot;woman&quot; &quot;woman&quot; &quot;man&quot; #&gt; [64] &quot;man&quot; &quot;woman&quot; &quot;man&quot; &quot;woman&quot; &quot;man&quot; &quot;man&quot; &quot;man&quot; &quot;woman&quot; &quot;man&quot; #&gt; [73] &quot;woman&quot; &quot;man&quot; &quot;woman&quot; &quot;woman&quot; &quot;woman&quot; &quot;man&quot; &quot;man&quot; &quot;woman&quot; &quot;woman&quot; #&gt; [82] &quot;man&quot; &quot;man&quot; &quot;man&quot; &quot;woman&quot; &quot;woman&quot; &quot;man&quot; &quot;woman&quot; &quot;man&quot; &quot;woman&quot; #&gt; [91] &quot;man&quot; &quot;man&quot; &quot;man&quot; &quot;man&quot; &quot;woman&quot; &quot;man&quot; &quot;man&quot; &quot;man&quot; &quot;woman&quot; #&gt; [100] &quot;woman&quot; We can see what the result was for each of the 100 draws we simulated. Now we will use the table() function to transform our results vector into a summary table that shows us how many times each value appeared. table(results) #&gt; results #&gt; man woman #&gt; 65 35 If we store this result in a vector results_table, we can then use the prop.table() function to know the proportion of each value: results_table &lt;- table(results) prop.table(results_table) #&gt; results #&gt; man woman #&gt; 0.65 0.35 We should not worry if the probability that it is a man has not come out exactly 60%. Recall that we are estimating the probability using a method that depends on the number of times we simulate the experiment. The more times we repeat the experiment the closer we will be to the value. For example, let’s replicate this experiment now 10,000 times. students &lt;- rep(c(&quot;woman&quot;, &quot;man&quot;), times = c(4, 6)) n_times &lt;- 10000 results &lt;- replicate(n_times, { sample(students, 1) }) results_table &lt;- table(results) prop.table(results_table) #&gt; results #&gt; man woman #&gt; 0.595 0.405 We see how the value converges to 60%. We should not worry if the value varies by a few digits from the one presented in this book given that we are simulating a random event. Finally, for this simple example we could also have used the mean() function. Although this calculates the average of a set of numbers, we could convert our students vector to numerical values, where each value is converted to 1 or 0 depending on some condition. To do this, R makes the conversion of vectors to 1 and 0 very simple using the comparator operator ==: students == &quot;man&quot; #&gt; [1] FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE When we apply the mean() function to this result, it coerces TRUE values to 1 and FALSE values to 0. Thus, if we apply the average of this list, we would have the percentage of men and with it the probability that when choosing a person it is a man: mean(students == &quot;man&quot;) #&gt; [1] 0.6 7.2.1 Other functions to create vectors We have already learned the rep() function to create vectors faster. Another function we find in R is the expand.grid(x, y) function which creates a data frame of all combinations between vectors x and y. greetings &lt;- c(&quot;Hello&quot;, &quot;Goodbye&quot;) names_list &lt;- c(&quot;Andrew&quot;, &quot;Joseph&quot;, &quot;John&quot;, &quot;Emily&quot;, &quot;Cesar&quot;, &quot;Jeremy&quot;) result &lt;- expand.grid(greeting = greetings, name = names_list) result #&gt; greeting name #&gt; 1 Hello Andrew #&gt; 2 Goodbye Andrew #&gt; 3 Hello Joseph #&gt; 4 Goodbye Joseph #&gt; 5 Hello John #&gt; 6 Goodbye John #&gt; 7 Hello Emily #&gt; 8 Goodbye Emily #&gt; 9 Hello Cesar #&gt; 10 Goodbye Cesar #&gt; 11 Hello Jeremy #&gt; 12 Goodbye Jeremy Finally, we have the paste(x,y) function, which concatenates two strings or vectors of strings adding a space in the middle. paste(result$greeting, result$name) #&gt; [1] &quot;Hello Andrew&quot; &quot;Goodbye Andrew&quot; &quot;Hello Joseph&quot; &quot;Goodbye Joseph&quot; #&gt; [5] &quot;Hello John&quot; &quot;Goodbye John&quot; &quot;Hello Emily&quot; &quot;Goodbye Emily&quot; #&gt; [9] &quot;Hello Cesar&quot; &quot;Goodbye Cesar&quot; &quot;Hello Jeremy&quot; &quot;Goodbye Jeremy&quot; Thus, we can easily generate, for example, a deck of cards distributed in 4 suits: hearts, diamonds, spades, and clubs. The cards of each suit are numbered from 1 to 10, where 1 is the Ace, and followed by Jack, Queen, and King. To do this, we would have to create a vector of suits and a vector of numbers to then create the combinationl and have the complete deck. numbers &lt;- c(&quot;Ace&quot;, &quot;Two&quot;, &quot;Three&quot;, &quot;Four&quot;, &quot;Five&quot;, &quot;Six&quot;, &quot;Seven&quot;, &quot;Eight&quot;, &quot;Nine&quot;, &quot;Ten&quot;, &quot;Jack&quot;, &quot;Queen&quot;, &quot;King&quot;) suits &lt;- c(&quot;of Hearts&quot;, &quot;of Diamonds&quot;, &quot;of Spades&quot;, &quot;of Clubs&quot;) # Create card combinations combination &lt;- expand.grid(number = numbers, suit = suits) # Concatenate vectors to have our final combination paste(combination$number, combination$suit) #&gt; [1] &quot;Ace of Hearts&quot; &quot;Two of Hearts&quot; &quot;Three of Hearts&quot; #&gt; [4] &quot;Four of Hearts&quot; &quot;Five of Hearts&quot; &quot;Six of Hearts&quot; #&gt; [7] &quot;Seven of Hearts&quot; &quot;Eight of Hearts&quot; &quot;Nine of Hearts&quot; #&gt; [10] &quot;Ten of Hearts&quot; &quot;Jack of Hearts&quot; &quot;Queen of Hearts&quot; #&gt; [13] &quot;King of Hearts&quot; &quot;Ace of Diamonds&quot; &quot;Two of Diamonds&quot; #&gt; [16] &quot;Three of Diamonds&quot; &quot;Four of Diamonds&quot; &quot;Five of Diamonds&quot; #&gt; [19] &quot;Six of Diamonds&quot; &quot;Seven of Diamonds&quot; &quot;Eight of Diamonds&quot; #&gt; [22] &quot;Nine of Diamonds&quot; &quot;Ten of Diamonds&quot; &quot;Jack of Diamonds&quot; #&gt; [25] &quot;Queen of Diamonds&quot; &quot;King of Diamonds&quot; &quot;Ace of Spades&quot; #&gt; [28] &quot;Two of Spades&quot; &quot;Three of Spades&quot; &quot;Four of Spades&quot; #&gt; [31] &quot;Five of Spades&quot; &quot;Six of Spades&quot; &quot;Seven of Spades&quot; #&gt; [34] &quot;Eight of Spades&quot; &quot;Nine of Spades&quot; &quot;Ten of Spades&quot; #&gt; [37] &quot;Jack of Spades&quot; &quot;Queen of Spades&quot; &quot;King of Spades&quot; #&gt; [40] &quot;Ace of Clubs&quot; &quot;Two of Clubs&quot; &quot;Three of Clubs&quot; #&gt; [43] &quot;Four of Clubs&quot; &quot;Five of Clubs&quot; &quot;Six of Clubs&quot; #&gt; [46] &quot;Seven of Clubs&quot; &quot;Eight of Clubs&quot; &quot;Nine of Clubs&quot; #&gt; [49] &quot;Ten of Clubs&quot; &quot;Jack of Clubs&quot; &quot;Queen of Clubs&quot; #&gt; [52] &quot;King of Clubs&quot; Once our deck is created we can inquire some probabilities easily with the created vector. Let’s calculate the probability that when choosing a card it is “King of Diamonds”: # Store the combinationl in the variable deck deck &lt;- paste(combination$number, combination$suit) mean(deck == &quot;King of Diamonds&quot;) #&gt; [1] 0.01923077 Or we can also calculate the probability that when choosing a card it is some Queen: # First create the vector of &quot;Queen of...&quot; queens &lt;- paste(&quot;Queen&quot;, suits) # Probability calculation mean(deck %in% queens) #&gt; [1] 0.07692308 This chapter highlighted two complementary approaches to probability. The mathematical definition (\\(P(A) = \\frac{\\text{favorable outcomes}}{\\text{total outcomes}}\\)) gives us exact answers for simple problems. However, for more complex scenarios, Monte Carlo simulation allows us to estimate probabilities by replicating experiments thousands of times using sample() and replicate(). We learned that mean(condition) is an efficient way to calculate proportions in R, and that increasing the number of repetitions brings our simulation estimates closer to the true theoretical probability. 7.3 Exercises Calculate the probability of not rolling a 1 on a single die roll and store it in a variable named prob. Then, use this variable to determine the probability that the number 1 does not appear in three consecutive rolls. Solution prob &lt;- 5 / 6 prob * prob * prob Imagine a container holding 5 blue, 3 yellow, and 4 gray marbles. Determine the probability that a marble chosen at random is blue. Solution marbles &lt;- rep(c(&quot;blue&quot;, &quot;yellow&quot;, &quot;gray&quot;), times = c(5, 3, 4)) # Solution using monte carlo simulation, repeating the event 10,000 times: results &lt;- replicate(10000, { sample(marbles, 1) }) prop.table(table(results)) # Solution using the `mean()` function: mean(marbles == &quot;blue&quot;) Mathematically it would be: Given the event: \\(X = chosen\\ marble\\ is\\ blue\\): \\(P(X)=\\frac{5}{5+3+4}=\\frac{5}{12}=41.67\\%\\) The probability that the marble is blue is 41.67%. Using the same container of marbles, calculate the probability that a randomly chosen marble is not blue. Solution marbles &lt;- rep(c(&quot;blue&quot;, &quot;yellow&quot;, &quot;gray&quot;), times = c(5, 3, 4)) mean(marbles != &quot;blue&quot;) The probability is 58.33%. Mathematically it would be: Given the event \\(X = chosen\\ marble\\ is\\ blue\\): \\(P(\\sim~X)=1-P(X)=1-\\frac{5}{12}=1-41.67\\%=58.33\\%\\) Consider drawing two marbles from the container in sequence without replacing the first one. Calculate the probability that the first marble is blue and the second is not blue. Create numeric variables for each color count to perform this calculation. Solution # Create variables blue &lt;- 5 yellow &lt;- 3 gray &lt;- 4 # Calculate probability that the first marble is blue: p_1 &lt;- blue / (blue + yellow + gray) # First calculate the probability that the second is blue: p_aux &lt;- (blue - 1) / (blue - 1 + yellow + gray) # Calculate the complement because they ask that the second is NOT blue: p_2 &lt;- 1 - p_aux # Calculate what is asked: p_1 * p_2 This is called sampling without replacement. We have two events, we are taking out two marbles. The second event depends on the first. These two events are not independent of each other. Repeat the previous experiment, but this time replace the first marble before drawing the second one. Update your R code to calculate the probability that the first marble is blue and the second is not blue under these conditions. Solution # Create variables blue &lt;- 5 yellow &lt;- 3 gray &lt;- 4 # Calculate probability that the first marble is blue: p_1 &lt;- blue / (blue + yellow + gray) # First calculate the probability that the second is blue: p_aux &lt;- blue / (blue + yellow + gray) # Calculate the complement because they ask that the second is NOT blue: p_2 &lt;- 1 - p_aux # Calculate what is asked: p_1 * p_2 This is called sampling with replacement. We have two events, we are taking out two marbles again. The second event does not depend on the first. These two events are independent. 7.4 Combinations and Permutations Some probability situations involve multiple events. When one of the events affects others, they are called dependent events. For example, when objects are chosen from a list or group and are not returned, the first choice reduces the options for future choices. There are two ways to order or combine results of dependent events. Permutations are groupings in which the order of objects matters. Combinations are groupings in which the content matters but the order does not. For this we will use the gtools package, which includes libraries like gtools that provide us with intuitive functionalities to work with permutations and combinations. # First install the gtools package install.packages(&quot;gtools&quot;) # To start using it, load the gtools library library(gtools) 7.4.1 Permutations The order matters when we calculate, for example, the winners of a competition. Suppose we have 10 students who are competing on equal terms for who builds the most accurate machine learning model. data_scientists &lt;- c(&quot;Jenny&quot;, &quot;Freddy&quot;, &quot;Yasan&quot;, &quot;Iver&quot;, &quot;Pamela&quot;, &quot;Alexandra&quot;, &quot;Bladimir&quot;, &quot;Enrique&quot;, &quot;Karen&quot;, &quot;Christiam&quot;) Only the top 3 will receive the prize. In this case the order matters, so we will use the function permutations(total, selection, data) where total indicates the size of the vector, selection indicates the size of the result I want, and finally data is my source vector. permutations(10, 3, v = data_scientists) We have already calculated all possible results. We can calculate on this result the probability that Freddy wins the competition and that Pamela is in second place. results &lt;- permutations(10, 3, v = data_scientists) # Total results: nrow(results) total &lt;- nrow(results) # Probability that Freddy wins: mean(results[, 1] == &quot;Freddy&quot; &amp; results[, 2] == &quot;Pamela&quot;) #&gt; [1] 0.01111111 7.4.2 Combinations The order does not matter when, for example, we form groups of 2 to participate in the competition. combinations(10, 2, v = data_scientists) If now only one team is going to win the prize, we could calculate the probability that the team made up of Pamela and Enrique are the ones who win. results &lt;- combinations(10, 2, v = data_scientists) # Total results: nrow(results) #&gt; [1] 45 # Probability: mean((results[, 1] == &quot;Pamela&quot; &amp; results[, 2] == &quot;Enrique&quot;) | (results[, 1] == &quot;Enrique&quot; &amp; results[, 2] == &quot;Pamela&quot;)) #&gt; [1] 0.02222222 Although we can obtain the probability by calculating all combinations, in R it will be very frequent to use Monte Carlo to estimate the probability by simulation. For the previous case we would not have to generate all combinations, but simply take a sample of two people who would be the members of the winning team. Recall that we have assumed that everyone has equal chances of winning. sample(data_scientists, 2) Then, we would have to replicate this experiment over and over again, store the sampling results and calculate the proportion of how many times the winning team was composed of Pamela and Enrique. n &lt;- 10000 result &lt;- replicate(n, { team &lt;- sample(data_scientists, 2) meets_condition &lt;- (team[1] == &quot;Pamela&quot; &amp; team[2] == &quot;Enrique&quot;) | (team[2] == &quot;Pamela&quot; &amp; team[1] == &quot;Enrique&quot;) meets_condition }) mean(result) #&gt; [1] 0.0232 Note that, as we saw previously, the value converges as we increase the number of times we repeat the experiment n. We have simulated repeating the experiment 10 thousand times. However, how many times would it be necessary to replicate the experiment to trust the results of the simulation? 7.5 Sufficient Experiments with Monte Carlo Simulation Intuitively we can indicate that the greater the number of experiments the more precise the estimated probability. We can, thus, do several simulations with different number of experiments for each simulation. In this way we could find a reasonable number of experiments for our simulation. To do this, first we create a numerical vector where the number of times we are going to simulate the experiment is indicated. Our vector will contain the following values: 10, 20, 40, 80, 160,…, etc. This means that the first time we will simulate the experiment 10 times, the second 20 times and so on. n_times &lt;- 10*2^(1:17) n_times #&gt; [1] 20 40 80 160 320 640 1280 2560 5120 #&gt; [10] 10240 20480 40960 81920 163840 327680 655360 1310720 Then, we use the code we created to replicate the experiment to create a function called probability_by_sample: probability_by_sample &lt;- function(n) { result &lt;- replicate(n, { team &lt;- sample(data_scientists, 2) meets_condition &lt;- (team[1] == &quot;Pamela&quot; &amp; team[2] == &quot;Enrique&quot;) | (team[2] == &quot;Pamela&quot; &amp; team[1] == &quot;Enrique&quot;) meets_condition }) mean(result) } We already have a function that allows us to replicate the experiment as many times as we want. For example, in the previous section we simulated 10 thousand experiments. Now that we have created the function we would do: # Probability using functions: probability_by_sample(10000) #&gt; [1] 0.0245 Again, this is a simulation. So every time we execute that function the probability will vary as it is a random sample. To apply a function on each of the values of a vector we use the function sapply(vector, function) where vector is the vector where the data on which I want to apply the function is and function is the function I want to apply. prob &lt;- sapply(n_times, probability_by_sample) prob #&gt; [1] 0.10000000 0.00000000 0.02500000 0.01250000 0.02812500 0.02968750 #&gt; [7] 0.02031250 0.02265625 0.02089844 0.02119141 0.02397461 0.02197266 #&gt; [13] 0.02225342 0.02208862 0.02212219 0.02229462 0.02235031 This gives us the probabilities depending on the number of times we repeat the experiment. Now let’s place these results in a scatter plot to see how it converges probabilities &lt;- data.frame( n = n_times, probability = prob ) probabilities |&gt; ggplot() + aes(n, probability) + geom_line() + geom_point() + xlab(&quot;# of times of the experiment&quot;) We can also change the scale to zoom in on the probabilities for smaller experiment number values and add a reference line with the theoretical probability value calculated previously: probabilities |&gt; ggplot() + aes(n, probability) + geom_line() + geom_point() + xlab(&quot;# of times of the experiment&quot;) + scale_x_continuous(trans = &quot;log2&quot;) + geom_hline(yintercept = 0.022222, color = &quot;blue&quot;, lty = 2) We observe that, for this experiment, repeating the experiment 10 thousand times (x-axis = 3 because it is \\(10^3\\)) already gives us a good approximation to the real value. 7.6 Case: Birthdays in Classrooms Let’s review the concepts learned with another example. In a Data Science for Managers class there are 50 students. Using Monte Carlo simulation, let’s estimate what is the probability that there are at least two people who have birthdays on the same day. (Let’s ignore those who have birthdays on February 29). First let’s list all the days of the year available for birthdays: days &lt;- 1:365 Let’s generate a random sample of 50 numbers from the days vector, but this time with replacement because a person could have the same day, and store it in the colleagues variable. colleagues &lt;- sample(days, 50, replace = TRUE) To validate if any of the values are repeated we will use the duplicated() function which validates if there are duplicate values within the vector: duplicated(colleagues) #&gt; [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #&gt; [13] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #&gt; [25] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE #&gt; [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE #&gt; [49] FALSE FALSE Finally, to determine if there was any TRUE value we use the any() function: any(duplicated(colleagues)) #&gt; [1] TRUE The result tells us whether it is true or not that there are at least two people who have birthdays on the same day. To estimate by Monte Carlo simulation what the probability is, we have to repeat the experiment many times and take the proportion of how many times we get TRUE as a result. # Monte Carlo simulation with 10 thousand repetitions n_times &lt;- 10000 results &lt;- replicate(n_times, { colleagues &lt;- sample(days, 50, replace = TRUE) # Returns a logical value of whether there are duplicates any(duplicated(colleagues)) }) # Probability: mean(results) #&gt; [1] 0.97 We see that the estimated probability is very high, above 95%. What would happen if I have a room of 25 people? To do this, we modify the previous code and create the variable classroom which will indicate the number of students in that class: # Monte Carlo simulation with 10 thousand repetitions n_times &lt;- 10000 classroom &lt;- 25 results &lt;- replicate(n_times, { # Returns a logical vector colleagues &lt;- sample(days, classroom, replace = TRUE) any(duplicated(colleagues)) }) # Probability: mean(results) #&gt; [1] 0.5618 Let’s now create the function estimate_probability and estimate using this function the probability of finding at least two people with the same birthday in a room of 25 people. This time we have to specify that the sampling is with “replacement” because by default the sample() function is “without replacement”. # Create the function estimate_probability &lt;- function(classroom, n_times = 10000){ results &lt;- replicate(n_times, { # Returns a logical vector colleagues &lt;- sample(days, classroom, replace = TRUE) any(duplicated(colleagues)) }) # Probability: mean(results) } estimate_probability(25) #&gt; [1] 0.5707 Finally, if we already have a function that calculates based on the number of people in a room we can create a numerical vector with the total number of people from different rooms and then apply the function we have created. The result can be stored in the variable prob. # Create 80 different classrooms # The first room with 1 person, the last room with 80 people classrooms &lt;- 1:80 # Estimate probability depending on the number of students per room prob &lt;- sapply(classrooms, estimate_probability) prob #&gt; [1] 0.0000 0.0028 0.0097 0.0174 0.0265 0.0395 0.0572 0.0723 0.0940 0.1219 #&gt; [11] 0.1404 0.1639 0.2004 0.2340 0.2489 0.2879 0.3152 0.3423 0.3849 0.4074 #&gt; [21] 0.4501 0.4776 0.5059 0.5451 0.5712 0.5969 0.6226 0.6469 0.6775 0.7033 #&gt; [31] 0.7284 0.7519 0.7798 0.7958 0.8204 0.8349 0.8429 0.8672 0.8844 0.8942 #&gt; [41] 0.9006 0.9191 0.9228 0.9313 0.9413 0.9469 0.9545 0.9629 0.9643 0.9706 #&gt; [51] 0.9705 0.9763 0.9805 0.9846 0.9865 0.9908 0.9890 0.9918 0.9946 0.9947 #&gt; [61] 0.9947 0.9961 0.9967 0.9981 0.9972 0.9977 0.9979 0.9985 0.9986 0.9989 #&gt; [71] 0.9997 0.9996 0.9994 0.9998 0.9998 0.9999 0.9996 0.9996 1.0000 1.0000 Thus, if we place it in a scatter plot we can see how the probability increases as there are more students: probabilities &lt;- data.frame( n = classrooms, probability = prob ) probabilities |&gt; ggplot() + aes(n, probability) + geom_point() + xlab(&quot;Number of students in each class&quot;) We can now impress our friends from different groups by telling them that, if they are in a room of 60 people, “you can bet them” that there are two people in that room who have birthdays on the same day. It is not definitive, but the chances are very much in our favor. 7.7 Exercises Alonso and Georgina play chess, and Georgina has a 60% chance of winning any given game. Without using a simulation, calculate the probability that Alonso wins at least one game out of four played. Solution Calculating the probability that Alonso has won at least once is the complement of the probability that Georgina has won all 4 times. Thus, we will first calculate the probability that Georgina has always won and then calculate the complement. # Probability that Georgina wins all 4 games prob &lt;- 0.6^4 # Probability that Alonso wins at least once 1 - prob Now, estimate the probability of Alonso winning at least one of four games using a Monte Carlo simulation. Assume Alonso has a 40% chance of winning each individual match. game_results &lt;- sample(c(&quot;loses&quot;,&quot;wins&quot;), 4, replace = TRUE, prob = c(0.6, 0.4)) Solution # Times I run the simulation n_times &lt;- 10000 # Generate result of experiments that Alonso wins alonso_wins &lt;- replicate(n_times, { game_results &lt;- sample(c(&quot;loses&quot;,&quot;wins&quot;), 4, replace = TRUE, prob = c(0.6, 0.4)) any(game_results == &quot;wins&quot;) }) # Estimate probability mean(alonso_wins) Generalize the previous simulation by creating a function probability_of_winning that accepts Alonso’s win probability p as an argument. Use this function to simulate outcomes for a sequence of win probabilities from 0.4 to 0.95 (steps of 0.025) and visualize the results in a scatter plot. Solution # Create function probability_of_winning &lt;- function(p){ n_times &lt;- 10000 alonso_wins &lt;- replicate(n_times, { game_results &lt;- sample(c(&quot;loses&quot;,&quot;wins&quot;), 4, replace = TRUE, prob = c(1-p, p)) any(game_results == &quot;wins&quot;) }) mean(alonso_wins) } # Create vector with different probabilities p &lt;- seq(0.4, 0.95, 0.025) prob &lt;- sapply(p, probability_of_winning) plot(p, prob, xlab=&quot;p: probability that Alonso wins in each game&quot;, ylab=&quot;prob: prob. that Alonso wins at least one game&quot;) 7.8 Integrative Exercise Let’s solve together this exercise that integrates everything we have learned in this chapter, called the Monty Hall problem. 7.8.1 Monty Hall Problem Monty Hall was a TV presenter who made famous a contest in his show which we are going to replicate below. We have three doors in front of us: Behind one of these doors is a zero-kilometer car, while in the other two there is a goat. We, as contest participants, have to choose together which door to open. Whatever is behind it will be ours. Suppose we have chosen door number 2. Once you announce our choice, Monty Hall tells us that he is going to help us and opens a door for us right now. He opens one of the other doors and it turns out there is a goat in door 3 that he opened. Monty Hall asks us: I am going to give you a chance to change doors and that will be your final choice, Would you change doors or stick with the door chosen at the beginning? Intuitively we knew that, when all doors were closed, the car is in one of 3 doors. The probability of winning would be \\(\\frac{1}{3} = 0.3333\\) so it didn’t matter which door to choose. But when he opens door number three he gives us information and the first thing we should ask ourselves is whether the probabilities have been affected or not. Although this is an advanced math exercise using change of variable, we can execute a Monte Carlo simulation to estimate the probabilities and solve it without using almost any mathematical formula. Let’s start by simulating the experiment. At the beginning we had three doors, door 1, 2 and 3. We will create the variable doors. doors &lt;- c(&quot;door 1&quot;, &quot;door 2&quot;, &quot;door 3&quot;) Then, we know that behind the doors there is a car and two goats distributed randomly. We will use the function sample to order them randomly. prizes &lt;- sample(c(&quot;car&quot;, &quot;goat&quot;, &quot;goat&quot;)) Since Monty Hall knows where the prize is. We are going to create a variable prize_door where we will store where the car is. prize_door &lt;- doors[prizes == &quot;car&quot;] Now we choose a door randomly and store our result in the variable choice. choice &lt;- sample(doors, 1) Given that we already have the chosen door shown we will simulate Monty Hall choosing the door to open. Since he is the presenter he will choose any door that is not the door where the prize is or your door. door_opened &lt;- sample(doors[!doors %in% c(choice, prize_door)],1) Finally, we are going to put all the code together and in the last line add the comparison of whether the prize door matches our choice. This time we are going to choose not to change doors, so our choice does not vary. doors &lt;- c(&quot;door 1&quot;, &quot;door 2&quot;, &quot;door 3&quot;) prizes &lt;- sample(c(&quot;car&quot;, &quot;goat&quot;, &quot;goat&quot;)) prize_door &lt;- doors[prizes == &quot;car&quot;] choice &lt;- sample(doors, 1) door_opened &lt;- sample(doors[!doors %in% c(choice, prize_door)],1) choice == prize_door With our experiment created we are going to simulate what would happen if we stick with the choice and what would happen if we change it. 7.8.1.1 Stick with the chosen door Let’s replicate about 10 thousand times to see the proportion of times we would win if we stick with our door. n_times &lt;- 10000 results &lt;- replicate(n_times, { doors &lt;- c(&quot;door 1&quot;, &quot;door 2&quot;, &quot;door 3&quot;) prizes &lt;- sample(c(&quot;car&quot;, &quot;goat&quot;, &quot;goat&quot;)) prize_door &lt;- doors[prizes == &quot;car&quot;] choice &lt;- sample(doors, 1) door_opened &lt;- sample(doors[!doors %in% c(choice, prize_door)],1) choice == prize_door }) mean(results) #&gt; [1] 0.3348 We see that the probability obtained by Monte Carlo simulation is an estimation very close to the probability that we had intuitively calculated. That is, if we keep our choice of the door we chose we have a 33.33% probability of winning. But, what happens if we change doors? Is the probability of winning the same? 7.8.1.2 Change door We are going to use the code and modify it by creating the variable new_choice to make the door change. n_times &lt;- 10000 results &lt;- replicate(n_times, { doors &lt;- c(&quot;door 1&quot;, &quot;door 2&quot;, &quot;door 3&quot;) prizes &lt;- sample(c(&quot;car&quot;, &quot;goat&quot;, &quot;goat&quot;)) prize_door &lt;- doors[prizes == &quot;car&quot;] choice &lt;- sample(doors, 1) door_opened &lt;- sample(doors[!doors %in% c(choice, prize_door)],1) new_choice &lt;- doors[!doors %in% c(choice, door_opened)] new_choice == prize_door }) mean(results) #&gt; [1] 0.6599 As we see, changing the door in this show gave us a probability of 66.66% of winning, while keeping our choice only 33.33%. It may sound counterintuitive, but statistically speaking it is better to change doors instead of trusting our luck and keeping the initial choice. "],["continuous-probabilities.html", "Chapter 8 Continuous Probabilities 8.1 Learning Objectives 8.2 Empirical Distribution 8.3 Theoretical Distribution 8.4 Key Takeaways 8.5 Exercises 8.6 Monte Carlo Simulation for Continuous Variables 8.7 Exercises", " Chapter 8 Continuous Probabilities Recall that a continuous variable is a variable that takes values along a continuum, that is, over an entire interval of values. An essential attribute of a continuous variable is that, unlike a discrete variable, it can never be measured exactly; the observed value depends largely on the precision of the measuring instruments. With a continuous variable, there is inevitably a measurement error. As an example, the height of a person (1.72m, 1.719m, 1.7186m….). Another example could be the time it takes an athlete to run 100 meters flat, since this time can take values such as 9.623 seconds; 10.456485 seconds; 12.456412 seconds; that is, an interval of values. 8.1 Learning Objectives After completing this chapter, you will be able to: In this chapter, we will learn how to handle data that can take any value within a range. We will distinguish between empirical distributions, based on real data, and theoretical distributions, which model ideal behavior. We will use the Cumulative Distribution Function (CDF) to calculate probabilities for intervals and apply the normal distribution to model real-world continuous data. Additionally, we will learn to assess whether data fits a normal distribution using Q-Q plots and the Shapiro-Wilk test, and perform Monte Carlo simulations for continuous variables using rnorm(). For example, recall that in the heights data frame we have the heights of a group of university students. heights |&gt; filter(sex == &quot;Male&quot;) |&gt; # Filter only males mutate(height_m = height/39.37) |&gt; # Convert to meters ggplot() + aes(sex, height_m, color = height_m) + geom_point(position = position_jitterdodge()) When graphing the data distribution, we intuitively realize that it does not make sense to calculate the proportion of people who measure exactly 1.73m because it would also serve us if a person measures 1.731, 1.729, or any close value that is not exactly 1.73, whether due to how it was measured or any other type of error. It makes more sense to analyze the data by intervals, as can be seen in this histogram that groups by intervals of 0.05 meters = 5 cm. heights |&gt; filter(sex == &quot;Male&quot;) |&gt; # Filter only males mutate(height_m = height/39.37) |&gt; # Convert to meters ggplot() + aes(height_m) + geom_histogram(binwidth = 0.05, color = &quot;black&quot;) It is much more practical to define a function that operates on intervals instead of unique values. For this we use the Cumulative Distribution Function (CDF). 8.2 Empirical Distribution When we use data to analyze its distribution, we speak of an empirical distribution. It is the actual distribution of a subject or an option, and measures the real and individual possibilities, regarding the measurement of the subject’s direct score, or of an option of which the frequency of occurrence has been measured. For example, for our case we can create the vector men (men) made up of all the values of the heights of men: men &lt;- heights |&gt; filter(sex == &quot;Male&quot;) |&gt; mutate(height_m = height/39.37) |&gt; pull(height_m) Then, we can create the function empirical_cdf (CDF) that takes x as a variable and calculates the proportion of men who measure less than or equal to x within the data found in the men vector. empirical_cdf &lt;- function(x){ mean(men &lt;= x) } Thus, if we want to calculate what the proportion of students who measure 1.73m or less would be. empirical_cdf(1.73) #&gt; [1] 0.3768473 On the other hand, recall that the median is the value that divides our data into two equal parts. So, if we calculate the median: median(men) #&gt; [1] 1.752604 And then we enter the value 1.7526035 into our function to ask what is the proportion of students who measure 1.7526035 or less, we should get a value very close to 50% by definition of the median. median_val &lt;- median(men) empirical_cdf(median_val) #&gt; [1] 0.5073892 So far we have calculated proportions with the cumulative distribution function. However, if we want to know what is the probability that when choosing a man at random they measure 1.9m or less, we could use the same empirical_cdf. Since each student has the same chance of being chosen, the answer to the question would be the proportion of students who measure 1.9 or less. \\(F(1.9) = P(x \\le 1.9)\\) empirical_cdf(1.9) #&gt; [1] 0.9396552 We observe that the probability is approximately 93.97%. If we now want to calculate the probability that someone chosen at random is taller than 1.80m, we first calculate the CDF for 1.8 and then obtain the complement. \\(P(x &gt; 1.8) = 1 - P(x \\le 1.8)\\) # Probability of measuring 1.80m or less prob &lt;- empirical_cdf(1.8) # Probability of measuring more than 1.80m 1 - prob #&gt; [1] 0.3583744 The probability is approximately 35.8%. If we now wanted to know the probability that when choosing someone at random they measure more than 1.6m, but not more than 1.95m, we would have. \\(P(x &gt; 1.6\\ \\cap\\ x \\le 1.95) = P(x \\le 1.95) - P(x \\le 1.6)\\) prob_1 &lt;- empirical_cdf(1.95) prob_2 &lt;- empirical_cdf(1.6) prob_1 - prob_2 #&gt; [1] 0.9445813 8.3 Theoretical Distribution On the other hand, a theoretical distribution is a distribution that is derived from certain principles or assumptions by logical and mathematical reasoning, as opposed to one derived from real-world data obtained by empirical research. Among them we have the normal distribution, the binomial distribution, and the Poisson distribution. For example, if we draw an approximate line of our data on men’s heights we would have this graph: heights |&gt; filter(sex == &quot;Male&quot;) |&gt; mutate(height_m = height/39.37) |&gt; ggplot() + aes(height_m) + geom_histogram(binwidth = 0.05, color = &quot;black&quot;) + geom_density(aes(y = after_stat(count)*0.05), colour = &quot;blue&quot;, lty = 5) We see that the distribution has an approximately symmetric, bell shape. This distribution could be modeled using a normal distribution (also called Gaussian distribution, Gauss curve, or Gauss bell). To do this, in R we will use the function pnorm(x, average, std_dev) to estimate the probability but using a normal distribution function with an average average and a standard deviation std_dev. In this way, we can estimate what is the probability that if we choose a value at random it is less than or equal to x. For example, let’s calculate again the probability that when choosing a man at random he measures 1.65m or less, we could use the same FDA and now the pnorm() function. # Using the empirical distribution (real data): empirical_cdf(1.9) #&gt; [1] 0.9396552 # Using the theoretical normal distribution (approx. data): avg &lt;- mean(men) std_dev &lt;- sd(men) probability &lt;- pnorm(1.9, avg, std_dev) probability #&gt; [1] 0.9357267 We obtain approximately the same results. Using a normal distribution facilitates our work when our data has a normal behavior. Mathematically we are calculating the area under the curve which is seen in blue: sec &lt;- seq(-4, 4, length = 100) * std_dev + avg normal &lt;- dnorm(sec, avg, std_dev) data.frame(value = normal) |&gt; ggplot() + aes(sec, value) + geom_line() + theme(axis.text.y = element_blank()) + xlab(&quot;Height (meters)&quot;) + ylab(&quot;&quot;) + ggtitle(&quot;Normal Distribution&quot;) + geom_area(aes(x = ifelse(sec &lt; 1.9, sec, 0)), fill = &quot;blue&quot;) + xlim(min(sec), max(sec)) + labs(subtitle = paste(&quot;P(x &lt;= 1.9) =&quot;, probability)) In the same way, we could estimate the probability that a person chosen at random measures more than 1.8m. # Using the empirical distribution (real data): 1 - empirical_cdf(1.8) #&gt; [1] 0.3583744 # Using the theoretical normal distribution (approx. data): avg &lt;- mean(men) std_dev &lt;- sd(men) probability &lt;- 1- pnorm(1.8, avg, std_dev) probability #&gt; [1] 0.3337484 Mathematically we are calculating the area under the curve which is seen in blue: sec &lt;- seq(-4, 4, length = 100) * std_dev + avg normal &lt;- dnorm(sec, avg, std_dev) data.frame(value = normal) |&gt; ggplot() + aes(sec, value) + geom_line() + theme(axis.text.y = element_blank()) + xlab(&quot;Height (meters)&quot;) + ylab(&quot;&quot;) + ggtitle(&quot;Normal Distribution&quot;) + geom_area(aes(x = ifelse(sec &gt; 1.8, sec, 0)), fill = &quot;blue&quot;) + xlim(min(sec), max(sec)) + labs(subtitle = paste(&quot;P(x &gt; 1.8) =&quot;, probability)) Finally, let’s recalculate the probability that when choosing someone at random they measure more than 1.6m, but not more than 1.95m, we would have. # Using the empirical distribution (real data): prob_1 &lt;- empirical_cdf(1.95) prob_2 &lt;- empirical_cdf(1.6) prob_1 - prob_2 #&gt; [1] 0.9445813 # Using the theoretical normal distribution (approx. data): avg &lt;- mean(men) std_dev &lt;- sd(men) probability &lt;- pnorm(1.95, avg, std_dev) - pnorm(1.6, avg, std_dev) probability #&gt; [1] 0.9405618 Mathematically we are calculating the area under the curve which is seen in blue: sec &lt;- seq(-4, 4, length = 100) * std_dev + avg normal &lt;- dnorm(sec, avg, std_dev) data.frame(value = normal) |&gt; ggplot() + aes(sec, value) + geom_line() + theme(axis.text.y = element_blank()) + xlab(&quot;Height (meters)&quot;) + ylab(&quot;&quot;) + ggtitle(&quot;Normal Distribution&quot;) + geom_area(aes(x = ifelse(sec &gt; 1.6 &amp; sec &lt;= 1.95, sec, 0)), fill = &quot;blue&quot;) + xlim(min(sec), max(sec)) + labs(subtitle = paste(&quot;P(1.6 &lt; x &lt;= 1.95) =&quot;, probability)) We can plot a Q-Q plot, which is a scatter plot created by plotting two sets of quantiles against each other. The function stat_qq(x) creates a normal Q-Q plot. This function plots the data in sorted order against the quantiles of a standard Normal distribution. The function stat_qq_line() adds a reference line. Although understanding this requires advanced statistics, we can interpret it that if when using this function the correlation is very close to the line then our data is very likely to follow a normal distribution. heights |&gt; filter(sex == &quot;Male&quot;) |&gt; mutate(height_m = height/39.37) |&gt; ggplot() + aes(sample = height_m) + stat_qq() + stat_qq_line() The points seem to fall on a straight line. This gives us a good indication that assuming our height data comes from a population that is normally distributed is reasonable. Observe that the y-axis plots the empirical quantiles and x-axis plots the theoretical quantiles. The latter are the quantiles of the standard Normal distribution with mean 0 and standard deviation 1. Visual inspection is not always reliable. It is possible to use a significance test that compares the sample distribution with a normal one to determine whether or not the data shows a serious deviation from normality. The most used test for these tests is the Shapiro-Wilk normality test. For this we will use the function shapiro.test(), which performs a normality test and gives us a p-value^(https://www.investopedia.com/terms/p/p-value.asp). It is based on the correlation between the data and the corresponding normal scores. If the p-value &gt; 0.05 then the data distribution is not significantly different from the normal distribution. In other words, we can assume normality. shapiro.test(men) #&gt; #&gt; Shapiro-Wilk normality test #&gt; #&gt; data: men #&gt; W = 0.96374, p-value = 2.623e-13 The p-value is less than 0.05 so, although our distribution resembles a normal one, it does not pass the significance test. 8.4 Key Takeaways We have explored the differences between empirical distributions, derived directly from observed data using functions like empirical_cdf(), and theoretical distributions, which rely on mathematical models such as the normal distribution. We learned to use pnorm(x, mean, sd) to calculate cumulative probabilities, providing a powerful tool for analyzing continuous variables. Finally, we introduced Q-Q plots and the Shapiro-Wilk test as method to verify if our data follows a normal distribution, with a p-value greater than 0.05 indicating that the assumption of normality is reasonable. 8.5 Exercises For the following exercises assume that the distribution of ages of students in the Data Science with R course approximates a normal distribution with an average of 24 years and a standard deviation of 3. If we select a student at random: Given the parameters of our student age distribution, calculate the probability that a randomly chosen student is at most 23 years old. Solution avg_e &lt;- 24 std_dev_e &lt;- 3 pnorm(23, avg_e, std_dev_e) Determine the probability that a selected student is older than 28 years. Solution avg_e &lt;- 24 std_dev_e &lt;- 3 1 - pnorm(28, avg_e, std_dev_e ) Calculate the probability that a student is between 22 and 27 years old (older than 22 but at most 27). Solution avg_e &lt;- 24 std_dev_e &lt;- 3 pnorm(27, avg_e, std_dev_e ) - pnorm(22, avg_e, std_dev_e ) Find the probability that a student’s age falls within one standard deviation of the average (i.e., between average - sd and average + sd). Solution avg_e &lt;- 24 std_dev_e &lt;- 3 max &lt;- avg_e + std_dev_e min &lt;- avg_e - std_dev_e pnorm(max, avg_e, std_dev_e ) - pnorm(min, avg_e, std_dev_e ) 8.6 Monte Carlo Simulation for Continuous Variables Although we have used a normal function to calculate the approximate probability, we can create more than one normal function with that average and that standard deviation. We will use the function rnorm(n, average, std_dev) to create a vector of n random data, such that they are normally distributed with an average avg and a standard deviation std_dev. Recall that our original data has the following characteristics: avg &lt;- mean(men) avg #&gt; [1] 1.760598 std_dev &lt;- sd(men) std_dev #&gt; [1] 0.09172018 length_val &lt;- length(men) length_val #&gt; [1] 812 If we want to generate a random normal distribution we will use rnorm(): # Creation of the normally distributed random vector: random_normal &lt;- rnorm(length_val, avg, std_dev) # We create a histogram to visualize it better hist(random_normal) result &lt;- paste(&quot;Sample:&quot;, length_val, &quot;. Average:&quot;, round(mean(random_normal), 3), &quot;. Std. dev.:&quot;, round(sd(random_normal), 3) ) mtext(result,3) We can execute the code again to verify that it generates another distribution for us: # Creation of the normally distributed random vector: random_normal &lt;- rnorm(length_val, avg, std_dev) # We create a histogram to visualize it better hist(random_normal) result &lt;- paste(&quot;Sample:&quot;, length_val, &quot;. Average:&quot;, round(mean(random_normal), 3), &quot;. Std. dev.:&quot;, round(sd(random_normal), 3) ) mtext(result,3) We can repeat this experiment of obtaining n random data that have approximately the same average and the same std_dev about 10 thousand times to calculate the proportion of times that a man measures more than 1.8m. n_times &lt;- 10000 simulation_results &lt;- replicate(n_times, { random_normal &lt;- rnorm(length_val, avg, std_dev) random_normal &gt; 1.8 }) mean(simulation_results) #&gt; [1] 0.3338273 Thus, we have obtained practically the same value that we achieved in the previous section, but this time estimating using the Monte Carlo simulation. 8.7 Exercises The distribution of the admission exam grades of the Univ. UNISM is distributed approximately normally. The average is 14.5 and the standard deviation is 1. We want to know the distribution of the first place. It is known that 5 thousand people apply once a year per exam and take a single exam. Generate 5 thousand grades about 1,000 times using Monte Carlo simulation and perform a histogram of the result. Solution n_times &lt;- 1000 avg &lt;- 14.5 std_dev &lt;- 1 max_grades &lt;- replicate(n_times, { exam_simulation &lt;- rnorm(5000, avg, std_dev) max(exam_simulation) }) hist(max_grades) shapiro.test(max_grades) Now, modify your simulation to analyze the distribution of the yearly averages. Generate the sample average for each of the 1,000 simulations and plot the histogram. Solution n_times &lt;- 1000 avg &lt;- 14.5 std_dev &lt;- 1 notas_avg &lt;- replicate(n_times, { exam_simulation &lt;- rnorm(5000, avg, std_dev) mean(exam_simulation) }) hist(notas_avg) shapiro.test(notas_avg) Using the results from your first simulation (distribution of maximum grades), estimate the probability that the highest grade in a given year exceeds 18.5. Solution n_times &lt;- 1000 avg &lt;- 14.5 std_dev &lt;- 1 max_grades &lt;- replicate(n_times, { exam_simulation &lt;- rnorm(5000, avg, std_dev) max(exam_simulation) }) mean(max_grades &gt; 18.5) "],["statistical-inference.html", "Chapter 9 Statistical Inference 9.1 Learning Objectives 9.2 Expected Value 9.3 Central Limit Theorem 9.4 Key Takeaways 9.5 Exercises 9.6 Parameter Estimation Method 9.7 Spread Estimation 9.8 Estimates Outside Election Polls 9.9 Exercises", " Chapter 9 Statistical Inference To infer means to draw a conclusion from general or particular facts. Statistical inference is a set of methods and techniques that allow deducing characteristics of a population using data from a random sample. The method we are going to use most to infer is the parameter estimation method. We estimate parameters of a population from a sample because very rarely we will be able to have access to all the data of the population. Such is the case of election polls, disease studies, etc. 9.1 Learning Objectives After completing this chapter, you will be able to: In this chapter, we will build the foundation for making valid conclusions from data. We will start by mastering the calculation of expected values and standard errors, which are critical for characterizing random variables. Then, we will explore the Central Limit Theorem to understand how sampling distributions behave. Finally, we will apply these concepts to estimate population parameters from sample data, calculating confidence intervals and margins of error to analyze real-world scenarios like election polling. We will introduce fundamental concepts such as expected value and standard error, which will be useful to us to make inferences. 9.2 Expected Value Let’s use the following case to understand this concept intuitively. We have been hired in a casino to analyze if it is reasonable to install a roulette with 37 values ranging from 0 to 36. The house wants to open the game with a special offer if the ball lands on 0 or 21 paying 10 to 1. This means that if a player plays and wins we pay them 10 dollars and if they lose they would pay us 1 dollar. With what we have learned so far we can simulate our game with the case data. We have 37 values, of which in 2 of them give a player a profit of +10 or a loss -1. Let’s also define prob_win as the probability that a player wins. # Total times played plays &lt;- 1 # Probability that a player wins each time prob_win &lt;- 2/37 prob_lose &lt;- 1 - prob_win # Random sample sample_vec &lt;- sample(c(10, -1), plays, replace = TRUE, prob = c(prob_win, prob_lose)) sample_vec #&gt; [1] -1 The distribution of this variable is simple given that it can only take two values: 10 or -1. When we simulate a very large number of games it can be seen how it is distributed according to the indicated probability of winning and losing. plays &lt;- 100000 prob_win &lt;- 2/37 prob_lose &lt;- 1 - prob_win tibble(outcome = sample_vec) |&gt; ggplot(aes(x = factor(outcome))) + geom_bar(fill = &quot;steelblue&quot;) + labs(x = &quot;Outcome ($)&quot;, y = &quot;Count&quot;, title = &quot;Distribution of Roulette Outcomes&quot;) We have been using Monte Carlo simulation to estimate what the mean of the game results would be in real life. # Estimation of the mean by Monte Carlo simulation mean(sample_vec) #&gt; [1] -1 In addition, we have seen that, the more the sample grows, our mean in the Monte Carlo simulation converges to a value, in this case the probability of winning mainly in the roulette. That value to which it converges we will call expected value, which as its name indicates will be the value we expect to obtain in reality. The more the sample size grows the more our sample mean converges to this expected value. The notation we will use will be \\(E[X]\\). When there are only two possible results \\(a\\) and \\(b\\) with proportions \\(p\\) and \\(1-p\\) respectively, the expected value will be calculated using this formula: \\(E[X] = ap + b(1-p)\\) # Expected Value: (10) * prob_win + (-1) * prob_lose #&gt; [1] -0.4054054 Previously we had calculated the mean using Monte Carlo simulation. If we compare it with the expected value we see how both numbers are approximately the same, as the theory predicts. Returning to the simulation of roulette games, a single person does not play so many times. Each person plays about 40 times a day at roulette. Thus, we can generate 40 games that a random player could play and find how much he would win: plays &lt;- 40 prob_win &lt;- 2/37 prob_lose &lt;- 1 - prob_win sample_vec &lt;- sample(c(10, -1), plays, replace = TRUE, prob = c(prob_win, prob_lose)) sum(sample_vec) #&gt; [1] -7 Finally, not only one person will play. Let’s replicate this sample about 100,000 times to simulate the number of players we would have in a quarter. players &lt;- 100000 plays &lt;- 40 prob_win &lt;- 2/37 prob_lose &lt;- 1 - prob_win set.seed(2025) winnings_simulation &lt;- replicate(players, { sample_vec &lt;- sample(c(10, -1), plays, replace = TRUE, prob = c(prob_win, prob_lose)) sum(sample_vec) }) So far we have done the same as we have learned in previous chapters. However, we could also see how the players’ winnings are distributed. And for that it is enough to create a histogram of the result. tibble(winnings = winnings_simulation) |&gt; ggplot(aes(x = winnings)) + geom_histogram(bins = 30, fill = &quot;steelblue&quot;, color = &quot;white&quot;) + labs(x = &quot;Total Winnings ($)&quot;, y = &quot;Count&quot;, title = &quot;Distribution of Player Winnings&quot;) It is not a coincidence that if we create a histogram with all the winnings of all the players the result looks like a normal distribution. In fact, that was the main approach that George Pólya made in 1920 when he presented his Central Limit Theorem. 9.3 Central Limit Theorem The Central Limit Theorem tells us that if we take several samples of the same size \\(n\\) and in each sample we sum the values within each sample we will obtain a value \\(S\\) (the sum) then we will find that its distribution approximates well to a normal curve. If we replicate this language to our example it would be: The central limit theorem tells us that if we take samples of 40 games for each player and then calculate for each player the total they won, then we will find that the distribution of the amount won by many players approximates a normal distribution. Since it is a new distribution, we can calculate its mean and standard deviation. Being samples we will use the learned term expected value of the sum to refer to the sample mean and we will add the term of standard error of the sum to refer to the sample standard deviation This would be the formula to calculate the expected value of the sum: \\(E[S_n] = n (ap+b(1-p))\\) plays &lt;- 40 prob_win &lt;- 2/37 prob_lose &lt;- 1 - prob_win # Expected value of the sum E_sum &lt;- plays * ( (10)*prob_win + (-1)*prob_lose ) E_sum #&gt; [1] -16.21622 And to calculate the standard error of the sum we will use the following formula: \\(SE[S_n]=\\sqrt{n}\\ |a-b|\\ \\sqrt{p(1-p)}\\) plays &lt;- 40 prob_win &lt;- 2/37 prob_lose &lt;- 1 - prob_win # Standard error of the sum SE_sum &lt;- sqrt(plays) * abs(10 - -1) * sqrt(prob_win*prob_lose) SE_sum #&gt; [1] 15.73149 With these two theoretical data, the expected value and the standard error, we can graph the normal curve of the sum of winnings of our game. What does this mean? That if theoretically we can already graph the normal curve then we can also calculate the probability that the sum is greater or less than some value. This is the main advantage of the Central Limit Theorem since we can calculate probabilities of the population using this approximation and the data of a single sample. For example, if we want to know what is the probability that a player wins money after playing 40 times in roulette we would have to calculate the probability that \\(S\\) is greater than zero, represented by the blue shaded area: To perform this calculation in R we would use the pnorm function: # Probability of getting more than 0 dollars having played 40 games: 1- pnorm(0, E_sum, SE_sum) #&gt; [1] 0.1513144 Let’s validate that the Monte Carlo simulation approximates this theoretical value we just calculated: # Probability of getting more than 0 dollars having played 40 games: mean(winnings_simulation &gt; 0) #&gt; [1] 0.16813 We have used two ways to estimate the probability, the theoretical estimation using the central limit theorem and the Monte Carlo simulation. These two numbers are quite close to the real probability. In both cases, the larger the sample, the more reasonable our estimation will be. On the other hand, the same happens if we wanted to analyze the average and not the sum of the winnings. But for the average case we will use the following formulas: Expected value of the average: \\(E[\\overline{X}]=ap+b(1-p)\\). Standard error of the average: \\(SE[\\overline{X}]=|a-b|\\sqrt{\\frac{p(1-p) }{n}}\\). 9.4 Key Takeaways This chapter provided the tools to quantify uncertainty. We learned that the Expected Value, \\(E[X]\\), represents the long-term average outcome of a random variable. The Central Limit Theorem is the bridge that allows us to approximate the sum of independent samples using a normal distribution, regardless of the original population’s shape. This theorem enables us to calculate the Expected Value of the Sum (\\(E[S_n] = n \\cdot E[X]\\)) and the Standard Error of the Sum (\\(SE[S_n] = \\sqrt{n} \\cdot SE[X]\\)), empowering us to estimate probabilities and make inferences about a population without needing to measure every single individual. 9.5 Exercises The admission exam of the National Univ. of San Marcos consists of 100 multiple choice questions (A, B, C, D, E) with a value of 20 points for each correct question and 1.125 for each wrong answer. We want to analyze what would happen if a student answers all 100 questions randomly and if there are chances of getting a vacancy knowing that minimum 900 points are needed to enter some career. Consider a multiple-choice exam with 100 questions, where each correct answer awards 20 points and each wrong answer deducts 1.125 points. Determine the expected value of points a student would receive for a single question if they guessed randomly among the 5 options. Solution points_correct &lt;- 20 points_wrong &lt;- -1.125 prob_correct &lt;- 1/5 prob_wrong &lt;- 1 - prob_correct # Expected value of guessing a question: E &lt;- points_correct * prob_correct + points_wrong * prob_wrong E Based on the expected value for a single question, calculate the total expected value if a student guesses on all 100 questions of the exam. Solution # Total questions: n &lt;- 100 # Expected value of the sum E_sum &lt;- n * E E_sum Calculate the standard error associated with the total score if a student guesses on all 100 questions. Solution # Standard error of the sum SE_sum &lt;- sqrt(n)*abs(points_correct - points_wrong) * sqrt(prob_correct*prob_wrong) SE_sum Using the Central Limit Theorem and the values calculated previously, determine the probability that a student guessing on all questions would achieve a score higher than 900 points. Solution min_score &lt;- 900 # Probability of obtaining less than the minimum: prob &lt;- pnorm(min_score, E_sum, SE_sum) # Probability of obtaining more than the minimum: 1 - prob This means that the probability that a student obtains the minimum score by guessing all the questions is: 0.0000000000014525. Conclusion: let’s study before taking the exam. It is not reasonable to take the exam randomly and mark randomly. Recall that e-n is the representation of \\(10^{-n}\\). Validate your theoretical calculation by running a Monte Carlo simulation. Simulate the exam scores for 22,000 applicants guessing randomly and calculate the proportion who score above 900 points. Solution total &lt;- 22000 set.seed(2025) admission_simulation &lt;- replicate(total, { exam_score &lt;- sample(c(points_correct, points_wrong), n, replace = TRUE, prob = c(prob_correct, prob_wrong) ) sum(exam_score) }) # Probability of obtaining more than 900 points: mean(admission_simulation &gt; 900) # Histogram if we want to see the distribution of points obtained: hist(admission_simulation) We see that the simulation gives us practically the same result. Practically there are no possibilities of entering UNMSM by guessing the answers. 9.6 Parameter Estimation Method So far, using Monte Carlo simulation we have built samples randomly, but knowing the probability of occurrence. However, we will not always know the proportion previously. If we have, for example, a population and we want to know how many have been infected by Covid-19, we cannot test everyone. Or if we have the total voters for an election, we cannot survey everyone to know who would win. Not only is it very expensive, but it would take us a lot of time. The parameter estimation method is the procedure used to know the characteristics of a population parameter, from the knowledge of a sample of \\(n\\) respondents We will analyze this following case. We have two political parties: Blue and Red. We do not know how much the total population is, nor the proportion that will vote for one or the other party. The only thing we can do is conduct voting intention polls. For example, these would be the results of the poll of a random sample of 10 people: Intuitively we know that we cannot deduce which party will win given that the sample is very small. To know which party will win we need to estimate as precisely as possible the parameter \\(p\\) that represents the proportion of voters of the Blue Party in the population and the parameter \\(1-p\\) that represents the proportion of voters of the Red party. Making some mathematical transformations to our theoretical estimates seen previously and defining \\(a=1\\) as value if they vote Blue and \\(b=0\\) if they do not vote for Blue, we can obtain the following theoretical estimates for this case: By defining a vote for Blue as \\(a=1\\) and a vote for others as \\(b=0\\), we can derive theoretical estimates that link our sample data to the unknown population parameter \\(p\\). First, consider the expected value of a single vote. Mathematically, \\(E[X]=p\\). This confirms that the value we expect to obtain from a single random voter matches the proportion \\(p\\) we are trying to find. Next, we look at the expected value of the average across multiple surveys. If we were to conduct many surveys of \\(n\\) respondents each, the average of these sample means, denoted as \\(E[\\overline{X}]\\), would also equal \\(p\\). This reinforces that our sample mean is an unbiased estimator of the population proportion. Finally, we must account for variability using the standard error of the average, defined as \\(SE[\\overline{X}]=\\sqrt{\\frac{p(1-p) }{n}}\\). This metric tells us how much the results of our multiple surveys would fluctuate around the true parameter \\(p\\), taking into account the sample size \\(n\\). The expected value of the average \\(E[\\overline{X}]\\), formula 2, is theoretically equal to the parameter \\(p\\) that we are looking to estimate. However, without knowing how much \\(p\\) is we would have to have multiple samples of \\(n\\) respondents, then calculate the mean for each case \\(\\overline{X}\\) and finally calculate the average of these values. This is very expensive, so we will look for another way to estimate \\(E[\\overline{X}]\\). Given that we do not have so far how to estimate \\(E[\\overline{X}]\\), and given that we know that \\(E[\\overline{X}]=p\\) then we could give several values to \\(p\\) and see the impact on the standard error of the average that we know also depends on \\(p\\). Let’s generate, first, a sequence of parameter \\(p\\), from 0% to 100%, 100 different values: p &lt;- seq(0, 1, length=100) p #&gt; [1] 0.00000000 0.01010101 0.02020202 0.03030303 0.04040404 0.05050505 #&gt; [7] 0.06060606 0.07070707 0.08080808 0.09090909 0.10101010 0.11111111 #&gt; [13] 0.12121212 0.13131313 0.14141414 0.15151515 0.16161616 0.17171717 #&gt; [19] 0.18181818 0.19191919 0.20202020 0.21212121 0.22222222 0.23232323 #&gt; [25] 0.24242424 0.25252525 0.26262626 0.27272727 0.28282828 0.29292929 #&gt; [31] 0.30303030 0.31313131 0.32323232 0.33333333 0.34343434 0.35353535 #&gt; [37] 0.36363636 0.37373737 0.38383838 0.39393939 0.40404040 0.41414141 #&gt; [43] 0.42424242 0.43434343 0.44444444 0.45454545 0.46464646 0.47474747 #&gt; [49] 0.48484848 0.49494949 0.50505051 0.51515152 0.52525253 0.53535354 #&gt; [55] 0.54545455 0.55555556 0.56565657 0.57575758 0.58585859 0.59595960 #&gt; [61] 0.60606061 0.61616162 0.62626263 0.63636364 0.64646465 0.65656566 #&gt; [67] 0.66666667 0.67676768 0.68686869 0.69696970 0.70707071 0.71717172 #&gt; [73] 0.72727273 0.73737374 0.74747475 0.75757576 0.76767677 0.77777778 #&gt; [79] 0.78787879 0.79797980 0.80808081 0.81818182 0.82828283 0.83838384 #&gt; [85] 0.84848485 0.85858586 0.86868687 0.87878788 0.88888889 0.89898990 #&gt; [91] 0.90909091 0.91919192 0.92929293 0.93939394 0.94949495 0.95959596 #&gt; [97] 0.96969697 0.97979798 0.98989899 1.00000000 Thinking of 100 different values of \\(p\\) would be like thinking of 100 different elections where the Blue party and the red one have participation, like the election for mayors nationwide. In some districts the candidate of the Blue party loses with 0%, in others ties at 50% and in others wins clearly with 100% of the votes. Intuitively we know that if our real proportion was \\(p=80\\%\\) for the Blue party, that is that 8 out of every 10 will vote Blue, then it is very likely that in each survey we take we will find that in that district the Blue party has the majority of votes. This is predicted with the formula seen before and also includes the size of the survey \\(n\\) as part of the calculation: \\(SE[\\overline{X}]=\\sqrt{\\frac{p(1-p) }{n}}\\) That said, let’s return to our vector p that contains several values of parameter \\(p\\). On those values we can calculate what would happen if we survey groups of 20 people. Knowing the sample size we can calculate the standard error of the average for each of the values of \\(p\\): # Total people in each survey: n &lt;- 20 # Standard error of the average: SE_avg &lt;- sqrt(p*(1-p))/sqrt(n) SE_avg #&gt; [1] 0.00000000 0.02235954 0.03145942 0.03833064 0.04402928 0.04896646 #&gt; [7] 0.05335399 0.05731823 0.06094183 0.06428243 0.06738214 0.07027284 #&gt; [13] 0.07297936 0.07552152 0.07791540 0.08017428 0.08230929 0.08432982 #&gt; [19] 0.08624394 0.08805856 0.08977974 0.09141275 0.09296223 0.09443229 #&gt; [25] 0.09582660 0.09714840 0.09840064 0.09958592 0.10070661 0.10176486 #&gt; [31] 0.10276258 0.10370152 0.10458327 0.10540926 0.10618079 0.10689904 #&gt; [37] 0.10756509 0.10817988 0.10874431 0.10925913 0.10972506 0.11014270 #&gt; [43] 0.11051262 0.11083529 0.11111111 0.11134044 0.11152357 0.11166072 #&gt; [49] 0.11175205 0.11179770 0.11179770 0.11175205 0.11166072 0.11152357 #&gt; [55] 0.11134044 0.11111111 0.11083529 0.11051262 0.11014270 0.10972506 #&gt; [61] 0.10925913 0.10874431 0.10817988 0.10756509 0.10689904 0.10618079 #&gt; [67] 0.10540926 0.10458327 0.10370152 0.10276258 0.10176486 0.10070661 #&gt; [73] 0.09958592 0.09840064 0.09714840 0.09582660 0.09443229 0.09296223 #&gt; [79] 0.09141275 0.08977974 0.08805856 0.08624394 0.08432982 0.08230929 #&gt; [85] 0.08017428 0.07791540 0.07552152 0.07297936 0.07027284 0.06738214 #&gt; [91] 0.06428243 0.06094183 0.05731823 0.05335399 0.04896646 0.04402928 #&gt; [97] 0.03833064 0.03145942 0.02235954 0.00000000 Now let’s generate a scatter plot of both the different values of \\(p\\) and the standard errors for each \\(p\\). tibble(p = p, SE_avg = SE_avg) |&gt; ggplot(aes(x = p, y = SE_avg)) + geom_point(color = &quot;steelblue&quot;) + coord_cartesian(ylim = c(0, 0.12)) + labs(x = &quot;Proportion (p)&quot;, y = &quot;Standard Error&quot;, title = &quot;Standard Error vs. Proportion&quot;) Thus, we see how we can obtain different standard errors of the average for different values of \\(p\\). Intuitively we had the notion of what would happen given a \\(p=80\\%\\). Now in the graph we see it better. If the real intention of vote was 80% in that district then when taking several surveys and seeing the results of each survey we would obtain as expected value 80% and as standard error 8.8% or 0.088 as seen in the graph highlighted in blue: coord_x &lt;- 0.8 coord_y &lt;- sqrt(coord_x * (1 - coord_x)) / sqrt(n) tibble(p = p, SE_avg = SE_avg) |&gt; ggplot(aes(x = p, y = SE_avg)) + geom_point(color = &quot;steelblue&quot;) + geom_hline(yintercept = coord_y, linetype = &quot;dashed&quot;, color = &quot;blue&quot;) + geom_vline(xintercept = coord_x, linetype = &quot;dashed&quot;, color = &quot;blue&quot;) + coord_cartesian(ylim = c(0, 0.12)) + labs(x = &quot;Proportion (p)&quot;, y = &quot;Standard Error&quot;, title = &quot;SE at p = 80%&quot;) With these values of \\(E[\\overline{X}]=p=80\\%\\) and \\(SE[\\overline{X}]=8.8\\%\\) of standard error we can calculate a range of one standard error around \\(80\\%\\), which would go from \\(71.2\\%\\) to \\(88.8\\%\\) and then calculate what would be the probability that the mean \\(\\overline{X}\\) found in one of the surveys falls in this range. Visually it would be: In R, calculating the probability that a data point falls in the range of 1 standard error would be: # Calculation of probability that dat is between -1 and 1 standard error: pnorm(1) - pnorm(-1) #&gt; [1] 0.6826895 We can expand to have a greater range of 2 standard errors around \\(80\\%\\) and increase our probability: In R it would be: # Calculation of probability that dat is between -2 and 2 standard errors: pnorm(2) - pnorm(-2) #&gt; [1] 0.9544997 The probability increases to 95%, however how do we interpret this?. We haven’t even calculated the real value of the mean \\(\\overline{X}\\) of some survey. Simple, this means that, theoretically, there is a 95% probability that the mean \\(\\overline{X}\\) that we find in each survey is in the range of 62% to 98%, two standard errors around \\(80\\%\\). 95% of the time in the worst case, in a survey of 20 people, the Blue party would obtain 62% and in the best case 98%, so we could predict that the Blue party will win. Or not? Several things should make noise to us so far. First, the range so large, from 62% to 98%. Second, we have assumed a scenario: that the Blue voting intention was known and was 80%. That is, we have assumed \\(p=80\\%\\) which allowed us to calculate \\(E[\\overline{X}]=80\\%\\) and place that value at the center of the normal. However, \\(p\\) is unknown and is precisely what we are trying to estimate. If, on the contrary, the result was tighter, for example \\(p=55\\%\\), such a wide range would not serve us. Let’s see how it would be: coord_x &lt;- 0.55 coord_y &lt;- sqrt(coord_x * (1 - coord_x)) / sqrt(n) tibble(p = p, SE_avg = SE_avg) |&gt; ggplot(aes(x = p, y = SE_avg)) + geom_point(color = &quot;steelblue&quot;) + geom_hline(yintercept = coord_y, linetype = &quot;dashed&quot;, color = &quot;blue&quot;) + geom_vline(xintercept = coord_x, linetype = &quot;dashed&quot;, color = &quot;blue&quot;) + coord_cartesian(ylim = c(0, 0.12)) + labs(x = &quot;Proportion (p)&quot;, y = &quot;Standard Error&quot;, title = &quot;SE at p = 55%&quot;) If the real voting intention was \\(55\\%\\) we would have an expected value of the average \\(E[\\overline{X}]=p=55\\%\\) and a corresponding standard error of the average \\(SE[\\overline{X}]=11\\%\\). Again, by Central Limit Theorem we can calculate a range of two standard errors around \\(55\\%\\): The calculation of the probability of being in that range in R would be the same because we continue in the range of 2 standard errors. Therefore the probability would be the same. # Calculation of probability that dat is between -2 and 2 standard errors: pnorm(2) - pnorm(-2) #&gt; [1] 0.9544997 However, what does change is the range. Now the range goes from 32.8% to 77.2%, two standard errors around the expected value of the average \\(E[\\overline{X}]\\). Although the probability is still 95%, that does not help us at all this time because there is 95% that what we find in our sample is a value between 33% and 77%. Some survey samples will give us 33% of votes for Blue and other samples 77%. And the problem lies in the number of samples taken \\(n\\). If we see again the formula we see how \\(n\\) influences the result. \\(SE[\\overline{X}]=\\sqrt{\\frac{p(1-p) }{n}}\\) Let’s increment then our number of respondents to 500: n &lt;- 500 p &lt;- seq(0, 1, length = 100) SE_avg &lt;- sqrt(p*(1-p))/sqrt(n) tibble(p = p, SE_avg = SE_avg) |&gt; ggplot(aes(x = p, y = SE_avg)) + geom_point(color = &quot;steelblue&quot;) + coord_cartesian(ylim = c(0, 0.12)) + labs(x = &quot;Proportion (p)&quot;, y = &quot;Standard Error&quot;, title = &quot;SE vs Proportion (n = 500)&quot;) This sample gives us smaller standard errors. For example, if the real proportion of voters of the Blue party was \\(p=55\\%\\) we would have \\(E[\\overline{X}]=p=55\\%\\) and a \\(SE[\\overline{X}]=2.2\\%\\): coord_x &lt;- 0.55 coord_y &lt;- sqrt(coord_x * (1 - coord_x)) / sqrt(n) tibble(p = p, SE_avg = SE_avg) |&gt; ggplot(aes(x = p, y = SE_avg)) + geom_point(color = &quot;steelblue&quot;) + geom_hline(yintercept = coord_y, linetype = &quot;dashed&quot;, color = &quot;blue&quot;) + geom_vline(xintercept = coord_x, linetype = &quot;dashed&quot;, color = &quot;blue&quot;) + coord_cartesian(ylim = c(0, 0.12)) + labs(x = &quot;Proportion (p)&quot;, y = &quot;Standard Error&quot;, title = &quot;SE at p = 55% (n = 500)&quot;) If we now calculate a range of two standard errors around \\(55\\%\\) we would have a range that goes from \\(50.6\\%\\) to \\(59.4\\%\\). Again, interpretation is that the mean that we find in our random survey has a 95% probability of being in that range. We see then that this theoretical prediction, the standard error, becomes smaller as the sample size \\(n\\) increases and in turn depends on the probability of the population \\(p\\) that we do not know. Moreover, with a real value of \\(p=0.5\\), (50%), we have the maximum value of the standard error that we can obtain. Thus, if we correct \\(p\\) at 50%, which would be the extreme of cases, a tie, we can calculate how the value of the standard error of the average changes according to the sample size: p &lt;- 0.5 n &lt;- seq(20, 5000, 20) SE_avg &lt;- sqrt(p*(1-p)/n) tibble(n = n, SE_avg = SE_avg) |&gt; ggplot(aes(x = n, y = SE_avg)) + geom_line(color = &quot;steelblue&quot;) + geom_hline(yintercept = 0.015, linetype = &quot;dashed&quot;, color = &quot;blue&quot;) + geom_vline(xintercept = 1000, linetype = &quot;dashed&quot;, color = &quot;blue&quot;) + labs(x = &quot;Sample Size (n)&quot;, y = &quot;Standard Error&quot;, title = &quot;SE vs Sample Size (p = 50%)&quot;) A sample of 1,000 people, for example, generates us a maximum standard error of 0.015 or 1.5%. 9.6.1 Margin of Error As we have already seen, we could consider a range of 1 standard error or 2 standard errors around \\(E[\\overline{X}]\\) and calculate the probability that our sample mean \\(\\overline{X}\\) is in that range. Or what is mathematically the same, we could say that if we build a range of 1 or 2 standard errors around our sample mean \\(\\overline{X}\\) there is a determined probability that in that range is included the expected value \\(E[\\overline{X}]\\) which is, by formula equal to \\(p\\), the value we want to estimate. It is crucial then to calculate the standard error of the average \\(SE[\\overline{X}]\\), but we see ourselves limited because it depends on \\(p\\). There is another way to calculate \\(SE[\\overline{X}]\\) without using \\(p\\) and is known as the standard error of estimation \\(\\hat{SE}[\\overline{X}]\\). For this we will use the following formula: \\(\\hat{SE}[\\overline{X}]=\\sqrt{\\frac{\\overline{X}(1-\\overline{X})}{n}}\\) Where, as we already know, \\(\\overline{X}\\) is the mean of our sample or sample mean. For our example case, it is the percentage that the Blue party obtained in the survey we conducted. Now that we have the sample mean \\(\\overline{X}\\) and we can already calculate the standard error of estimation \\(\\hat{SE}[\\overline{X}]\\) we can start building ranges around \\(\\overline{X}\\) that increase the probability of finding \\(p\\). To make communication simpler, we will use by convention the notation margin of error to indicate that we are going to take a range of 2 standard errors of estimation. For example, we have a sample of 1100 people and after reviewing the survey results we have a sample mean of \\(\\overline{X}=56\\%\\) for the Blue party. With this we can estimate the standard error of estimation \\(\\hat{SE}[\\overline{X}]\\) with the formula we just described and finally calculate the margin of error. # Total respondents total &lt;- 1100 # Survey results, 56% indicated Blue: X_avg &lt;- 0.56 # Standard error estimation SE_est &lt;- sqrt(X_avg * (1 - X_avg)/total) SE_est #&gt; [1] 0.01496663 # Margin of error, MoE MoE &lt;- 2 * SE_est MoE #&gt; [1] 0.02993326 With this we would have that from a sample of 1100 people, we have estimated 56% voting intention for the Blue party with a margin of error of \\(+- 2.99\\%\\). Finally, let’s see examples of the different surveys conducted in April and early May 2020 to measure voting intentions in the US. We see as columns: The table columns provide key details about each survey. The Poll column identifies the surveying company, while Date indicates when the survey was conducted. Sample shows the number of respondents, which varies by pollster, and MoE represents the margin of error. The Candidates columns display the backing for each presidential contender (note that percentages may not sum to 100% due to checking for blank or null votes). Finally, Spread estimates the lead one candidate holds over the other. If, on the other hand, we ask ourselves why larger surveys are not done, for example 50,000 people, the reason is that: You might wonder why we rely on smaller samples instead of surveying, say, 50,000 people. The primary constraint is cost; reaching such a large audience is prohibitively expensive. Furthermore, parameter estimation is inherently theoretical—providing a razor-thin margin of error implies a false sense of absolute certainty. In reality, voter opinions are fluid, no survey is perfectly random (often missing rural populations), and respondents who claim they will vote might ultimately stay home. 9.6.2 Confidence Intervals Confidence intervals are a very useful concept widely used by Data Scientists. However, it is nothing more than another way of expressing what we have already learned so far. And it is that a confidence interval of 95% tells us that there is a 95% probability that the interval we generate includes the parameter \\(p\\) that we want to estimate. This is nothing more than another way of indicating that we have to build an interval considering the margin of error, that is two standard errors around our sample mean. For the sample of 1100 people we saw in the previous section, we reported an estimate of 56% with a margin of error of \\(+- 2.99\\%\\). If we now want to use confidence intervals in our language we would say: We estimate 56% for the Blue party with a confidence interval of 95%. This confidence interval goes from 53% to 59%. 9.7 Spread Estimation Although we are interested in estimating the proportion that the Blue party would obtain \\(p\\), sometimes it is more useful to know the difference (by how much it wins/loses). For example, when we have two parties in the second round of elections not only do we have votes for Blue and Red, but also blank/spoiled. Also, in regular elections we have more than one pollster doing several surveys. So one could give 45% for Blue, 41% for Red. While another can give 41% for Blue and 38% for Red, etc. If we compare surveys, rather than knowing the exact percentage it is more useful to know by how much the blue party wins, since if we see that in all, for example it wins by 4%, with a tiny standard error, then \\(p\\) would not matter much. Only with the difference data we could take get an idea of who will win. This difference is called spread. We had defined that the voting intention for the Blue party was \\(p\\) and for the red party \\(1-p\\). So what we would expect to obtain for the difference would be \\(p - (1-p)\\), that is \\(2p - 1\\). Standard error of the spread: \\(SE[spread]=2\\sqrt{\\frac{p(1-p) }{n}}\\) We see that the standard error is twice the standard error of the average, which depends on \\(p\\), and we have already found previously an estimation to not depend on \\(p\\) but on the mean of our sample. So we will use: \\(\\hat{SE[spread]}=2\\sqrt{\\frac{\\overline{X}(1-\\overline{X}) }{n}}\\) Let’s see with an example these concepts. Let’s study the 2016 US elections. In this case we have multiple pollsters, conducting multiple surveys months prior to elections, and mainly two parties competing for president. We are going to use the polls_us_election_2016 data frame included in the dslabs library which includes data from multiple surveys conducted for the 2016 US elections between Hillary Clinton and Donald Trump. The first thing we will do is explore the data: library(dslabs) head(polls_us_election_2016) As we see, we do not have the standard error, nor the confidence interval. So we will proceed to make some mutations applying the formulas learned so far focusing on the voting intention for Hillary Clinton. surveys &lt;- polls_us_election_2016 |&gt; filter(state == &quot;U.S.&quot;) |&gt; mutate(X_avg = rawpoll_clinton/100) |&gt; mutate(SE_prom = sqrt((X_avg*(1-X_avg))/samplesize)) |&gt; mutate(inferior = X_avg - 2*SE_prom, superior = X_avg + 2*SE_prom) |&gt; select(pollster, enddate, X_avg, SE_prom, inferior, superior) # First 5 rows surveys |&gt; head(5) #&gt; pollster enddate X_avg SE_prom inferior superior #&gt; 1 ABC News/Washington Post 2016-11-06 0.4700 0.010592790 0.4488144 0.4911856 #&gt; 2 Google Consumer Surveys 2016-11-07 0.3803 0.002978005 0.3743440 0.3862560 #&gt; 3 Ipsos 2016-11-06 0.4200 0.010534681 0.3989306 0.4410694 #&gt; 4 YouGov 2016-11-07 0.4500 0.008204286 0.4335914 0.4664086 #&gt; 5 Gravis Marketing 2016-11-06 0.4700 0.003869218 0.4622616 0.4777384 For example, IPSOS in a survey published on 11/06/16 estimated 42% voting intention for Clinton with a 95% confidence interval in a range going from 39.89% to 44.10%. Does this data mean that they estimated she would lose? No, given that in this case we are using real data the proportion of votes for Clinton with those for Trump will not sum 100%. In fact, on actual election day Clinton obtained 48.2% and Trump 46.1% of total votes cast. That is real \\(p\\) was 48.2%. What we could calculate is how many of these pollsters guessed right in their estimation. That is, if in their confidence intervals is the \\(p=48.2\\%\\) that Clinton finally obtained. To do this, we will add a column guessed_right (guessed_right) with the validation of whether it is in the confidence interval and then use summarize() to calculate the percentage of surveys that guessed right. surveys |&gt; mutate(guessed_right = inferior &lt;= 0.482 &amp; 0.482 &lt;= superior) |&gt; summarize(mean(guessed_right)) #&gt; mean(guessed_right) #&gt; 1 0.2802893 Only 28% of published surveys published confidence intervals that included \\(p\\). This, among many other reasons, because at the beginning there are many more undecided who finally decide in the last weeks. Let’s analyze now how many guessed right in the spread. It could be that even though they did not estimate exact \\(p\\) the difference did remain over time. To do this, let’s add to our surveys the column spread with the difference of votes: surveys_spread &lt;- polls_us_election_2016 |&gt; filter(state == &quot;U.S.&quot;) |&gt; mutate(spread = (rawpoll_clinton - rawpoll_trump)/100) And now we are going to do a trick calculating the spread of our sample: \\(spread=2*\\overline{X}-1\\) We can transform this formula: \\(spread-1=2*\\overline{X}\\) \\(\\frac{spread-1}{2}=\\overline{X}\\) Or what is the same: \\(\\overline{X}=\\frac{spread-1}{2}\\) This formula gives us an approximation of how much \\(\\overline{X}\\) would be transformed to a scale of 0 to 100%. With it, let’s calculate the standard error and the confidence interval: surveys_spread &lt;- surveys_spread |&gt; mutate(X_avg = (spread + 1)/2) |&gt; mutate(SE_spread = 2*sqrt((X_avg*(1-X_avg))/samplesize)) |&gt; mutate(inferior = spread - 2*SE_spread, superior = spread + 2*SE_spread) |&gt; select(pollster, enddate, spread, SE_spread, inferior, superior) # First 5 rows surveys_spread |&gt; head(5) #&gt; pollster enddate spread SE_spread inferior #&gt; 1 ABC News/Washington Post 2016-11-06 0.0400 0.021206832 -0.002413664 #&gt; 2 Google Consumer Surveys 2016-11-07 0.0234 0.006132712 0.011134575 #&gt; 3 Ipsos 2016-11-06 0.0300 0.021334733 -0.012669466 #&gt; 4 YouGov 2016-11-07 0.0400 0.016478037 0.007043926 #&gt; 5 Gravis Marketing 2016-11-06 0.0400 0.007746199 0.024507601 #&gt; superior #&gt; 1 0.08241366 #&gt; 2 0.03566542 #&gt; 3 0.07266947 #&gt; 4 0.07295607 #&gt; 5 0.05549240 Now let’s calculate how many of these pollsters guessed right in their estimation. That is, if in their confidence intervals is the real value of \\(spread=48.2\\%-46.1\\%=2.1\\%\\) that Clinton finally obtained spread. To do this, we will add the column guessed_right and then summarize(). surveys_spread |&gt; mutate(guessed_right = inferior &lt;= 0.021 &amp; 0.021 &lt;= superior) |&gt; summarize(mean(guessed_right)) #&gt; mean(guessed_right) #&gt; 1 0.6735986 In this case we see how 67.3% of the time, surveys correctly estimated the difference in votes favorable to Clinton. As a clarification, final reminder of this case, even though Clinton obtained more votes she did not win the elections because the US system is different and not necessarily if you win in votes you obtain the presidency. 9.8 Estimates Outside Election Polls We have used election polls to understand statistical inference concepts. However, most Data Scientists are not related to voting intention estimation calculations. That does not mean we will not use those concepts. The central limit theorem not only works in election polls. What it means is that we will use some slightly different formulas that apply to more daily life cases. From what we have learned so far the main change is the formula to calculate the standard error. We will use instead the standard deviation \\(\\sigma\\) of the sample to calculate the standard error: \\(SE[\\overline{X}]=\\frac{\\sigma}{\\sqrt{n}}\\) Where \\(\\overline{X}\\) is the average of our random sample and \\(n\\) is the sample size. 9.8.1 Example: Estimating Average Height Let’s apply this to a real-world scenario. Suppose we want to estimate the average height of adult males in a population, but we can only measure a sample of 50 people. Using the heights dataset from dslabs, let’s simulate this process: # Load the heights data data(heights) # Our &quot;population&quot; (in reality, we wouldn&#39;t have access to this) population &lt;- heights |&gt; filter(sex == &quot;Male&quot;) |&gt; mutate(height_cm = height * 2.54) |&gt; # Convert to cm pull(height_cm) # True population parameters (unknown in real life) true_mean &lt;- mean(population) true_sd &lt;- sd(population) cat(&quot;True population mean:&quot;, round(true_mean, 2), &quot;cm\\n&quot;) #&gt; True population mean: 176.06 cm cat(&quot;True population SD:&quot;, round(true_sd, 2), &quot;cm\\n&quot;) #&gt; True population SD: 9.17 cm Now let’s take a random sample and build a 95% confidence interval: set.seed(42) # For reproducibility n &lt;- 50 # Sample size # Take a random sample sample_heights &lt;- sample(population, n, replace = FALSE) # Sample statistics sample_mean &lt;- mean(sample_heights) sample_sd &lt;- sd(sample_heights) # Standard error of the mean SE &lt;- sample_sd / sqrt(n) # 95% Confidence interval (2 standard errors) ci_lower &lt;- sample_mean - 2 * SE ci_upper &lt;- sample_mean + 2 * SE cat(&quot;Sample mean:&quot;, round(sample_mean, 2), &quot;cm\\n&quot;) #&gt; Sample mean: 176.97 cm cat(&quot;Standard error:&quot;, round(SE, 2), &quot;cm\\n&quot;) #&gt; Standard error: 1.44 cm cat(&quot;95% CI: [&quot;, round(ci_lower, 2), &quot;,&quot;, round(ci_upper, 2), &quot;] cm\\n&quot;) #&gt; 95% CI: [ 174.09 , 179.84 ] cm cat(&quot;Does CI contain true mean?&quot;, ci_lower &lt;= true_mean &amp; true_mean &lt;= ci_upper, &quot;\\n&quot;) #&gt; Does CI contain true mean? TRUE Let’s visualize this with a Monte Carlo simulation to verify that our confidence intervals work as expected: set.seed(123) n_simulations &lt;- 1000 n &lt;- 50 results &lt;- tibble( sim_id = 1:n_simulations ) |&gt; mutate( sample_data = map(sim_id, ~ sample(population, n, replace = FALSE)), sample_mean = map_dbl(sample_data, mean), sample_sd = map_dbl(sample_data, sd), SE = sample_sd / sqrt(n), ci_lower = sample_mean - 2 * SE, ci_upper = sample_mean + 2 * SE, contains_true = ci_lower &lt;= true_mean &amp; true_mean &lt;= ci_upper ) # Proportion of CIs that contain the true mean cat(&quot;Proportion of 95% CIs containing true mean:&quot;, round(mean(results$contains_true), 3), &quot;\\n&quot;) #&gt; Proportion of 95% CIs containing true mean: 0.956 This confirms that approximately 95% of our confidence intervals capture the true population mean—exactly as the theory predicts! 9.9 Exercises The most common data a Data Scientist manages comes from people, some attribute/characteristic of them. In these exercises we are going to use the heights data frame that we already used for other purposes in previous chapters. library(dslabs) data(heights) Create a vector named x to extract the height data for all individuals in the dataset. Then, report the population’s average and standard deviation, ensuring you convert the heights to meters first. Solution x &lt;- heights |&gt; filter(sex == &quot;Male&quot;) |&gt; mutate(height_m = height/39.37) |&gt; pull(height_m) # Average of the population mean(x) # Standard deviation sd(x) Mathematically we use x in lowercase to refer to our total population and X to refer to a random sample. We will denote the population mean as \\(\\mu\\) and the population standard deviation as \\(\\sigma\\) Most of the time we will not have access to the mean and standard deviation of the population because it is very large and highly expensive. Assume we cannot access the entire population and can only obtain a random sample of 100 people. Simulate this by creating a random sample with replacement from x, storing the values in a vector called X. Using this sample data, construct a 95% confidence interval to estimate the population average. Solution N &lt;- 100 X &lt;- sample(x, N, replace = TRUE) # Expected value mean(X) # Standard error se &lt;- sd(X)/sqrt(N) # 95% confidence interval is approx. 2 se ic &lt;- c(mean(X) - 2*se, mean(X) + 2*se) Validate your estimation method using a Monte Carlo simulation. Repeat the sampling process 10,000 times and calculate the percentage of expected confidence intervals that successfully capture the true population mean. Solution true_mean &lt;- mean(x) N &lt;- 100 n_times &lt;- 10000 set.seed(2025) simulation &lt;- replicate(n_times, { X &lt;- sample(x, N, replace = TRUE) se &lt;- sd(X)/sqrt(N) lower &lt;- mean(X) - 2*se upper &lt;- mean(X) + 2*se between(true_mean, lower, upper) }) mean(simulation) "],["introduction-1.html", "Introduction", " Introduction A frequent error in Data Science projects is thinking that they start with analysis. In fact, when a data analyst is asked where they spend most of their time, the answer remains the same: 80% in Data Wrangling9. Data in its natural form (Raw Data), usually contain registration errors that make exact analysis impossible. Being recorded by different systems and people, it is normal to end up with a file in which the same value is expressed in different ways (for example, a date can be recorded as June 28, or as 28/06), there may be blank records, and of course, grammatical errors. When analyzing this data, all these records have to be pre-processed. That is, the data must be cleaned, unified, consolidated, and normalized so that it can be used to extract valuable information. Data Wrangling is the process of preparing data to be leveraged. In the following chapters, we will see several common steps of the Data Wrangling process such as Importing data into R from files, converting data to tidy type, string processing, html processing, date and time formatting, and text mining. In this section, we will master the essential skills for getting data into R and reshaping it for analysis. We will start by learning how to import data from diverse sources, including CSV files, Excel spreadsheets, and web pages. Once loaded, we will explore how to transform data between “wide” and “tidy” (long) formats using pivot_longer() and pivot_wider(), ensuring our data is structured correctly for visualization and modeling. We will also cover how to combine multiple datasets using the powerful family of join functions (left_join(), inner_join(), etc.). Finally, we will delve into specialized processing techniques, including web scraping, string manipulation with regular expressions, date conversion with lubridate, and the fundamentals of text mining to extract insights from unstructured text. https://www.infoworld.com/article/3228245/the-80-20-data-science-dilemma.html↩︎ "],["data-import-and-consolidation.html", "Chapter 10 Data import and consolidation 10.1 Importing from files 10.2 Tidy data 10.3 Exercises 10.4 Joining tables 10.5 Web Scraping 10.6 Exercises", " Chapter 10 Data import and consolidation 10.1 Importing from files For importing files, whether they are text types or spreadsheets, we need to know where we are going to import the data from. 10.1.1 Working Directory By default, when we import files, R will search in the working directory. To find out the path of our working directory we will use the getwd() function. getwd() #&gt; [1] &quot;c:/Documents/dparedesi/git-repository/DS&quot; This is the path where we can place our files to load them. If we want to load data from another folder we can change the working directory using setwd(). setwd(&quot;c:/Documents/Projects/R Files&quot;) getwd() #&gt; [1] &quot;c:/Documents/Projects/R Files&quot; Note: While setwd() works, it is generally discouraged in reproducible workflows because it creates absolute paths that break when code is shared or moved. Consider using RStudio Projects combined with the here package, which provides here::here() to construct relative paths that work across different systems. For practical purposes, we are going to use a file already available in one of the previously installed packages, dslabs, when we analyzed the danger level to decide which US state to travel to. To do this, we can use the system.file() function and determine the path where the dslabs package was installed. dslabs_path &lt;- system.file(package=&quot;dslabs&quot;) Likewise, we can list the files and folders within that path using the list.files() function. dslabs_path &lt;- system.file(package=&quot;dslabs&quot;) list.files(dslabs_path) #&gt; [1] &quot;data&quot; &quot;DESCRIPTION&quot; &quot;extdata&quot; &quot;help&quot; &quot;html&quot; #&gt; [6] &quot;INDEX&quot; &quot;Meta&quot; &quot;NAMESPACE&quot; &quot;R&quot; &quot;script&quot; The folder we will use is extdata. We can access the path of this folder if we modify the parameters of the system.file() function. dslabs_path &lt;- system.file(&quot;extdata&quot;, package=&quot;dslabs&quot;) list.files(dslabs_path) #&gt; [1] &quot;2010_bigfive_regents.xls&quot; #&gt; [2] &quot;calificaciones.csv&quot; #&gt; [3] &quot;carbon_emissions.csv&quot; #&gt; [4] &quot;fertility-two-countries-example.csv&quot; #&gt; [5] &quot;HRlist2.txt&quot; #&gt; [6] &quot;life-expectancy-and-fertility-two-countries-example.csv&quot; #&gt; [7] &quot;murders.csv&quot; #&gt; [8] &quot;olive.csv&quot; #&gt; [9] &quot;RD-Mortality-Report_2015-18-180531.pdf&quot; #&gt; [10] &quot;ssa-death-probability.csv&quot; The file we will use is murders.csv. To build the complete path of this file we can concatenate the strings or we can also directly use the file.path(path, file_name) function. csv_example_path &lt;- file.path(dslabs_path, &quot;murders.csv&quot;) Finally, we will copy the file to our working directory with the file.copy(source_path, destination_path) function. file.copy(csv_example_path, getwd()) #&gt; [1] TRUE We can validate the copy with the file.exists(file_name) function. file.exists(&quot;murders.csv&quot;) #&gt; [1] TRUE It is recommended to check the documentation of the file manipulation functions. ?files 10.1.2 readr and readxl packages Now that we have the file in our working directory, we will use functions within the readr and readxl packages to import files into R. Both are included in the tidyverse package that we previously installed. library(tidyverse) # Here readr is included automatically library(readxl) The functions we will use the most will be read_csv() and read_excel(). The latter supports .xls and .xlsx extensions. data_df &lt;- read_csv(&quot;murders.csv&quot;, show_col_types = FALSE) # Once imported we can remove the file if we wish file.remove(&quot;murders.csv&quot;) #&gt; [1] TRUE We see how by default it detects the headers in the first row and assigns them a default data type. Let’s now explore our data_df object. data_df #&gt; # A tibble: 51 × 5 #&gt; state abb region population total #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Alabama AL South 4779736 135 #&gt; 2 Alaska AK West 710231 19 #&gt; 3 Arizona AZ West 6392017 232 #&gt; 4 Arkansas AR South 2915918 93 #&gt; 5 California CA West 37253956 1257 #&gt; 6 Colorado CO West 5029196 65 #&gt; 7 Connecticut CT Northeast 3574097 97 #&gt; 8 Delaware DE South 897934 38 #&gt; 9 District of Columbia DC South 601723 99 #&gt; 10 Florida FL South 19687653 669 #&gt; # ℹ 41 more rows The first thing it indicates is that the object is of type tibble. This object is very similar to a data frame, but with improved features such as, for example, the number of rows and columns in the console, the data type under the header, the default report of only the first 10 records automatically, among many others that we will discover in this chapter. The same syntax and logic would apply for importing an excel file. In this case we are importing directly from the package path and not from our working directory. excel_example_path &lt;- file.path(dslabs_path, &quot;2010_bigfive_regents.xls&quot;) data_df_from_excel &lt;- read_excel(excel_example_path) readr gives us 7 different types of functions for importing flat files: The readr package provides a versatile suite of functions for importing flat files, each tailored to a specific delimiter or format. The most common is read_csv(), designed for comma-separated values. For tab-separated files, we use read_tsv(), while read_delim() offers a general-purpose solution for files with custom delimiters (like semicolons or pipes). Other specialized functions include read_fwf() for fixed-width files, read_table() for whitespace-separated columns, and read_log() for parsing standard web server logs. Tip: For files that use semicolons as delimiters (common in European locales), use read_csv2() or read_delim() with delim = \";\". Also, if you encounter import issues, use problems(data_df) after importing to diagnose parsing errors. [!TIP] Clean Column Names: Interpreted data often has column names with spaces or capital letters (e.g., “Customer ID”). We highly recommend piping your data into janitor::clean_names() immediately after reading it to standardize everything to snake_case (e.g., “customer_id”). 10.1.3 Importing files from the internet We have seen how we can enter the full path to load a file directly from another source different from our working directory. In the same way, if we have a file in an internet path we can pass it directly to R since read_csv() and the other readr import functions support URL input as a parameter. Here we see the import of grades from students of the Data Science with R course. ##### Example 1: # Historical grades data url &lt;- &quot;https://dparedesi.github.io/DS-with-R-datasets/student-grades.csv&quot; grades &lt;- read_csv(url, show_col_types = FALSE) grades &lt;- grades |&gt; mutate(total = (P1 + P2 + P3 + P4 + P5 + P6)/30*20) grades |&gt; select(P1, P2, P3, P4, P5, total) |&gt; summary() From this we could visualize a histogram: hist(grades$total) Or we could compare between genders which one has the highest median: grades |&gt; ggplot() + aes(gender, total) + geom_boxplot() We could also extract updated Covid-19 information. ##### Example 2: # Covid-19 Data url &lt;- &quot;https://covid.ourworldindata.org/data/owid-covid-data.csv&quot; internet_data &lt;- read_csv(url, show_col_types = FALSE) internet_data |&gt; arrange(desc(date)) |&gt; head(10) 10.2 Tidy data Ordered data or tidy data are those obtained from a process called data tidying. It is one of the important cleaning processes during processing of large data or ‘big data’ and is a very used step in Data Science. The main characteristics are that each different observation of that variable has to be in a different row and that each variable you measure has to be in a column (Leek 2015). As we may have noticed, we have been using tidy data since the first chapters. However, not all our data comes ordered. Most of it comes in what we call wide data or wide data. For example, we have previously used data from Gapminder. Let’s filter the data from Germany and South Korea to remember how we had our data. gapminder |&gt; filter(country %in% c(&quot;South Korea&quot;, &quot;Germany&quot;)) |&gt; head(10) #&gt; country year infant_mortality life_expectancy fertility population #&gt; 1 Germany 1960 34.0 69.26 2.41 73179665 #&gt; 2 South Korea 1960 80.2 53.02 6.16 25074028 #&gt; 3 Germany 1961 NA 69.85 2.44 73686490 #&gt; 4 South Korea 1961 76.1 53.75 5.99 25808542 #&gt; 5 Germany 1962 NA 70.01 2.47 74238494 #&gt; 6 South Korea 1962 72.4 54.51 5.79 26495107 #&gt; 7 Germany 1963 NA 70.10 2.49 74820389 #&gt; 8 South Korea 1963 68.8 55.27 5.57 27143075 #&gt; 9 Germany 1964 NA 70.66 2.49 75410766 #&gt; 10 South Korea 1964 65.3 56.04 5.36 27770874 #&gt; gdp continent region #&gt; 1 NA Europe Western Europe #&gt; 2 28928298962 Asia Eastern Asia #&gt; 3 NA Europe Western Europe #&gt; 4 30356298714 Asia Eastern Asia #&gt; 5 NA Europe Western Europe #&gt; 6 31102566019 Asia Eastern Asia #&gt; 7 NA Europe Western Europe #&gt; 8 34067175844 Asia Eastern Asia #&gt; 9 NA Europe Western Europe #&gt; 10 36643076469 Asia Eastern Asia We notice that each row is an observation and each column represents a variable. It is ordered data, tidy data. On the other hand, we can see what the data was like for these two countries if we access the source in the dslabs package. fertility_path &lt;- file.path(dslabs_path, &quot;fertility-two-countries-example.csv&quot;) wide_data &lt;- read_csv(fertility_path, show_col_types = FALSE) wide_data #&gt; # A tibble: 2 × 57 #&gt; country `1960` `1961` `1962` `1963` `1964` `1965` `1966` `1967` `1968` `1969` #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Germany 2.41 2.44 2.47 2.49 2.49 2.48 2.44 2.37 2.28 2.17 #&gt; 2 South K… 6.16 5.99 5.79 5.57 5.36 5.16 4.99 4.85 4.73 4.62 #&gt; # ℹ 46 more variables: `1970` &lt;dbl&gt;, `1971` &lt;dbl&gt;, `1972` &lt;dbl&gt;, `1973` &lt;dbl&gt;, #&gt; # `1974` &lt;dbl&gt;, `1975` &lt;dbl&gt;, `1976` &lt;dbl&gt;, `1977` &lt;dbl&gt;, `1978` &lt;dbl&gt;, #&gt; # `1979` &lt;dbl&gt;, `1980` &lt;dbl&gt;, `1981` &lt;dbl&gt;, `1982` &lt;dbl&gt;, `1983` &lt;dbl&gt;, #&gt; # `1984` &lt;dbl&gt;, `1985` &lt;dbl&gt;, `1986` &lt;dbl&gt;, `1987` &lt;dbl&gt;, `1988` &lt;dbl&gt;, #&gt; # `1989` &lt;dbl&gt;, `1990` &lt;dbl&gt;, `1991` &lt;dbl&gt;, `1992` &lt;dbl&gt;, `1993` &lt;dbl&gt;, #&gt; # `1994` &lt;dbl&gt;, `1995` &lt;dbl&gt;, `1996` &lt;dbl&gt;, `1997` &lt;dbl&gt;, `1998` &lt;dbl&gt;, #&gt; # `1999` &lt;dbl&gt;, `2000` &lt;dbl&gt;, `2001` &lt;dbl&gt;, `2002` &lt;dbl&gt;, `2003` &lt;dbl&gt;, … We see that the original data had two rows, one per country and then each column represented a year. This is what we call wide data or wide data. Normally we will have wide data that we first have to convert to tidy data to later be able to perform our analyses. 10.2.1 Transforming to tidy data The tidyverse library provides two functions to reshape data between wide and long (tidy) formats. We use pivot_longer() to convert from wide data to tidy data and pivot_wider() to convert from tidy data to wide data. 10.2.1.1 pivot_longer function Let’s see the utility with the wide_data object that we created in the previous section as a result of importing data from the csv. First apply the pivot_longer() function to explore the conversion that is performed by default. tidy_data &lt;- wide_data |&gt; pivot_longer(cols = -country, names_to = &quot;key&quot;, values_to = &quot;value&quot;) tidy_data #&gt; # A tibble: 112 × 3 #&gt; country key value #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Germany 1960 2.41 #&gt; 2 Germany 1961 2.44 #&gt; 3 Germany 1962 2.47 #&gt; 4 Germany 1963 2.49 #&gt; 5 Germany 1964 2.49 #&gt; 6 Germany 1965 2.48 #&gt; 7 Germany 1966 2.44 #&gt; 8 Germany 1967 2.37 #&gt; 9 Germany 1968 2.28 #&gt; 10 Germany 1969 2.17 #&gt; # ℹ 102 more rows We see how the pivot_longer() function has collected the columns into two, the names column (“key”) and the values column (“value”). We can change the title of these new columns, for example “year” and “fertility”. tidy_data &lt;- wide_data |&gt; pivot_longer(cols = -country, names_to = &quot;year&quot;, values_to = &quot;fertility&quot;) tidy_data #&gt; # A tibble: 112 × 3 #&gt; country year fertility #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Germany 1960 2.41 #&gt; 2 Germany 1961 2.44 #&gt; 3 Germany 1962 2.47 #&gt; 4 Germany 1963 2.49 #&gt; 5 Germany 1964 2.49 #&gt; 6 Germany 1965 2.48 #&gt; 7 Germany 1966 2.44 #&gt; 8 Germany 1967 2.37 #&gt; 9 Germany 1968 2.28 #&gt; 10 Germany 1969 2.17 #&gt; # ℹ 102 more rows We use cols = -country to exclude the country column from being pivoted. By default the column names are collected as text. To convert them to numbers we use the names_transform argument. tidy_data &lt;- wide_data |&gt; pivot_longer(cols = -country, names_to = &quot;year&quot;, values_to = &quot;fertility&quot;, names_transform = list(year = as.integer)) tidy_data #&gt; # A tibble: 112 × 3 #&gt; country year fertility #&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Germany 1960 2.41 #&gt; 2 Germany 1961 2.44 #&gt; 3 Germany 1962 2.47 #&gt; 4 Germany 1963 2.49 #&gt; 5 Germany 1964 2.49 #&gt; 6 Germany 1965 2.48 #&gt; 7 Germany 1966 2.44 #&gt; 8 Germany 1967 2.37 #&gt; 9 Germany 1968 2.28 #&gt; 10 Germany 1969 2.17 #&gt; # ℹ 102 more rows This data would now be ready to create graphs using ggplot(). tidy_data |&gt; ggplot() + aes(year, fertility, color = country) + geom_point() 10.2.1.2 pivot_wider function Sometimes, as we will see in the following section, it will be useful to go back from rows to columns. For this we will use the pivot_wider() function, where we specify names_from (the column containing the new column names) and values_from (the column containing the values). Additionally, we can use the : operator to indicate from which column to which column we want to select. tidy_data |&gt; pivot_wider(names_from = year, values_from = fertility) |&gt; select(country, `1965`:`1970`) #&gt; # A tibble: 2 × 7 #&gt; country `1965` `1966` `1967` `1968` `1969` `1970` #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Germany 2.48 2.44 2.37 2.28 2.17 2.04 #&gt; 2 South Korea 5.16 4.99 4.85 4.73 4.62 4.53 10.2.2 separate function In the cases described above we had a situation with relatively ordered data. We only had to do a collection transformation and converted to tidy data. However, the data is not always stored in such an easily interpretable way. Sometimes we have data like this: path &lt;- file.path(dslabs_path, &quot;life-expectancy-and-fertility-two-countries-example.csv&quot;) data &lt;- read_csv(path, show_col_types = FALSE) data |&gt; select(1:5) #Report first 5 columns #&gt; # A tibble: 2 × 5 #&gt; country `1960_fertility` `1960_life_expectancy` `1961_fertility` #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Germany 2.41 69.3 2.44 #&gt; 2 South Korea 6.16 53.0 5.99 #&gt; # ℹ 1 more variable: `1961_life_expectancy` &lt;dbl&gt; If we apply pivot_longer() directly we would not have our data ordered yet. Let’s see: data |&gt; pivot_longer(cols = -country, names_to = &quot;key_col&quot;, values_to = &quot;value_col&quot;) #&gt; # A tibble: 224 × 3 #&gt; country key_col value_col #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Germany 1960_fertility 2.41 #&gt; 2 Germany 1960_life_expectancy 69.3 #&gt; 3 Germany 1961_fertility 2.44 #&gt; 4 Germany 1961_life_expectancy 69.8 #&gt; 5 Germany 1962_fertility 2.47 #&gt; 6 Germany 1962_life_expectancy 70.0 #&gt; 7 Germany 1963_fertility 2.49 #&gt; 8 Germany 1963_life_expectancy 70.1 #&gt; 9 Germany 1964_fertility 2.49 #&gt; 10 Germany 1964_life_expectancy 70.7 #&gt; # ℹ 214 more rows We will use the separate() function to separate a column into multiple columns using a specific separator. In this case our separator would be the character _. Also, we will add the attribute extra=\"merge\" to indicate that if there is more than one separator character, do not separate them and keep them joined. data |&gt; pivot_longer(cols = -country, names_to = &quot;key_col&quot;, values_to = &quot;value_col&quot;) |&gt; separate(key_col, c(&quot;year&quot;, &quot;other_var&quot;), sep=&quot;_&quot;, extra = &quot;merge&quot;) #&gt; # A tibble: 224 × 4 #&gt; country year other_var value_col #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Germany 1960 fertility 2.41 #&gt; 2 Germany 1960 life_expectancy 69.3 #&gt; 3 Germany 1961 fertility 2.44 #&gt; 4 Germany 1961 life_expectancy 69.8 #&gt; 5 Germany 1962 fertility 2.47 #&gt; 6 Germany 1962 life_expectancy 70.0 #&gt; 7 Germany 1963 fertility 2.49 #&gt; 8 Germany 1963 life_expectancy 70.1 #&gt; 9 Germany 1964 fertility 2.49 #&gt; 10 Germany 1964 life_expectancy 70.7 #&gt; # ℹ 214 more rows We already have the year separated, but this data is still not tidy data since there is a row for fertility and a row for life expectancy for each country. We have to pass these values from row to columns. And for that we already learned that we can use the pivot_wider() function data |&gt; pivot_longer(cols = -country, names_to = &quot;key_col&quot;, values_to = &quot;value_col&quot;) |&gt; separate(key_col, c(&quot;year&quot;, &quot;other_var&quot;), sep=&quot;_&quot;, extra = &quot;merge&quot;) |&gt; pivot_wider(names_from = other_var, values_from = value_col) #&gt; # A tibble: 112 × 4 #&gt; country year fertility life_expectancy #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Germany 1960 2.41 69.3 #&gt; 2 Germany 1961 2.44 69.8 #&gt; 3 Germany 1962 2.47 70.0 #&gt; 4 Germany 1963 2.49 70.1 #&gt; 5 Germany 1964 2.49 70.7 #&gt; 6 Germany 1965 2.48 70.6 #&gt; 7 Germany 1966 2.44 70.8 #&gt; 8 Germany 1967 2.37 71.0 #&gt; 9 Germany 1968 2.28 70.6 #&gt; 10 Germany 1969 2.17 70.5 #&gt; # ℹ 102 more rows In other cases, instead of separating a column we will want to join them. In future cases we will see how the unite(column_1, column2) function can also be useful. 10.3 Exercises Access the Uber Peru 2010 dataset from this link and attempt to import it into an object named uber_peru_2010. Pay attention to the delimiters used in the file. Solution url &lt;- &quot;https://dparedesi.github.io/DS-with-R-datasets/uber-peru-2010.csv&quot; # We will use read_csv since it is separated by commas uber_peru_2010 &lt;- read_csv(url, show_col_types = FALSE) # Upon importing it we realize it is separated by &quot;;&quot; uber_peru_2010 |&gt; head() # Therefore we import again using read_delim uber_peru_2010 &lt;- read_delim(&quot;external/uber-peru-2010.csv&quot;, delim = &quot;;&quot;, col_types = cols(.default = &quot;c&quot;) ) uber_peru_2010 |&gt; head() Import the SINADEF deaths registry from this source into an object called deaths. Ensure you handle the file encoding correctly to avoid character issues. Solution url &lt;- &quot;https://www.datosabiertos.gob.pe/sites/default/files/sinadef-deaths.csv&quot; url &lt;- &quot;https://dparedesi.github.io/DS-with-R-datasets/sinadef-deaths.csv&quot; # We will use read_delim because it is delimited by &quot;;&quot; and not by &quot;,&quot; # Also we change the encoding to avoid error in loading deaths &lt;- read_delim(url, &quot;;&quot;, local = locale(encoding = &quot;latin1&quot;)) Download the resource file from this link to a temporary location. Validating that the file exists, load the specific sheet named “Deflators” into an object named data. Solution # Store the url url &lt;- &quot;https://dparedesi.github.io/DS-with-R-datasets/resources-other-idd.xlsx&quot; # Create a temporary name &amp; path for our file. See: ?tempfile temp_file &lt;- tempfile() # Download the file to our temp download.file(url, temp_file) # Import the excel dat &lt;- read_excel(temp_file, sheet = &quot;Deflators&quot;) # Remove the temporary file file.remove(temp_file) For the following files run the following code so that you have access to the objects referred to in the problems: # GDP by countries url &lt;- &quot;https://dparedesi.github.io/DS-with-R-datasets/gdp.csv&quot; gdp &lt;- read_csv(url, show_col_types = FALSE) # Diseases by years by countries url &lt;- &quot;https://dparedesi.github.io/DS-with-R-datasets/diseases-evolution.csv&quot; diseases_wide &lt;- read_csv(url, show_col_types = FALSE) # Number of female mayors url &lt;- &quot;https://dparedesi.github.io/DS-with-R-datasets/female-mayors.csv&quot; female_mayors &lt;- read_csv(url, show_col_types = FALSE) # Evolution of a university url &lt;- &quot;https://dparedesi.github.io/DS-with-R-datasets/university.csv&quot; university &lt;- read_csv(url, show_col_types = FALSE) Examine the structure of the gdp dataset. Transform it into a tidy format suitable for analysis, and then create a line plot visualizing the evolution of GDP over time for each country. Solution # To tidy data gdp &lt;- gdp |&gt; pivot_longer(cols = -country, names_to = &quot;year&quot;, values_to = &quot;gdp&quot;, names_transform = list(year = as.integer)) gdp # Visualization gdp |&gt; ggplot() + aes(year, gdp, color=country) + geom_line() The diseases_wide object contains disease counts in a wide format. Reshape this dataframe into a tidy structure where the specific diseases are consolidated into a single column. Solution # Solution diseases_1 &lt;- diseases_wide |&gt; pivot_longer(cols = c(-country, -year, -population), names_to = &quot;disease&quot;, values_to = &quot;count&quot;) diseases_1 # Alternative solution. Instead of indicating what to omit, we indicate what to take into account diseases_2 &lt;- diseases_wide |&gt; pivot_longer(cols = HepatitisA:Rubella, names_to = &quot;disease&quot;, values_to = &quot;count&quot;) diseases_2 Convert the female_mayors dataset, which is currently in a long format, into a wide format where the variables are spread across columns. Solution female_mayors &lt;- female_mayors |&gt; pivot_wider(names_from = variable, values_from = total) The university dataset is untidy. Reshape it by first pivoting longer to gather variables, separating the combined variable names, and then pivoting wider to achieve a final tidy structure. Solution university &lt;- university |&gt; pivot_longer(cols = -semester, names_to = &quot;variable&quot;, values_to = &quot;value&quot;) |&gt; separate(variable, c(&quot;name&quot;, &quot;variable2&quot;), sep=&quot;_&quot;) |&gt; pivot_wider(names_from = variable2, values_from = value) university 10.4 Joining tables Regularly we will have data from different sources that we then have to combine to be able to perform our analyses. For this we will learn different groups of functions that will allow us to combine multiple objects. 10.4.1 Join functions Join functions are the most used in table crossing. To use them we have to make sure we have the dplyr library installed. library(dplyr) This library includes a variety of functions to combine tables. The dplyr package offers a family of join functions to combine tables based on common keys. The most frequently used is left_join(), which preserves all rows from the first (left) table and appends matching data from the second. Conversely, right_join() keeps all rows from the second table. inner_join() is more restrictive, retaining only the rows that have matching keys in both tables, effectively filtering for the intersection. full_join() does the opposite, keeping all rows from both tables and filling missing values with NA. Finally, filtering joins like semi_join() (keeps rows in the first table that match the second) and anti_join() (keeps rows in the first table that do not match the second) are excellent for data validation and filtering without adding new columns. To see the join functions with examples we will use the following files: url_1 &lt;- &quot;https://dparedesi.github.io/DS-with-R-datasets/join-card.csv&quot; url_2 &lt;- &quot;https://dparedesi.github.io/DS-with-R-datasets/join-customer.csv&quot; card_data_1 &lt;- read_csv(url_1, col_types = cols(id = col_character())) customer_data_2 &lt;- read_csv(url_2, col_types = cols(id = col_character())) card_data_1 #&gt; # A tibble: 6 × 3 #&gt; id customer_type card #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 45860518 premium VISA gold #&gt; 2 46534312 bronze Mastercard Black #&gt; 3 47564535 silver VISA platinum #&gt; 4 48987654 bronze American Express #&gt; 5 78765434 gold VISA Signature #&gt; 6 41346556 premium Diners Club customer_data_2 #&gt; # A tibble: 8 × 4 #&gt; id first_name last_name mother_last_name #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 49321442 Iver Castro Rivera #&gt; 2 47564535 Enrique Gutierrez Rivasplata #&gt; 3 48987654 Alexandra Cupe Gaspar #&gt; 4 47542345 Christiam Olortegui Roca #&gt; 5 41346556 Karen Jara Mory #&gt; 6 45860518 Hebert Lopez Chavez #&gt; 7 71234321 Jesus Valle Mariños #&gt; 8 73231243 Jenny Sosa Sosa 10.4.1.1 Left join Given two tables with the same identifier (in our case our identifier consists only of a single column: ID), the left join function maintains the information of the first table and completes it with the data that crosses in the second table left_join(card_data_1, customer_data_2, by = c(&quot;id&quot;)) #&gt; # A tibble: 6 × 6 #&gt; id customer_type card first_name last_name mother_last_name #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 45860518 premium VISA gold Hebert Lopez Chavez #&gt; 2 46534312 bronze Mastercard Black &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 3 47564535 silver VISA platinum Enrique Gutierrez Rivasplata #&gt; 4 48987654 bronze American Express Alexandra Cupe Gaspar #&gt; 5 78765434 gold VISA Signature &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 6 41346556 premium Diners Club Karen Jara Mory As we can see, the first three columns are exactly the same as we initially had and to the right of those columns we see the columns of the other table for the values ​​that did cross the data. In this case we are facing a data inconsistency since all customers of card_data_1 should be in customer_data_2. This inconsistency could lead us to have to map the data loss process, etc. 10.4.1.2 Right join Given two tables with the same identifier, the right join function maintains the information of the second table and completes it with the data that crosses in the first table right_join(card_data_1, customer_data_2, by = &quot;id&quot;) #&gt; # A tibble: 8 × 6 #&gt; id customer_type card first_name last_name mother_last_name #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 45860518 premium VISA gold Hebert Lopez Chavez #&gt; 2 47564535 silver VISA platinum Enrique Gutierrez Rivasplata #&gt; 3 48987654 bronze American Express Alexandra Cupe Gaspar #&gt; 4 41346556 premium Diners Club Karen Jara Mory #&gt; 5 49321442 &lt;NA&gt; &lt;NA&gt; Iver Castro Rivera #&gt; 6 47542345 &lt;NA&gt; &lt;NA&gt; Christiam Olortegui Roca #&gt; 7 71234321 &lt;NA&gt; &lt;NA&gt; Jesus Valle Mariños #&gt; 8 73231243 &lt;NA&gt; &lt;NA&gt; Jenny Sosa Sosa The idea is the same as in left_join, only this time the NA are in the first two columns. 10.4.1.3 Inner join In this case we will only have the intersection of the tables. Only the result of the data that are in both tables will be shown. inner_join(card_data_1, customer_data_2, by = &quot;id&quot;) #&gt; # A tibble: 4 × 6 #&gt; id customer_type card first_name last_name mother_last_name #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 45860518 premium VISA gold Hebert Lopez Chavez #&gt; 2 47564535 silver VISA platinum Enrique Gutierrez Rivasplata #&gt; 3 48987654 bronze American Express Alexandra Cupe Gaspar #&gt; 4 41346556 premium Diners Club Karen Jara Mory 10.4.1.4 Full join Full join is a total crossing of both. It shows us all the data that are in both the first and the second table. full_join(card_data_1, customer_data_2, by = &quot;id&quot;) #&gt; # A tibble: 10 × 6 #&gt; id customer_type card first_name last_name mother_last_name #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 45860518 premium VISA gold Hebert Lopez Chavez #&gt; 2 46534312 bronze Mastercard Black &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 3 47564535 silver VISA platinum Enrique Gutierrez Rivasplata #&gt; 4 48987654 bronze American Express Alexandra Cupe Gaspar #&gt; 5 78765434 gold VISA Signature &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 6 41346556 premium Diners Club Karen Jara Mory #&gt; 7 49321442 &lt;NA&gt; &lt;NA&gt; Iver Castro Rivera #&gt; 8 47542345 &lt;NA&gt; &lt;NA&gt; Christiam Olortegui Roca #&gt; 9 71234321 &lt;NA&gt; &lt;NA&gt; Jesus Valle Mariños #&gt; 10 73231243 &lt;NA&gt; &lt;NA&gt; Jenny Sosa Sosa Tip: To join on multiple columns, use a vector: by = c(\"col1\", \"col2\"). To join on columns with different names, use named vectors: by = c(\"left_col\" = \"right_col\"). 10.4.1.5 Semi join The case of the semi join is very similar to left_join with the difference that it only shows us the columns of the first table and eliminates the data that did not manage to cross (what in left_join comes out as NA). Also, none of the columns of table 2 appear. This is like doing a filter requesting the following: show me only the data from table 1 that is also in table 2. semi_join(card_data_1, customer_data_2, by = &quot;id&quot;) #&gt; # A tibble: 4 × 3 #&gt; id customer_type card #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 45860518 premium VISA gold #&gt; 2 47564535 silver VISA platinum #&gt; 3 48987654 bronze American Express #&gt; 4 41346556 premium Diners Club 10.4.1.6 Anti join In the case of anti_join we have the opposite of semi_join since it shows the data from table 1 that are not in table 2. anti_join(card_data_1, customer_data_2, by = &quot;id&quot;) #&gt; # A tibble: 2 × 3 #&gt; id customer_type card #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 46534312 bronze Mastercard Black #&gt; 2 78765434 gold VISA Signature 10.4.2 Joining without a common identifier Likewise, we will have some moments when we need to combine only two objects, without using any type of intersection. For this we will use the bind functions. These functions allow us to put together two vectors or tables either in rows or columns. 10.4.2.1 Union of vectors If we have two or more vectors of the same size we can create the union of the columns to create a table using the bind_cols() function. Let’s see with an example: vector_1 &lt;- c(&quot;hello&quot;, &quot;Have you seen&quot;, &quot;the&quot;) vector_2 &lt;- c(&quot;Julian&quot;, &quot;Carla&quot;, &quot;Wednesday&quot;) result &lt;- bind_cols(greeting = vector_1, nouns = vector_2) result #&gt; # A tibble: 3 × 2 #&gt; greeting nouns #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 hello Julian #&gt; 2 Have you seen Carla #&gt; 3 the Wednesday 10.4.2.2 Union of tables In the case of tables the use is the same. Likewise, we can also join the rows of two or more tables. To see its application let’s first create some example tables: table_1 &lt;- data.frame( name = c(&quot;Jhasury&quot;, &quot;Thomas&quot;, &quot;Andres&quot;, &quot;Josep&quot;), surname = c(&quot;Campos&quot;, &quot;Gonzales&quot;, &quot;Santiago&quot;, &quot;Villaverde&quot;), address = c(&quot;Jr. los campos 471&quot;, &quot;Av. Casuarinas 142&quot;, NA, &quot;Av. Tupac Amaru 164&quot;), phone = c(&quot;976567325&quot;, &quot;956732587&quot;, &quot;961445664&quot;, &quot;987786453&quot;) ) table_2 &lt;- data.frame( age = c(21, 24, 19, 12), sign = c(&quot;Aries&quot;, &quot;Capricorn&quot;, &quot;Sagittarius&quot;, &quot;Libra&quot;) ) # Create a table from row 2 to 3 of table_1 table_3 &lt;- table_1[2:3, ] Once we have our tables let’s proceed to join them. We see that they do not have a common identifier. result &lt;- bind_cols(table_1, table_2) result #&gt; name surname address phone age sign #&gt; 1 Jhasury Campos Jr. los campos 471 976567325 21 Aries #&gt; 2 Thomas Gonzales Av. Casuarinas 142 956732587 24 Capricorn #&gt; 3 Andres Santiago &lt;NA&gt; 961445664 19 Sagittarius #&gt; 4 Josep Villaverde Av. Tupac Amaru 164 987786453 12 Libra or joining by rows like this: result &lt;- bind_rows(table_1, table_3) result #&gt; name surname address phone #&gt; 1 Jhasury Campos Jr. los campos 471 976567325 #&gt; 2 Thomas Gonzales Av. Casuarinas 142 956732587 #&gt; 3 Andres Santiago &lt;NA&gt; 961445664 #&gt; 4 Josep Villaverde Av. Tupac Amaru 164 987786453 #&gt; 5 Thomas Gonzales Av. Casuarinas 142 956732587 #&gt; 6 Andres Santiago &lt;NA&gt; 961445664 10.5 Web Scraping Web Scraping is the process of extracting data from a website. We will use it when we need to extract data directly from tables that are presented on websites. For this we will use the rvest library, included in the tidyverse library. library(tidyverse) library(rvest) Important: When scraping websites, always respect the site’s robots.txt file and terms of service. Avoid making excessive requests that could overload servers. For commercial use, consider whether the data is licensed or requires permission. The function we will use the most will be read_html() and as an argument we will place the url of the web from where we want to extract the data. We are not talking about a url that downloads a text file but a web page like this: Thus, we will use read_html() to store all the web html and then little by little access the table data in R. html_data &lt;- read_html(&quot;https://es.wikipedia.org/wiki/Anexo:Pa%C3%ADses_hispanos_por_poblaci%C3%B3n&quot;) html_data #&gt; {html_document} #&gt; &lt;html class=&quot;client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-pinned-clientpref-1 vector-feature-night-mode-enabled skin-theme-clientpref-day vector-sticky-header-enabled vector-toc-available&quot; lang=&quot;es&quot; dir=&quot;ltr&quot;&gt; #&gt; [1] &lt;head&gt;\\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8 ... #&gt; [2] &lt;body class=&quot;skin--responsive skin-vector skin-vector-search-vue mediawik ... Now that we have the data stored in the object we have to go looking for the data, doing scraping. For this we will use the html_nodes(\"table\") function to access the “table” node. web_tables &lt;- html_data |&gt; html_elements(&quot;table&quot;) Finally, we have to go index by index looking for the table that interests us. To give it table format we will use html_table. In this case we will use double brackets because it is a list of lists and the setNames() function to change the name of the columns. # We format as table and store in raw_table raw_table &lt;- web_tables[[1]] |&gt; html_table() # Change header names raw_table &lt;- raw_table |&gt; setNames( c(&quot;N&quot;, &quot;country&quot;, &quot;population&quot;, &quot;pop_prop&quot;, &quot;avg_change&quot;, &quot;link&quot;) ) # Convert to tibble raw_table &lt;- raw_table |&gt; as_tibble() # Report first rows raw_table |&gt; head(5) #&gt; # A tibble: 1 × 2 #&gt; N country #&gt; &lt;lgl&gt; &lt;chr&gt; #&gt; 1 NA Este artículo o sección se encuentra desactualizado.La información sumi… We already have our data imported and we could already start exploring its content in detail. 10.6 Exercises For the following exercises we will use objects from the Lahman library, which contains US baseball player data. Run the following Script before starting to solve the exercises. install.packages(&quot;Lahman&quot;) library(Lahman) # Top 10 players of the year 2016 top_players &lt;- Batting |&gt; filter(yearID == 2016) |&gt; arrange(desc(HR)) |&gt; # sorted by number of &quot;Home run&quot; slice(1:10) # Take from row 1 to 10 top_players &lt;- top_players |&gt; as_tibble() # List of all baseball players from recent years master &lt;- Master |&gt; as_tibble() # Awards won by players awards &lt;- AwardsPlayers |&gt; filter(yearID == 2016) |&gt; as_tibble() Using the top_players and Master datasets, join them to retrieve the playerID, first name, last name, and home runs (HR) for the top 10 players of 2016. Solution top_10 &lt;- left_join(top_players, master, by = &quot;playerID&quot;) |&gt; select(playerID, nameFirst, nameLast, HR) top_10 Identify the intersection of top players and award winners. List the ID and names of the top 10 players from 2016 who also won at least one award that year. Solution semi_join(top_10, awards, by = &quot;playerID&quot;) Find the players who won awards in 2016 but did not make it into the top 10 list. Report their IDs and names. Solution # First we calculate all prizes of those who are not top 10: non_top_award_ids &lt;- anti_join(awards, top_10, by = &quot;playerID&quot;) |&gt; select(playerID) # As a player could have obtained several prizes we obtain unique values non_top_award_ids &lt;- unique(non_top_award_ids) # Then we cross with the master to obtain the names other_names &lt;- left_join(non_top_award_ids, master, by = &quot;playerID&quot;) |&gt; select(playerID, nameFirst, nameLast) other_names Scrape the MLB payroll data from http://www.stevetheump.com/Payrolls.htm. Store the entire page html, extract the tables, and specifically isolate the fourth table (node 4), formatting it as a data frame. Solution url &lt;- &quot;http://www.stevetheump.com/Payrolls.htm&quot; html &lt;- read_html(url) nodes &lt;- html |&gt; html_elements(&quot;table&quot;) nodes[[4]] |&gt; html_table() Using the scraped tables, prepare the 2019 payroll (node 4) and 2018 payroll (node 5) data. Standardize the column names to team, payroll_2019, and payroll_2018 respectively. Finally, perform a full join to combine these datasets by team name. Solution payroll_2019 &lt;- nodes[[4]] |&gt; html_table() payroll_2018 &lt;- nodes[[5]] |&gt; html_table() ####### Payroll 2019: ################ #We eliminate row 15 which is the league average: payroll_2019 &lt;- payroll_2019[-15, ] #We filter the requested columns: payroll_2019 &lt;- payroll_2019 |&gt; select(X2, X4) |&gt; rename(team = X2, payroll_2019 = X4) # We eliminate row 1 since it is the source header payroll_2019 &lt;- payroll_2019[-1,] ####### Payroll 2018: ################ # We select the two columns that interest us and #change name to headers payroll_2018 &lt;- payroll_2018 |&gt; select(Team, Payroll) |&gt; rename(team = Team, payroll_2018 = Payroll) ####### Full join: ################ full_join(payroll_2018, payroll_2019, by = &quot;team&quot;) References "],["introduction-2.html", "Introduction 10.7 Learning Objectives 10.8 Chapter Structure", " Introduction We have seen so far how to work with data: importing, cleaning, and visualizing it. Performing analysis of what has happened allows us to take a determined action to change the course of a business. However, the true power of data science lies in using this data to predict the future. Predictive analysis is a technique that every Data Scientist must master, and Machine learning provides us with robust algorithms to make these predictions. Machine learning is the study of computer algorithms that improve automatically through experience. It is a subset of artificial intelligence where algorithms create mathematical models based on sample data, known as “training data”, to make predictions or decisions without being explicitly programmed to do so. Applications range from recommendation engines (like Netflix or Spotify) to fraud detection and self-driving cars (Michell 1997). A good data scientist knows how to build prediction algorithms using machine learning. In this book, we will focus on the two main approaches: We will explore two primary approaches to machine learning. Supervised Learning involves training models on labeled data where we know the correct answer, allowing us to predict outcomes for new, unseen data—either as numbers (regression) or categories (classification). In contrast, Unsupervised Learning deals with unlabeled data, where the goal is to discover hidden patterns, structures, or groupings (clustering) without a pre-defined answer key. Keep in mind that there are also other approaches, such as semi-supervised learning or reinforcement learning where the algorithm learns from a real or synthetic environment. These approaches will not be covered in this book, which focuses on the foundational techniques for starting out as a data scientist. 10.7 Learning Objectives By the end of this chapter, you will be able to: In this chapter, we will learn to distinguish between supervised and unsupervised learning approaches. We will implement core algorithms such as k-Nearest Neighbors (kNN), Logistic Regression, and Random Forest for classification tasks, and build regression models to predict continuous variables. Additionally, we will evaluate model performance using essential metrics like confusion matrices and ROC curves, apply clustering techniques to segment data, and select the optimal model using the modern tidymodels framework. 10.8 Chapter Structure We will cover the two main approaches to machine learning: Our journey covers these two fundamental pillars. We begin with Supervised Learning, focusing on models that learn from historical data to predict future outcomes, covering both classification and regression problems. We then move to Unsupervised Learning, techniques designed to find structure in unlabeled data, such as grouping similar customers or reducing complex datasets to their essential features. Keep in mind that there are also other approaches, such as semi-supervised learning or reinforcement learning where the algorithm learns from a real or synthetic environment. These approaches will not be covered in this book, which focuses on the most commonly used approaches for starting out as a data scientist. References "],["supervised-learning.html", "Chapter 11 Supervised Learning 11.1 Classification and Regression 11.2 kNN: k-Nearest Neighbors 11.3 tidymodels Framework 11.4 Confusion Matrix 11.5 Exercises 11.6 Simple Linear Regression 11.7 Multiple Linear Regression 11.8 Standard Method for Evaluating Accuracy 11.9 Selection of the Most Optimal Model 11.10 Exercises 11.11 Ethics: Bias in Algorithmic Decision Making", " Chapter 11 Supervised Learning To understand supervised learning intuitively, we can compare it to how humans learn from examples. Consider a doctor diagnosing patients. The doctor has trained for years using textbooks and case studies where the symptoms (inputs) and the correct diagnosis (output) were known. During their residency, they test their knowledge under the supervision of experienced mentors. Finally, once licensed, they apply this knowledge to diagnose new patients where the outcome is unknown. This is an example of supervised learning. This process mirrors supervised learning. First, the model undergoes a Training phase, learning from labeled data (like symptoms and their corresponding diagnoses). Next, during Testing, it is evaluated on new, unseen cases where the outcome is already known to verify its accuracy. finally, the model enters the Prediction phase, where it applies its learned knowledge to real-world data to generate diagnoses (predictions) for unknown cases. In machine learning terms: In machine learning terms, we have Inputs (features or predictors) which are the data points used to make a prediction, and Outputs (target or response), which are the values we aim to predict. The ultimate Goal is to learn the mathematical relationship between these inputs and outputs to accurately forecast outcomes for future data. [!TIP] Key Terminology Key Terminology In supervised learning, the Input (or Feature/Independent Variable) refers to the data used to make a prediction, such as patient symptoms or house characteristics. The Output (or Target/Dependent Variable) is the value we want to predict, like a medical diagnosis or a house price. We rely on Labels, which are the known “answers” in our training data, to teach the model. Common applications include: Common applications of these techniques include Spam Detection (classifying emails as “Spam” or “Not Spam”), Credit Scoring (predicting the likelihood of a customer repaying a loan), and House Price Prediction (estimating a property’s value based on its location and size). 11.1 Classification and Regression We divide supervised learning into two main types based on the target variable: We divide supervised learning into two main types based on the nature of the target variable. Classification is used when the target is a discrete category, such as determining if an email is spam (Yes/No) or predicting which product a customer will buy (A, B, or C). Regression, on the other hand, is used when the target is a continuous number, like estimating the price of a house or forecasting the number of units to be sold next month. In the following sections, we will learn algorithms for both tasks. 11.2 kNN: k-Nearest Neighbors Let’s start with a simple but very useful classification algorithm, the k-Nearest Neighbors algorithm (kNN). 11.2.1 Two variables as input Let’s start by understanding it visually. Imagine that we have two variables as input and as output it gives us whether it is Red Class or Blue Class. This data is our training data. [!NOTE] When to use kNN? kNN is excellent for small datasets with few dimensions (variables) because it is simple and explains non-linear patterns well. However, it becomes very slow and less accurate as the dataset grows in size or number of variables (the “curse of dimensionality”). Now that we have our training data, we will start using the test data. As we want to predict the class, the output, we will see how one of these data points would look visually and paint it yellow. Next, we calculate the distance between this point and the other data points. We have traced only some distances, but we could do it with all of them. For this example, we will take the k = 3 nearest neighbors. Why 3? It is common to pick an odd number to avoid ties (where the vote is 50/50). We notice that if we focus only on the 3 nearest neighbors, there are more reds than blues, so our prediction will be that this point must be Class R (red). Calculating the distance on a Cartesian plane is relatively simple, we only have variables as input: on the x-axis and y-axis. However, the same logic can be taken to more variables. 11.2.2 Multiple variables as input Let’s see how it would be with 4 variables as input. We are going to work again with the iris data frame, which, as we will recall, has 4 attributes of a plant and the last column is the species to which it belongs. data(iris) iris |&gt; head(10) #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; 1 5.1 3.5 1.4 0.2 setosa #&gt; 2 4.9 3.0 1.4 0.2 setosa #&gt; 3 4.7 3.2 1.3 0.2 setosa #&gt; 4 4.6 3.1 1.5 0.2 setosa #&gt; 5 5.0 3.6 1.4 0.2 setosa #&gt; 6 5.4 3.9 1.7 0.4 setosa #&gt; 7 4.6 3.4 1.4 0.3 setosa #&gt; 8 5.0 3.4 1.5 0.2 setosa #&gt; 9 4.4 2.9 1.4 0.2 setosa #&gt; 10 4.9 3.1 1.5 0.1 setosa The idea is as follows, we will take training data, 50 data points. From this data, we have the 4 input attributes and the last column is the output, the species. We will use the kNN algorithm taking this training data as input to create our model. Then, with testing data, another 50 data points, we will test our model. Let’s start by taking a random sample of 100 records and separate half for training and half for testing. Since we have 150 data points in our data frame, let’s take a sample of the indices. In this case, we are going to use the set.seed(n) function to force the random sample values to be the same always. Thus, we can all obtain the same results and the explanation in the book in these chapters is consistent with the results that each reader obtains. For a real exercise, we should not include that line. It is recommended to read the documentation ?set.seed(). # 28 is the author&#39;s birthday set.seed(28) sample_idx &lt;- sample(150, 100) train_idx &lt;- sample(sample_idx, 50) test_idx &lt;- sample_idx[!sample_idx %in% train_idx] Now that we have the indices we can build our training data and our test. iris_train &lt;- iris[train_idx, ] iris_test &lt;- iris[test_idx, ] iris_train_input &lt;- iris_train[, -5] iris_train_output &lt;- iris_train[, 5] iris_test_input &lt;- iris_test[, -5] iris_test_output &lt;- iris_test[, 5] Although we could build the algorithms to calculate the minimum distances for each point, R provides us with libraries that facilitate the creation of these models. To do this, we will load the class library, which will allow us to execute kNN quickly. install.packages(&quot;class&quot;) library(class) This library provides us with the knn() function, which will take the training data to create the model and once the model is created it will take the test data to predict the output for our test data. iris_test_output_kNN &lt;- knn(train = iris_train_input, cl = iris_train_output, test = iris_test_input, k = 3) iris_test_output_kNN #&gt; [1] versicolor versicolor versicolor versicolor setosa versicolor #&gt; [7] virginica virginica virginica virginica versicolor versicolor #&gt; [13] virginica versicolor versicolor versicolor setosa versicolor #&gt; [19] versicolor virginica virginica setosa versicolor versicolor #&gt; [25] versicolor virginica setosa setosa versicolor versicolor #&gt; [31] virginica setosa setosa virginica virginica setosa #&gt; [37] setosa virginica setosa versicolor setosa virginica #&gt; [43] setosa setosa setosa virginica virginica versicolor #&gt; [49] virginica versicolor #&gt; Levels: setosa versicolor virginica Thus, the knn function throws us the prediction just by entering the training data as attributes, the test inputs, and how many nearest neighbors it will look for (k). And not only that, we can compare our prediction with the test output to see how accurate (accuracy) our model is. To do this, we calculate the percentage of correct predictions regarding the test output. mean(iris_test_output_kNN == iris_test_output) #&gt; [1] 0.94 In addition, we can place a summary in a table, also known as a confusion matrix, to see how many predicted values were equal to the real ones using the table() function. table(iris_test_output_kNN, iris_test_output) #&gt; iris_test_output #&gt; iris_test_output_kNN setosa versicolor virginica #&gt; setosa 14 0 0 #&gt; versicolor 0 18 2 #&gt; virginica 0 1 15 Let’s interpret this result cell by cell: Our kNN model predicted 14 values as species “setosa” and it turns out that in our test the real value, output, was also setosa. Our model predicted 20 as species versicolor. However, in the real-test data, of those 20, only 18 are versicolor and 2 are virginica. Our model predicted 16 as species virginica. However, in the real-test data, of those 16, only 15 are virginica. 11.2.3 Diverse values of k So far we have only used a single value for k, 3 nearest neighbors. However, we could see the accuracy for different values of k. Since we have 50 values in our training data, we will see the hits taking a maximum of 50 nearest neighbors. k &lt;- 1:50 result_df &lt;- data.frame(k, precision = 0) for(n in k){ iris_test_output_kNN &lt;- knn(train = iris_train_input, cl = iris_train_output, test = iris_test_input, k = n) result_df$precision[n] &lt;- mean(iris_test_output_kNN == iris_test_output) } result_df |&gt; ggplot() + aes(k, precision) + geom_line() As we can see, the accuracy (precision) changes with k. This illustrates a fundamental concept in Machine Learning: Overfitting (Low k): When k is too low (e.g., k=1), the model pays excessive attention to individual data points, including noise. It effectively “memorizes” the training data but fails to generalize to new, unseen examples. Underfitting (High k): Conversely, if k is too high (e.g., k=50), the model becomes overly simple, averaging out the signal and missing the distinct patterns that differentiate the classes. Finding the sweet spot between these two extremes is the goal of Hyperparameter Tuning. It will depend on each case to choose the best “k” for our model to balance this trade-off (often called the Bias-Variance Tradeoff). We have thus built our first machine learning model. 11.3 tidymodels Framework Now that we have created our first machine learning model, we have seen ourselves with many lines of code. For example, to split the sample into training and test, to calculate the optimal “k”, etc. To make the work easier, we will use the tidymodels framework. tidymodels10 is a collection of packages for modeling and machine learning using tidyverse principles. It provides a unified, modern interface for: It provides a unified, modern interface via a suite of specialized packages: rsample for data splitting and resampling, recipes for feature engineering and preprocessing, parsnip for specifying models, tune for hyperparameter optimization, yardstick for metrics and model evaluation, and workflows to bundle everything together. install.packages(&#39;tidymodels&#39;) library(tidymodels) #&gt; ── Attaching packages ─────────── tidymodels 1.4.1 ── #&gt; ✔ broom 1.0.11 ✔ tailor 0.1.0 #&gt; ✔ dials 1.4.2 ✔ tune 2.0.1 #&gt; ✔ infer 1.1.0 ✔ workflows 1.3.0 #&gt; ✔ parsnip 1.4.0 ✔ workflowsets 1.1.1 #&gt; ✔ recipes 1.3.1 ✔ yardstick 1.3.2 #&gt; ✔ rsample 1.3.1 #&gt; ── Conflicts ────────────── tidymodels_conflicts() ── #&gt; ✖ NLP::annotate() masks ggplot2::annotate() #&gt; ✖ scales::discard() masks purrr::discard() #&gt; ✖ Matrix::expand() masks tidyr::expand() #&gt; ✖ dplyr::filter() masks stats::filter() #&gt; ✖ recipes::fixed() masks stringr::fixed() #&gt; ✖ dplyr::lag() masks stats::lag() #&gt; ✖ Matrix::pack() masks tidyr::pack() #&gt; ✖ rsample::permutations() masks gtools::permutations() #&gt; ✖ dials::prune() masks dendextend::prune() #&gt; ✖ yardstick::spec() masks readr::spec() #&gt; ✖ recipes::step() masks stats::step() #&gt; ✖ Matrix::unpack() masks tidyr::unpack() #&gt; ✖ recipes::update() masks Matrix::update(), stats::update() We are going to do another example with k-nearest neighbors, but this time using the functions of the tidymodels framework. The data for this example will be obtained from the ISLR library, which contains the daily percentage returns for the S&amp;P 500 stock index between 2001 and 2005. This data frame has 8 columns that we will use as input and the last column that has two classes (whether the index goes up or down) that we will use as output (See ?Smarket). install.packages(&quot;ISLR&quot;) library(ISLR) data(Smarket) # Data frame that we will use Smarket |&gt; head(10) #&gt; Year Lag1 Lag2 Lag3 Lag4 Lag5 Volume Today Direction #&gt; 1 2001 0.381 -0.192 -2.624 -1.055 5.010 1.1913 0.959 Up #&gt; 2 2001 0.959 0.381 -0.192 -2.624 -1.055 1.2965 1.032 Up #&gt; 3 2001 1.032 0.959 0.381 -0.192 -2.624 1.4112 -0.623 Down #&gt; 4 2001 -0.623 1.032 0.959 0.381 -0.192 1.2760 0.614 Up #&gt; 5 2001 0.614 -0.623 1.032 0.959 0.381 1.2057 0.213 Up #&gt; 6 2001 0.213 0.614 -0.623 1.032 0.959 1.3491 1.392 Up #&gt; 7 2001 1.392 0.213 0.614 -0.623 1.032 1.4450 -0.403 Down #&gt; 8 2001 -0.403 1.392 0.213 0.614 -0.623 1.4078 0.027 Up #&gt; 9 2001 0.027 -0.403 1.392 0.213 0.614 1.1640 1.303 Up #&gt; 10 2001 1.303 0.027 -0.403 1.392 0.213 1.2326 0.287 Up # We make some translations for ease of analysis Smarket &lt;- Smarket |&gt; rename(Direction = Direction) |&gt; mutate(Direction = ifelse(Direction == &quot;Up&quot;, &quot;Up&quot;, &quot;Down&quot;)) |&gt; mutate(across(c(&quot;Direction&quot;), ~as.factor(.))) 11.3.1 Creation of training and test data From the total of our data frame, we will split a part of the data for training and the other to do the tests. tidymodels provides the initial_split() function from the rsample package which creates a clean split object. We allocate 75% of the data for training using the prop argument, and we can use strata to ensure balanced class distribution. set.seed(28) data_split &lt;- initial_split(Smarket, prop = 0.75, strata = Direction) SP_train &lt;- training(data_split) SP_test &lt;- testing(data_split) # Check the split nrow(SP_train) #&gt; [1] 937 nrow(SP_test) #&gt; [1] 313 This function makes sampling data much simpler and returns a split object that we can use with training() and testing() accessor functions. 11.3.2 Training our prediction algorithm In tidymodels, we build models in a structured way using three key components: In tidymodels, we build models in a structured way using three key components: Model Specification (via parsnip) to define the algorithm, Recipes (via recipes) to define preprocessing steps, and Workflows (via workflows) to bundle the model and recipe together into a single execution unit. Let’s break down these components: Let’s break down these components. The Model Specification (parsnip) tells R what kind of model we want (e.g., “nearest neighbor”) and which computational engine to use (e.g., “kknn”), decoupling intent from implementation. The Recipe (recipes) acts as a blueprint for data processing, handling tasks like normalization (scaling variables) and converting categorical variables. Finally, the Workflow (workflows) container holds the model and recipe together, ensuring that the exact same preprocessing steps are applied automatically when predicting on new data. Let’s start by specifying our k-nearest neighbors model. We use tune() as a placeholder for the neighbors parameter to indicate we want to find the optimal value. # Model specification knn_spec &lt;- nearest_neighbor(neighbors = tune()) |&gt; set_engine(&quot;kknn&quot;) |&gt; set_mode(&quot;classification&quot;) knn_spec #&gt; K-Nearest Neighbor Model Specification (classification) #&gt; #&gt; Main Arguments: #&gt; neighbors = tune() #&gt; #&gt; Computational engine: kknn 11.3.3 Data Pre-processing with Recipes tidymodels uses recipes for preprocessing. The scale method (division by standard deviation) and centering (subtraction of the mean) are implemented with step_normalize(). # Define preprocessing recipe knn_recipe &lt;- recipe(Direction ~ ., data = SP_train) |&gt; step_normalize(all_numeric_predictors()) knn_recipe #&gt; #&gt; ── Recipe ─────────────────────────────────────────── #&gt; #&gt; ── Inputs #&gt; Number of variables by role #&gt; outcome: 1 #&gt; predictor: 8 #&gt; #&gt; ── Operations #&gt; • Centering and scaling for: #&gt; all_numeric_predictors() 11.3.4 Creating a Workflow A workflow bundles the recipe and model specification together for easy training and prediction. # Bundle into workflow knn_workflow &lt;- workflow() |&gt; add_recipe(knn_recipe) |&gt; add_model(knn_spec) knn_workflow #&gt; ══ Workflow ═════════════════════════════════════════ #&gt; Preprocessor: Recipe #&gt; Model: nearest_neighbor() #&gt; #&gt; ── Preprocessor ───────────────────────────────────── #&gt; 1 Recipe Step #&gt; #&gt; • step_normalize() #&gt; #&gt; ── Model ──────────────────────────────────────────── #&gt; K-Nearest Neighbor Model Specification (classification) #&gt; #&gt; Main Arguments: #&gt; neighbors = tune() #&gt; #&gt; Computational engine: kknn 11.3.5 Parameter Tuning with Cross-Validation One of the most important parts of training machine learning models is tuning the parameters. We use vfold_cv() to create cross-validation folds and tune_grid() to search for the best hyperparameters. set.seed(28) # Create 5-fold cross-validation folds &lt;- vfold_cv(SP_train, v = 5, strata = Direction) # Create a grid of k values to try k_grid &lt;- grid_regular(neighbors(range = c(1, 50)), levels = 20) # Tune the model knn_tune_results &lt;- tune_grid( knn_workflow, resamples = folds, grid = k_grid ) knn_tune_results #&gt; # Tuning results #&gt; # 5-fold cross-validation using stratification #&gt; # A tibble: 5 × 4 #&gt; splits id .metrics .notes #&gt; &lt;list&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 &lt;split [748/189]&gt; Fold1 &lt;tibble [60 × 5]&gt; &lt;tibble [0 × 4]&gt; #&gt; 2 &lt;split [750/187]&gt; Fold2 &lt;tibble [60 × 5]&gt; &lt;tibble [0 × 4]&gt; #&gt; 3 &lt;split [750/187]&gt; Fold3 &lt;tibble [60 × 5]&gt; &lt;tibble [0 × 4]&gt; #&gt; 4 &lt;split [750/187]&gt; Fold4 &lt;tibble [60 × 5]&gt; &lt;tibble [0 × 4]&gt; #&gt; 5 &lt;split [750/187]&gt; Fold5 &lt;tibble [60 × 5]&gt; &lt;tibble [0 × 4]&gt; We can visualize the tuning results using autoplot(): autoplot(knn_tune_results) We can see the accuracy for each value of “k”. The show_best() function shows us the top performing values: show_best(knn_tune_results, metric = &quot;accuracy&quot;) #&gt; # A tibble: 5 × 7 #&gt; neighbors .metric .estimator mean n std_err .config #&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 44 accuracy binary 0.905 5 0.0114 pre0_mod18_post0 #&gt; 2 42 accuracy binary 0.904 5 0.0116 pre0_mod17_post0 #&gt; 3 37 accuracy binary 0.902 5 0.0115 pre0_mod15_post0 #&gt; 4 31 accuracy binary 0.902 5 0.0112 pre0_mod13_post0 #&gt; 5 47 accuracy binary 0.901 5 0.0119 pre0_mod19_post0 11.3.6 Finalizing the Model Once we’ve found the best hyperparameters, we finalize our workflow with those values: # Select the best k value best_k &lt;- select_best(knn_tune_results, metric = &quot;accuracy&quot;) best_k #&gt; # A tibble: 1 × 2 #&gt; neighbors .config #&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 44 pre0_mod18_post0 # Finalize the workflow with the best parameters final_knn_workflow &lt;- finalize_workflow(knn_workflow, best_k) # Fit the final model on the entire training set SP_knn_trained &lt;- fit(final_knn_workflow, data = SP_train) SP_knn_trained #&gt; ══ Workflow [trained] ═══════════════════════════════ #&gt; Preprocessor: Recipe #&gt; Model: nearest_neighbor() #&gt; #&gt; ── Preprocessor ───────────────────────────────────── #&gt; 1 Recipe Step #&gt; #&gt; • step_normalize() #&gt; #&gt; ── Model ──────────────────────────────────────────── #&gt; #&gt; Call: #&gt; kknn::train.kknn(formula = ..y ~ ., data = data, ks = min_rows(44L, data, 5)) #&gt; #&gt; Type of response variable: nominal #&gt; Minimal misclassification: 0.0864461 #&gt; Best kernel: optimal #&gt; Best k: 44 We see the substantial improvement now that we have adjusted some parameters and made it reprocess first. Note that each time we adjust parameters, the value of “k” can change until the most optimal one is found. In this case, it changed to k = 29. This does not mean that the lower the “k”, the better the algorithm, only that it is the most optimal for this particular case with these adjustments made. 11.3.7 Testing the prediction model We already have our model trained and ready to test it. tidymodels makes it easy to make predictions using the augment() function which adds predictions directly to our test data. # Make predictions on test data SP_predictions &lt;- augment(SP_knn_trained, new_data = SP_test) # View predictions SP_predictions |&gt; select(Direction, .pred_class, .pred_Down, .pred_Up) |&gt; head(10) #&gt; # A tibble: 10 × 4 #&gt; Direction .pred_class .pred_Down .pred_Up #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Up Up 0.243 0.757 #&gt; 2 Up Up 0.404 0.596 #&gt; 3 Down Down 0.611 0.389 #&gt; 4 Down Down 0.977 0.0231 #&gt; 5 Up Up 0.230 0.770 #&gt; 6 Down Up 0.472 0.528 #&gt; 7 Down Down 0.955 0.0447 #&gt; 8 Up Up 0.0361 0.964 #&gt; 9 Down Down 0.522 0.478 #&gt; 10 Down Down 1 0 The augment() function adds three columns: .pred_class (the predicted class), and probability columns for each class (.pred_Down and .pred_Up). This makes it very easy to compare predictions with actual values. As we can see, for each test value the model calculates the estimated probability for each class. The algorithm assigns the class with the highest probability. 11.3.8 Model Evaluation with yardstick To evaluate our model, we use the yardstick package. The conf_mat() function creates a confusion matrix, and we can calculate various metrics like accuracy, sensitivity, and specificity. # Confusion matrix SP_predictions |&gt; conf_mat(truth = Direction, estimate = .pred_class) #&gt; Truth #&gt; Prediction Down Up #&gt; Down 132 4 #&gt; Up 19 158 # Calculate accuracy SP_predictions |&gt; accuracy(truth = Direction, estimate = .pred_class) #&gt; # A tibble: 1 × 3 #&gt; .metric .estimator .estimate #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 accuracy binary 0.927 # Calculate multiple metrics at once SP_predictions |&gt; metrics(truth = Direction, estimate = .pred_class) #&gt; # A tibble: 2 × 3 #&gt; .metric .estimator .estimate #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 accuracy binary 0.927 #&gt; 2 kap binary 0.852 We obtain the accuracy as well as other metrics. The yardstick package provides many evaluation functions including sens() (sensitivity), spec() (specificity), precision(), recall(), and more. 11.4 Confusion Matrix We have already used confusion matrices in our two previous examples. Now it is our turn to properly understand its definition as well as some of the evaluation metrics of this matrix. A confusion matrix, also known as an error matrix, allows us to visualize the performance of an algorithm, generally a supervised learning one (in unsupervised learning it is generally called a matching matrix). Each row of the matrix represents the instances in a predicted class, while each column represents the instances in a real class (or vice versa). The name derives from the fact that it makes it easy to see if the system confuses two classes (i.e., commonly mislabeling one as another). Binary classifications, when the outcome can take only two classes, yield this following confusion matrix. 11.4.1 Accuracy We have already been using this term in our examples. The accuracy of the model can be calculated from the confusion matrix: \\(Accuracy=\\frac{TP+TN}{TP+TN+FP+FN}\\) The accuracy of the model is the proportion of times the algorithm predicted correctly, regarding the total data evaluated. 11.4.2 Sensitivity Sensitivity (also called true positive rate, recall, or probability of detection in some fields) measures the proportion of real positives that are correctly identified as such (for example, the percentage of sick people who are correctly identified as having the condition). \\(Sensitivity=\\frac{TP}{TP+FN}\\) 11.4.3 Specificity Specificity (also called true negative rate) measures the proportion of real negatives that are correctly identified as such (for example, the percentage of healthy people who are correctly identified as not having the condition). \\(Specificity=\\frac{TN}{TN+FP}\\) 11.5 Exercises Using the tidymodels library, partition the iris data frame in such a way as to have 70% training data and 30% test data. Solution iris_split &lt;- initial_split(iris, prop = 0.7, strata = Species) iris_train &lt;- training(iris_split) iris_test &lt;- testing(iris_split) Using tidymodels and the training data obtained in the previous exercise, create a k-nearest neighbor model with tuning. Plot the result. Solution # Model specification iris_knn_spec &lt;- nearest_neighbor(neighbors = tune()) |&gt; set_engine(&quot;kknn&quot;) |&gt; set_mode(&quot;classification&quot;) # Recipe with preprocessing iris_recipe &lt;- recipe(Species ~ ., data = iris_train) |&gt; step_normalize(all_numeric_predictors()) # Workflow iris_workflow &lt;- workflow() |&gt; add_recipe(iris_recipe) |&gt; add_model(iris_knn_spec) # Cross-validation and tuning iris_folds &lt;- vfold_cv(iris_train, v = 5) iris_tune &lt;- tune_grid(iris_workflow, resamples = iris_folds, grid = 20) autoplot(iris_tune) Use the model created in the previous exercise to predict the outputs of the test object. Report the confusion matrix. Solution # Finalize model with best k best_k &lt;- select_best(iris_tune, metric = &quot;accuracy&quot;) final_iris_wf &lt;- finalize_workflow(iris_workflow, best_k) iris_fit &lt;- fit(final_iris_wf, data = iris_train) # Predict and evaluate iris_predictions &lt;- augment(iris_fit, new_data = iris_test) iris_predictions |&gt; conf_mat(truth = Species, estimate = .pred_class) 11.6 Simple Linear Regression Now we have to predict on continuous variables, the supervision algorithms for these cases are called regression. To understand linear regression we are going to start with an example with a single variable as input, this is known as Simple Linear Regression. To do this we are going to use data from the HistData library where we will find a dataset that enumerates the individual observations of 934 children in 205 families stored in the object GaltonFamilies. install.packages(&quot;HistData&quot;) library(HistData) data(GaltonFamilies) # We make some filters to have one dad and one son per family heights_df &lt;- GaltonFamilies |&gt; filter(gender == &quot;male&quot;) |&gt; group_by(family) |&gt; slice_sample(n = 1) |&gt; # random sample of 1 son per family ungroup() |&gt; select(father, childHeight) |&gt; rename(son = childHeight) |&gt; mutate(father = father/39.37) |&gt; # From inches to meters mutate(son = son/39.37) # From inches to meters Visually we could see if there is a relationship between the heights of dad and son: heights_df |&gt; ggplot() + aes(father, son) + geom_point() + geom_abline(lty = 2) As we can see, there is a positive correlation, such that the taller the father, the son grows to be taller as an adult. This line, however, is nothing more than a default line. The challenge lies in finding which line minimizes the distance of the points to this line, known as error minimization. We could try to predict the height the son will have from the father’s height using the equation of this line: \\(Y = \\beta_0+\\beta_1X\\) Where \\(X\\) is an independent, explanatory variable, in this case the dad’s height. \\(\\beta_1\\) is a parameter that measures the influence that the explanatory variable has on the dependent variable \\(Y\\) and \\(\\beta_0\\) is the intercept or constant term. In our case, the son’s height. In statistics, linear regression or linear adjustment is a mathematical model used to approximate the dependency relationship between a dependent variable \\(Y\\) and the independent variables \\(X_i\\). Thus, our problem boils down to training our model to find the values of the intercept, \\(\\beta_0\\), and the value of the parameter accompanying \\(X_1\\), \\(\\beta_1\\), to then use these data as prediction in our test data. heights_split &lt;- initial_split(heights_df, prop = 0.5) heights_train &lt;- training(heights_split) heights_test &lt;- testing(heights_split) Now that we have our data we can train our model using tidymodels. We specify a linear regression model with linear_reg(). # Model specification lm_spec &lt;- linear_reg() |&gt; set_engine(&quot;lm&quot;) |&gt; set_mode(&quot;regression&quot;) # Recipe lm_recipe &lt;- recipe(son ~ father, data = heights_train) # Workflow lm_workflow &lt;- workflow() |&gt; add_recipe(lm_recipe) |&gt; add_model(lm_spec) # Cross-validation heights_folds &lt;- vfold_cv(heights_train, v = 10) lm_results &lt;- fit_resamples(lm_workflow, resamples = heights_folds) # View results collect_metrics(lm_results) #&gt; # A tibble: 2 × 6 #&gt; .metric .estimator mean n std_err .config #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 rmse standard 0.0584 10 0.00362 pre0_mod0_post0 #&gt; 2 rsq standard 0.362 10 0.0821 pre0_mod0_post0 We see as main results the RMSE, which stands for root mean square error, and is the value that linear regression seeks to minimize. In addition, we have the R squared or \\(R^2\\), which is the coefficient of determination which determines the quality of the model to replicate the results. The higher and closer to 1, the better the quality of the model. [!WARNING] Correlation implies association, not causation: A high \\(R^2\\) or strong correlation means the variables move together, but it does not prove that one causes the other. There could be confounding variables at play. Now let’s fit the final model and make predictions: # Fit final model heights_fit &lt;- fit(lm_workflow, data = heights_train) # Make predictions heights_predictions &lt;- augment(heights_fit, new_data = heights_test) # Calculate RMSE heights_predictions |&gt; rmse(truth = son, estimate = .pred) #&gt; # A tibble: 1 × 3 #&gt; .metric .estimator .estimate #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 rmse standard 0.0642 [!TIP] Model Diagnostics: In a rigorous analysis, you should also inspect the residuals (the difference between predicted and actual values). The augment() function includes a .resid column for this purpose. Plotting residuals helps verify that your model isn’t missing non-linear patterns. If we wish we can also report the coefficients of the equation and visualize them: \\(Y = \\beta_0+\\beta_1X\\) # Extract model coefficients heights_fit |&gt; extract_fit_parsnip() |&gt; tidy() #&gt; # A tibble: 2 × 5 #&gt; term estimate std.error statistic p.value #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 (Intercept) 0.644 0.183 3.51 0.000708 #&gt; 2 father 0.631 0.104 6.05 0.0000000361 model_coefs &lt;- heights_fit |&gt; extract_fit_parsnip() |&gt; tidy() intercept_val &lt;- model_coefs$estimate[1] slope_val &lt;- model_coefs$estimate[2] #Visualization heights_df |&gt; ggplot() + aes(father, son) + geom_point() + geom_abline(lty = 2, intercept = intercept_val, slope = slope_val, color = &quot;red&quot;) 11.7 Multiple Linear Regression Now that we know linear regression we can execute a multiple linear regression model, which involves more than 1 variable as input. To do this, we will use the diamonds dataset containing the prices and other attributes of almost 54,000 diamonds. library(ggplot2) data(&quot;diamonds&quot;) diamonds &lt;- diamonds |&gt; rename(price = price) diamonds |&gt; head(10) #&gt; # A tibble: 10 × 10 #&gt; carat cut color clarity depth table price x y z #&gt; &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 #&gt; 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 #&gt; 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 #&gt; 4 0.29 Premium I VS2 62.4 58 334 4.2 4.23 2.63 #&gt; 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 #&gt; 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 #&gt; 7 0.24 Very Good I VVS1 62.3 57 336 3.95 3.98 2.47 #&gt; 8 0.26 Very Good H SI1 61.9 55 337 4.07 4.11 2.53 #&gt; 9 0.22 Fair E VS2 65.1 61 337 3.87 3.78 2.49 #&gt; 10 0.23 Very Good H VS1 59.4 61 338 4 4.05 2.39 We split the data in two taking 70% of data for training: set.seed(28) diamonds_split &lt;- initial_split(diamonds, prop = 0.7, strata = price) diamonds_train &lt;- training(diamonds_split) diamonds_test &lt;- testing(diamonds_split) We now create our multiple linear regression model and report both the error results and the coefficients of the linear equation using a tidymodels workflow. # Model specification diamonds_spec &lt;- linear_reg() |&gt; set_engine(&quot;lm&quot;) |&gt; set_mode(&quot;regression&quot;) # Recipe diamonds_recipe &lt;- recipe(price ~ ., data = diamonds_train) # Workflow diamonds_workflow &lt;- workflow() |&gt; add_recipe(diamonds_recipe) |&gt; add_model(diamonds_spec) # Cross-validation diamonds_folds &lt;- vfold_cv(diamonds_train, v = 10) diamonds_results &lt;- fit_resamples(diamonds_workflow, resamples = diamonds_folds) collect_metrics(diamonds_results) #&gt; # A tibble: 2 × 6 #&gt; .metric .estimator mean n std_err .config #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 rmse standard 1136. 10 19.7 pre0_mod0_post0 #&gt; 2 rsq standard 0.919 10 0.00256 pre0_mod0_post0 We see that it gives us the RMSE and an R squared quite closer to 1, which denotes a high quality of the model to replicate the results. Let’s use our model to predict the prices of the test data. # Fit final model diamonds_fit &lt;- fit(diamonds_workflow, data = diamonds_train) # Extract coefficients diamonds_fit |&gt; extract_fit_parsnip() |&gt; tidy() #&gt; # A tibble: 24 × 5 #&gt; term estimate std.error statistic p.value #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 (Intercept) 6975. 473. 14.7 5.45e- 49 #&gt; 2 carat 11437. 60.7 188. 0 #&gt; 3 cut.L 571. 27.1 21.1 7.51e- 98 #&gt; 4 cut.Q -305. 21.7 -14.0 1.07e- 44 #&gt; 5 cut.C 139. 18.6 7.48 7.72e- 14 #&gt; 6 cut^4 -23.2 14.8 -1.56 1.18e- 1 #&gt; 7 color.L -1980. 20.8 -95.1 0 #&gt; 8 color.Q -685. 19.0 -36.1 6.69e-281 #&gt; 9 color.C -186. 17.7 -10.5 6.23e- 26 #&gt; 10 color^4 36.8 16.2 2.27 2.33e- 2 #&gt; # ℹ 14 more rows # Prediction and Error calculation diamonds_predictions &lt;- augment(diamonds_fit, new_data = diamonds_test) # Mean Squared Error Calculation RMSE: diamonds_predictions |&gt; rmse(truth = price, estimate = .pred) #&gt; # A tibble: 1 × 3 #&gt; .metric .estimator .estimate #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 rmse standard 1119. Thus, we have learned to perform one more machine learning model: linear regression, both simple and multiple. 11.8 Standard Method for Evaluating Accuracy Now that we know how to build models we will apply metrics that allow us better accuracy in classification models for two classes. To do this let’s recall the results of the model we created using the k-nearest neighbors algorithm to predict if the S&amp;P index goes up or down. SP_knn_trained #&gt; ══ Workflow [trained] ═══════════════════════════════ #&gt; Preprocessor: Recipe #&gt; Model: nearest_neighbor() #&gt; #&gt; ── Preprocessor ───────────────────────────────────── #&gt; 1 Recipe Step #&gt; #&gt; • step_normalize() #&gt; #&gt; ── Model ──────────────────────────────────────────── #&gt; #&gt; Call: #&gt; kknn::train.kknn(formula = ..y ~ ., data = data, ks = min_rows(44L, data, 5)) #&gt; #&gt; Type of response variable: nominal #&gt; Minimal misclassification: 0.0864461 #&gt; Best kernel: optimal #&gt; Best k: 44 In the penultimate line it can be read that accuracy (accuracy) was used to select the most optimal model using the largest value. However, this is not the only way to determine which is the most optimal model. Let’s remember how accuracy (accuracy) is calculated by default, we have used the simple rule that if the probability of it being of a certain class is more than 50% then that class is assigned and then we calculate the proportion of hits among the total cases. However, it doesn’t have to be 50%, we could be more demanding and indicate that if the probability is greater than 60% or 80% then a certain class is assigned. We see that there are different probabilities and that would give us different accuracy. This is how the area under the Receiver Operating Characteristic curve indicator arises, ROC (Fawcett 2005). This indicator measures how well a model can distinguish between two classes and is considered the standard method for evaluating the accuracy of predictive distribution models (Jorge M. Lobo 2007) and calculates accuracies not only for when we discriminate starting from 50%, but for more probability values. To use this metric we will modify our control parameters adding three attributes that will allow calculating the ROC. SP2_ctrl &lt;- metric_set(roc_auc, accuracy) # We define folds set.seed(28) SP2_folds &lt;- vfold_cv(SP_train, v = 5, strata = Direction) With these modified parameters we will proceed to re-train our model selecting by ROC AUC. set.seed(28) # Tune grid specifying ROC as the metric to optimize SP2_knn_res &lt;- tune_grid( knn_workflow, resamples = SP2_folds, grid = 20, metrics = SP2_ctrl ) show_best(SP2_knn_res, metric = &quot;roc_auc&quot;) #&gt; # A tibble: 5 × 7 #&gt; neighbors .metric .estimator mean n std_err .config #&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 15 roc_auc binary 0.965 5 0.00586 pre0_mod13_post0 #&gt; 2 13 roc_auc binary 0.962 5 0.00632 pre0_mod12_post0 #&gt; 3 12 roc_auc binary 0.960 5 0.00667 pre0_mod11_post0 #&gt; 4 11 roc_auc binary 0.957 5 0.00717 pre0_mod10_post0 #&gt; 5 10 roc_auc binary 0.955 5 0.00739 pre0_mod09_post0 We see that now ROC was used to select the most optimal model. The closer the ROC value is to 1 the better our model will be. With this model we can predict values from the test data. # Select best k based on ROC best_k_roc &lt;- select_best(SP2_knn_res, metric = &quot;roc_auc&quot;) # Finalize workflow final_knn_roc &lt;- finalize_workflow(knn_workflow, best_k_roc) # Fit and predict SP2_knn_fit &lt;- fit(final_knn_roc, data = SP_train) SP2_predictions &lt;- augment(SP2_knn_fit, new_data = SP_test) # Evaluate SP2_predictions |&gt; conf_mat(truth = Direction, estimate = .pred_class) #&gt; Truth #&gt; Prediction Down Up #&gt; Down 135 15 #&gt; Up 16 147 SP2_predictions |&gt; accuracy(truth = Direction, estimate = .pred_class) #&gt; # A tibble: 1 × 3 #&gt; .metric .estimator .estimate #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 accuracy binary 0.901 We see how our accuracy (accuracy) has increased from 91.99% to 93.27%. This metric is highly recommended to improve the accuracy of our model, in addition to allowing us to more easily use it as a comparator between different models we can create. 11.9 Selection of the Most Optimal Model We have learned how to create some machine learning models. As we must have noticed, with tidymodels we follow the same pattern for partitioning, training, and prediction. The variation lies in how to pre-process the data and the parameter tuning. We could thus create multiple models, but finally we have to verify one which will serve us to make our predictions. In this section, we are going to compare different predictive models accepting their default values and choose the best one using the tools presented in previous sections. To do this, we are going to use a new case. This time we are evaluating the behavior of our 5,000 clients, some of whom have unsubscribed from our services. We have 19 predictors, most of them numeric, in the mlc_churn dataset. To access the data we have to load the modeldata library. install.packages(&quot;modeldata&quot;) library(modeldata) data(mlc_churn) str(mlc_churn) #&gt; tibble [5,000 × 20] (S3: tbl_df/tbl/data.frame) #&gt; $ state : Factor w/ 51 levels &quot;AK&quot;,&quot;AL&quot;,&quot;AR&quot;,..: 17 36 32 36 37 2 20 25 19 50 ... #&gt; $ account_length : int [1:5000] 128 107 137 84 75 118 121 147 117 141 ... #&gt; $ area_code : Factor w/ 3 levels &quot;area_code_408&quot;,..: 2 2 2 1 2 3 3 2 1 2 ... #&gt; $ international_plan : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 2 2 2 1 2 1 2 ... #&gt; $ voice_mail_plan : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 2 2 1 1 1 1 2 1 1 2 ... #&gt; $ number_vmail_messages : int [1:5000] 25 26 0 0 0 0 24 0 0 37 ... #&gt; $ total_day_minutes : num [1:5000] 265 162 243 299 167 ... #&gt; $ total_day_calls : int [1:5000] 110 123 114 71 113 98 88 79 97 84 ... #&gt; $ total_day_charge : num [1:5000] 45.1 27.5 41.4 50.9 28.3 ... #&gt; $ total_eve_minutes : num [1:5000] 197.4 195.5 121.2 61.9 148.3 ... #&gt; $ total_eve_calls : int [1:5000] 99 103 110 88 122 101 108 94 80 111 ... #&gt; $ total_eve_charge : num [1:5000] 16.78 16.62 10.3 5.26 12.61 ... #&gt; $ total_night_minutes : num [1:5000] 245 254 163 197 187 ... #&gt; $ total_night_calls : int [1:5000] 91 103 104 89 121 118 118 96 90 97 ... #&gt; $ total_night_charge : num [1:5000] 11.01 11.45 7.32 8.86 8.41 ... #&gt; $ total_intl_minutes : num [1:5000] 10 13.7 12.2 6.6 10.1 6.3 7.5 7.1 8.7 11.2 ... #&gt; $ total_intl_calls : int [1:5000] 3 3 5 7 3 6 7 6 4 5 ... #&gt; $ total_intl_charge : num [1:5000] 2.7 3.7 3.29 1.78 2.73 1.7 2.03 1.92 2.35 3.02 ... #&gt; $ number_customer_service_calls: int [1:5000] 1 1 0 2 3 0 3 0 1 0 ... #&gt; $ churn : Factor w/ 2 levels &quot;yes&quot;,&quot;no&quot;: 2 2 2 2 2 2 2 2 2 2 ... # We translate outputs mlc_churn &lt;- mlc_churn |&gt; rename(churn_status = churn) |&gt; mutate(churn_status = ifelse(churn_status == &quot;yes&quot;, &quot;Yes&quot;, &quot;No&quot;)) |&gt; mutate(churn_status = as.factor(churn_status)) # Proportion of &quot;Yes&quot; and &quot;No&quot;s: prop.table(table(mlc_churn$churn_status)) #&gt; #&gt; No Yes #&gt; 0.8586 0.1414 We create now sample of training and test, 70% training. set.seed(28) churn_split &lt;- initial_split(mlc_churn, prop = 0.7, strata = churn_status) churn_train &lt;- training(churn_split) churn_test &lt;- testing(churn_split) Up to here we have done exactly the same step as in previous models. However, previously we have specified the cross-validation method within our control parameters. Now we will create a shared validation set to compare all models fairly. We will create a list of 5 folds using the function vfold_cv() from rsample. set.seed(28) churn_folds &lt;- vfold_cv(churn_train, v = 5, strata = churn_status) churn_folds #&gt; # 5-fold cross-validation using stratification #&gt; # A tibble: 5 × 2 #&gt; splits id #&gt; &lt;list&gt; &lt;chr&gt; #&gt; 1 &lt;split [2799/700]&gt; Fold1 #&gt; 2 &lt;split [2799/700]&gt; Fold2 #&gt; 3 &lt;split [2799/700]&gt; Fold3 #&gt; 4 &lt;split [2799/700]&gt; Fold4 #&gt; 5 &lt;split [2800/699]&gt; Fold5 We will use the ROC metric for all models. In tidymodels, we define the metrics we want to calculate using a metric_set(). churn_metrics &lt;- metric_set(roc_auc, accuracy, sensitivity, specificity) The next step would be to choose the machine learning algorithms we want to use to create our models. parsnip provides a consistent interface for different models. We can check available engines for a model type, for example: show_engines(&quot;nearest_neighbor&quot;) #&gt; # A tibble: 2 × 2 #&gt; engine mode #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 kknn classification #&gt; 2 kknn regression We will create a series of models and compare them using ROC AUC. First, let’s define a common recipe for preprocessing. churn_recipe &lt;- recipe(churn_status ~ ., data = churn_train) |&gt; step_dummy(all_nominal_predictors(), -churn_status) |&gt; step_normalize(all_numeric_predictors()) 11.9.1 k-Nearest Neighbors Model Although it is a very simple model, it is also very useful. Let’s start with this model that we already learned to create during this chapter. # Spec knn_spec &lt;- nearest_neighbor(neighbors = tune()) |&gt; set_engine(&quot;kknn&quot;) |&gt; set_mode(&quot;classification&quot;) # Workflow knn_workflow &lt;- workflow() |&gt; add_recipe(churn_recipe) |&gt; add_model(knn_spec) # Tune knn_res &lt;- tune_grid( knn_workflow, resamples = churn_folds, grid = 10, metrics = churn_metrics ) show_best(knn_res, metric = &quot;roc_auc&quot;) #&gt; # A tibble: 5 × 7 #&gt; neighbors .metric .estimator mean n std_err .config #&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 15 roc_auc binary 0.684 5 0.0101 pre0_mod10_post0 #&gt; 2 13 roc_auc binary 0.679 5 0.00972 pre0_mod09_post0 #&gt; 3 11 roc_auc binary 0.676 5 0.0110 pre0_mod08_post0 #&gt; 4 10 roc_auc binary 0.674 5 0.0119 pre0_mod07_post0 #&gt; 5 8 roc_auc binary 0.665 5 0.0145 pre0_mod06_post0 11.9.2 Generalized Linear Model - GLM The generalized linear model (GLM) is a flexible generalization of ordinary linear regression. To do this we need to install the glmnet library before creating our model via tidymodels. install.packages(&quot;glmnet&quot;) # Spec glm_spec &lt;- logistic_reg(penalty = tune(), mixture = tune()) |&gt; set_engine(&quot;glmnet&quot;) |&gt; set_mode(&quot;classification&quot;) # Workflow glm_workflow &lt;- workflow() |&gt; add_recipe(churn_recipe) |&gt; add_model(glm_spec) # Tune glm_res &lt;- tune_grid( glm_workflow, resamples = churn_folds, grid = 10, metrics = churn_metrics ) show_best(glm_res, metric = &quot;roc_auc&quot;) #&gt; # A tibble: 5 × 8 #&gt; penalty mixture .metric .estimator mean n std_err .config #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 0.00599 1 roc_auc binary 0.819 5 0.00937 pre0_mod08_post0 #&gt; 2 0.0774 0.261 roc_auc binary 0.813 5 0.00862 pre0_mod09_post0 #&gt; 3 0.000464 0.578 roc_auc binary 0.808 5 0.0102 pre0_mod07_post0 #&gt; 4 0.00000278 0.894 roc_auc binary 0.807 5 0.0104 pre0_mod05_post0 #&gt; 5 0.00000000129 0.789 roc_auc binary 0.807 5 0.0104 pre0_mod02_post0 11.9.3 Random Forest Model Random Forest is a supervised machine learning technique based on decision trees. We will use the random forest model (RF). To do this we will first install the ranger library and then create the model via tidymodels. install.packages(&quot;ranger&quot;) # Spec rf_spec &lt;- rand_forest(mtry = tune(), trees = 1000, min_n = tune()) |&gt; set_engine(&quot;ranger&quot;) |&gt; set_mode(&quot;classification&quot;) # Workflow rf_workflow &lt;- workflow() |&gt; add_recipe(churn_recipe) |&gt; add_model(rf_spec) # Tune rf_res &lt;- tune_grid( rf_workflow, resamples = churn_folds, grid = 10, metrics = churn_metrics ) #&gt; i Creating pre-processing data to finalize 1 unknown parameter: &quot;mtry&quot; show_best(rf_res, metric = &quot;roc_auc&quot;) #&gt; # A tibble: 5 × 8 #&gt; mtry min_n .metric .estimator mean n std_err .config #&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 8 31 roc_auc binary 0.914 5 0.00998 pre0_mod02_post0 #&gt; 2 16 2 roc_auc binary 0.912 5 0.0104 pre0_mod03_post0 #&gt; 3 23 18 roc_auc binary 0.911 5 0.00975 pre0_mod04_post0 #&gt; 4 31 35 roc_auc binary 0.909 5 0.0101 pre0_mod05_post0 #&gt; 5 53 40 roc_auc binary 0.906 5 0.00890 pre0_mod08_post0 11.9.4 Support Vector Machine Model - SVM Support vector machines or support vector machines are a set of supervised learning algorithms. To create this model we will use the kernlab engine. install.packages(&quot;kernlab&quot;) # Spec svm_spec &lt;- svm_rbf(cost = tune(), rbf_sigma = tune()) |&gt; set_engine(&quot;kernlab&quot;) |&gt; set_mode(&quot;classification&quot;) # Workflow svm_workflow &lt;- workflow() |&gt; add_recipe(churn_recipe) |&gt; add_model(svm_spec) # Tune svm_res &lt;- tune_grid( svm_workflow, resamples = churn_folds, grid = 10, metrics = churn_metrics ) #&gt; maximum number of iterations reached 2.273272e-05 2.273273e-05maximum number of iterations reached 0.001260726 0.001226596maximum number of iterations reached 0.008990742 0.0089403maximum number of iterations reached 4.01911e-05 4.01911e-05maximum number of iterations reached 0.0004458751 0.0004390186maximum number of iterations reached 0.01426775 0.01386837maximum number of iterations reached 2.311619e-05 2.311619e-05maximum number of iterations reached 0.0004666225 0.0004600749maximum number of iterations reached 0.009561785 0.009488703maximum number of iterations reached 4.18965e-05 4.189651e-05maximum number of iterations reached 0.01467671 0.01418266maximum number of iterations reached 2.221129e-05 2.22113e-05maximum number of iterations reached 0.0009695224 0.0009465917maximum number of iterations reached 0.009269646 0.009208682maximum number of iterations reached 3.924845e-05 3.924845e-05maximum number of iterations reached 0.0002350936 0.0002328733maximum number of iterations reached 0.01304753 0.01272522maximum number of iterations reached 2.357812e-05 2.357812e-05maximum number of iterations reached 0.0003822001 0.000377737maximum number of iterations reached 0.009766638 0.009695014maximum number of iterations reached 4.284351e-05 4.284352e-05maximum number of iterations reached 0.01553881 0.01501075maximum number of iterations reached 2.310753e-05 2.310753e-05maximum number of iterations reached 0.0004798964 0.0004730654maximum number of iterations reached 0.009636723 0.009566509maximum number of iterations reached 4.147277e-05 4.147278e-05maximum number of iterations reached 0.0002249601 0.0002227933maximum number of iterations reached 0.01498293 0.01448009 show_best(svm_res, metric = &quot;roc_auc&quot;) #&gt; # A tibble: 5 × 8 #&gt; cost rbf_sigma .metric .estimator mean n std_err .config #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 0.00310 0.00599 roc_auc binary 0.864 5 0.00811 pre0_mod02_post0 #&gt; 2 32 0.000464 roc_auc binary 0.862 5 0.00900 pre0_mod10_post0 #&gt; 3 1 0.0000359 roc_auc binary 0.794 5 0.0103 pre0_mod07_post0 #&gt; 4 0.0312 0.00000278 roc_auc binary 0.793 5 0.0114 pre0_mod04_post0 #&gt; 5 0.000977 0.000000215 roc_auc binary 0.793 5 0.0114 pre0_mod01_post0 11.9.5 Naive Bayes Model Naïve Bayes (NB) is one of the simplest, yet powerful, algorithms for classification. It is based on Bayes’ Theorem, which describes the probability of an event, based on prior knowledge of conditions that might be related to the event. For example, if cancer is related to age, then, using Bayes’ theorem, a person’s age can be used to more accurately assess the probability that they have cancer. To use this model we will use the naivebayes library within tidymodels. install.packages(c(&quot;naivebayes&quot;, &quot;discrim&quot;)) library(naivebayes) library(discrim) # Required for parsnip integration # Spec library(discrim) #&gt; #&gt; Attaching package: &#39;discrim&#39; #&gt; The following object is masked from &#39;package:dials&#39;: #&gt; #&gt; smoothness nb_spec &lt;- naive_Bayes() |&gt; set_engine(&quot;naivebayes&quot;) |&gt; set_mode(&quot;classification&quot;) # Workflow nb_workflow &lt;- workflow() |&gt; add_recipe(churn_recipe) |&gt; add_model(nb_spec) # Tune nb_res &lt;- fit_resamples( nb_workflow, resamples = churn_folds, metrics = churn_metrics ) collect_metrics(nb_res) #&gt; # A tibble: 4 × 6 #&gt; .metric .estimator mean n std_err .config #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 accuracy binary 0.859 5 0.000246 pre0_mod0_post0 #&gt; 2 roc_auc binary 0.840 5 0.00983 pre0_mod0_post0 #&gt; 3 sensitivity binary 1 5 0 pre0_mod0_post0 #&gt; 4 specificity binary 0 5 0 pre0_mod0_post0 11.9.6 Model Comparison To compare the models, we can extract the metrics from each tuning result and visualize them. # Collect metrics knn_metrics &lt;- collect_metrics(knn_res) |&gt; mutate(model = &quot;kNN&quot;) glm_metrics &lt;- collect_metrics(glm_res) |&gt; mutate(model = &quot;GLM&quot;) rf_metrics &lt;- collect_metrics(rf_res) |&gt; mutate(model = &quot;RF&quot;) svm_metrics &lt;- collect_metrics(svm_res) |&gt; mutate(model = &quot;SVM&quot;) nb_metrics &lt;- collect_metrics(nb_res) |&gt; mutate(model = &quot;Naive Bayes&quot;) # Combine all_metrics &lt;- bind_rows(knn_metrics, glm_metrics, rf_metrics, svm_metrics, nb_metrics) # Visualize ROC AUC all_metrics |&gt; filter(.metric == &quot;roc_auc&quot;) |&gt; ggplot(aes(x = model, y = mean, fill = model)) + geom_col() + labs(y = &quot;ROC AUC&quot;, title = &quot;Model Comparison&quot;) + theme(legend.position = &quot;none&quot;) For this case the random forest model (RF) seems to be the best. This is not surprising given that this algorithm is related to its ability to cope with different input types and require little preprocessing. We can make our models better by pre-processing data and changing the ad-hoc parameters of each model. 11.9.7 Predicting using the best model Now that we have our best model (Random Forest), we proceed to perform the prediction on the test set. We need to finalize the workflow with the best hyperparameters from the tuning step first. # Select best parameters for RF best_rf &lt;- select_best(rf_res, metric = &quot;roc_auc&quot;) # Finalize workflow final_rf_workflow &lt;- finalize_workflow(rf_workflow, best_rf) # Fit on training data optimal_model &lt;- fit(final_rf_workflow, data = churn_train) # Predict on test data churn_predictions &lt;- augment(optimal_model, new_data = churn_test) # Evaluate results churn_predictions |&gt; conf_mat(truth = churn_status, estimate = .pred_class) #&gt; Truth #&gt; Prediction No Yes #&gt; No 1285 66 #&gt; Yes 3 147 churn_predictions |&gt; accuracy(truth = churn_status, estimate = .pred_class) #&gt; # A tibble: 1 × 3 #&gt; .metric .estimator .estimate #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 accuracy binary 0.954 Thus, we have found how to create a customer churn prediction model given 19 prediction variables with an accuracy of 96%. 11.10 Exercises The attrition data frame from the modeldata library shows data from a list of almost 1,500 employees of a company. Create a copy of this data frame and store it in the workers object. Then, build an RF model with this data to predict the Attrition field (job desertion). Where the class “Yes” means they resigned and “No” means they still work. Solution data(attrition) str(attrition) workers &lt;- attrition workers &lt;- workers |&gt; rename(attrition_status = Attrition) # 70% for the training data set.seed(28) workers_split &lt;- initial_split(workers, prop = 0.7, strata = attrition_status) workers_train &lt;- training(workers_split) workers_test &lt;- testing(workers_split) # Recipe workers_recipe &lt;- recipe(attrition_status ~ ., data = workers_train) |&gt; step_dummy(all_nominal_predictors(), -attrition_status) |&gt; step_normalize(all_numeric_predictors()) # We create CV folds workers_folds &lt;- vfold_cv(workers_train, v = 5, strata = attrition_status) workers_metrics &lt;- metric_set(roc_auc, accuracy) # We create the model rf_spec &lt;- rand_forest(trees = 1000) |&gt; set_engine(&quot;ranger&quot;) |&gt; set_mode(&quot;classification&quot;) rf_wf &lt;- workflow() |&gt; add_recipe(workers_recipe) |&gt; add_model(rf_spec) workers_rf_res &lt;- fit_resamples( rf_wf, resamples = workers_folds, metrics = workers_metrics ) collect_metrics(workers_rf_res) Using the training data from the previous exercise, build the GLM model using tidymodels. Solution # Spec glm_spec &lt;- logistic_reg(penalty = tune(), mixture = tune()) |&gt; set_engine(&quot;glmnet&quot;) |&gt; set_mode(&quot;classification&quot;) # Workflow glm_wf &lt;- workflow() |&gt; add_recipe(workers_recipe) |&gt; add_model(glm_spec) # Tune workers_glm_res &lt;- tune_grid( glm_wf, resamples = workers_folds, grid = 10, metrics = workers_metrics ) show_best(workers_glm_res, metric = &quot;roc_auc&quot;) Using the training data, build the SVM model. Solution # Spec svm_spec &lt;- svm_rbf(cost = tune(), rbf_sigma = tune()) |&gt; set_engine(&quot;kernlab&quot;) |&gt; set_mode(&quot;classification&quot;) # Workflow svm_wf &lt;- workflow() |&gt; add_recipe(workers_recipe) |&gt; add_model(svm_spec) # Tune workers_svm_res &lt;- tune_grid( svm_wf, resamples = workers_folds, grid = 10, metrics = workers_metrics ) show_best(workers_svm_res, metric = &quot;roc_auc&quot;) From the created models, which is the most optimal? Solution # Collect best metrics from each model rf_best &lt;- show_best(workers_rf_res, metric = &quot;roc_auc&quot;, n = 1) glm_best &lt;- show_best(workers_glm_res, metric = &quot;roc_auc&quot;, n = 1) svm_best &lt;- show_best(workers_svm_res, metric = &quot;roc_auc&quot;, n = 1) # Create comparison tibble model_comparison &lt;- bind_rows( rf_best |&gt; mutate(model = &quot;Random Forest&quot;), glm_best |&gt; mutate(model = &quot;GLM&quot;), svm_best |&gt; mutate(model = &quot;SVM&quot;) ) # Visualize model_comparison |&gt; ggplot(aes(x = reorder(model, mean), y = mean, fill = model)) + geom_col() + geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err), width = 0.2) + labs(y = &quot;ROC AUC&quot;, x = &quot;Model&quot;, title = &quot;Model Comparison by ROC AUC&quot;) + theme(legend.position = &quot;none&quot;) + coord_flip() We see how the results overlap, so we could opt for the two that have the highest mean ROC and among them choose the one that gives us a smaller range of values. Create the confusion matrices for the three models created. Solution # Finalize and fit each model best_rf &lt;- select_best(workers_rf_res, metric = &quot;roc_auc&quot;) final_rf_wf &lt;- finalize_workflow(rf_wf, best_rf) rf_fit &lt;- fit(final_rf_wf, data = workers_train) best_glm &lt;- select_best(workers_glm_res, metric = &quot;roc_auc&quot;) final_glm_wf &lt;- finalize_workflow(glm_wf, best_glm) glm_fit &lt;- fit(final_glm_wf, data = workers_train) best_svm &lt;- select_best(workers_svm_res, metric = &quot;roc_auc&quot;) final_svm_wf &lt;- finalize_workflow(svm_wf, best_svm) svm_fit &lt;- fit(final_svm_wf, data = workers_train) # Predictions and confusion matrices rf_preds &lt;- augment(rf_fit, new_data = workers_test) rf_preds |&gt; conf_mat(truth = attrition_status, estimate = .pred_class) glm_preds &lt;- augment(glm_fit, new_data = workers_test) glm_preds |&gt; conf_mat(truth = attrition_status, estimate = .pred_class) svm_preds &lt;- augment(svm_fit, new_data = workers_test) svm_preds |&gt; conf_mat(truth = attrition_status, estimate = .pred_class) Keep in mind that the model with the highest ROC value will not necessarily have the highest accuracy. Therefore the choice of the model was performed in a previous step. The ROC better balances sensitivity with the false positive rate. 11.11 Ethics: Bias in Algorithmic Decision Making In the previous exercise, we built models to predict employee attrition using variables like Gender, Age, and MaritalStatus. While mathematically sound, obtaining a high accuracy score does not mean the model is “good” or “fair” to use in the real world. 11.11.1 The Risk of Proxy Variables Even if we remove explicit sensitive attributes (like Gender or Ethnicity), other variables can act as proxies. * Zip Code: Often correlates with race or socioeconomic status. * Years of Experience: Strongly correlated with Age. 11.11.2 Feedback Loops If a company uses an algorithm to decide who to hire or fire based on historical data, they may perpetuate historical biases. * Scenario: If a company historically didn’t hire women for leadership roles, the training data will show that women are “less likely to succeed” in those roles. * Result: The model creates a feedback loop, rejecting qualified female candidates because they don’t match the historical pattern of “success”. [!IMPORTANT] What can we do? Audit your Data: checking for representation balance (e.g., is one group significantly smaller?). Model Explainability: Use tools like DALEX or vip (variable importance) to understand why the model is making a decision. If MaritalStatus is the top predictor for firing someone, is that ethical? Human in the Loop: These models should support human decision-making, not replace it entirely. As Data Scientists, our responsibility extends beyond the AUC score. We must ensure our models do not harm individuals or groups. References "],["unsupervised-learning.html", "Chapter 12 Unsupervised Learning 12.1 Learning Objectives 12.2 Applications of Unsupervised Learning 12.3 K-Means Clustering 12.4 Hierarchical Clustering 12.5 Dimensionality Reduction 12.6 Exercises", " Chapter 12 Unsupervised Learning Now that we know how to create supervised learning algorithms, understanding unsupervised learning becomes an intuitive exercise. While in supervised learning we have a set of variables that we use to predict a certain output class (up/down, resign/not resign), in unsupervised learning we do not have expected output classes. In supervised learning we had training data and testing data that allowed us to validate the effectiveness of the model by its closeness to the known class. In unsupervised learning we do not have a default output. This in turn generates a great challenge because it is very difficult to know if we have already finished the work or if we can still generate another model with which we feel more satisfied. The simplest example to understand this type of learning is when we have our customer base and we want to segment them for the first time. In that case, we look for customers who behave in the same way, but being the first time, we don’t know how many segments we can have. The challenge lies in determining the cut-off: how many segments do we seek to create? 12.1 Learning Objectives By the end of this section, you will be able to: By the end of this section, you will be able to clearly differentiate between supervised and unsupervised learning, understanding the distinct challenges of working with unlabeled data. We will apply the k-means clustering algorithm to segment data into optimal groups and use hierarchical clustering to visualize relationships through dendrograms. Furthermore, you will learn to evaluate the quality of these clusters using the elbow and silhouette methods, and perform basic dimensionality reduction to simplify complex datasets. 12.2 Applications of Unsupervised Learning The main applications of unsupervised learning are related to data clustering. Here, the goal is to find homogeneous subgroups within the data. These algorithms are based on the distance between observations. The customer segmentation example would be an example of clustering. The most commonly used clustering algorithms are: k-means clustering and hierarchical clustering. 12.3 K-Means Clustering To understand this method we will use examples first with a minimal amount of variables and then little by little we will create a more generic model. 12.3.1 Clustering with k = 2 Suppose we have a list of players on a soccer field and we take a photo from above to have their coordinates (variable 1 would be the x-axis and variable 2 would be the y-axis). We cannot see which team each player belongs to so we will paint everyone as black dots. players &lt;- tibble(x = c(-1, -2, 8, 7, -12, -15, -13, 15, 21, 12, -25, 26), y = c(1, -3, 6, -8, 8, 0, -10, 16, 2, -15, 1, 0) ) players |&gt; ggplot() + aes(x, y) + geom_point(size = 5) This method allows us to group based on the definition of centroids. We will define as many centroids as groups we want to obtain. Since for this case we know that there must be two teams, we will use 2 centroids (k = 2). The k-means algorithm then places these 2 points (centroids) randomly on the plane in a first iteration. Then, it calculates the distance between each center and the other data points. If it is closer to a centroid then it assigns it to centroid 1, otherwise to centroid 2. A first grouping has already been performed. Now each centroid within each group is located at the mean of the other points in its group and another iteration occurs to reassign all points. This iteration is done over and over again until the centroids are fixed. To create this model in R we will use the function kmeans(data, centers = k). kmeans_model &lt;- kmeans(players, centers = 2) # We print the coordinates of the centers kmeans_model$centers #&gt; x y #&gt; 1 -11.33333 -0.5000000 #&gt; 2 14.83333 0.1666667 This means that for these two centers the average distance to the other points is the minimum, therefore the algorithm assigns them to one group or another. Let’s see approximately where these centers are located if we marked them with an x. Thus, once the model is created we can obtain the clustering results, team 1 or team 2. team &lt;- kmeans_model$cluster We can add this team assignment as one more column of our players data set to be able to visualize them in R. # We add the cluster column players_grouped &lt;- players |&gt; mutate(cluster = team) # We visualize the players according to the grouping players_grouped |&gt; ggplot() + aes(x, y, fill = factor(cluster)) + geom_point(size = 5, pch = 21) + scale_fill_manual(values=c(&quot;#EE220D&quot;, &quot;#01A2FF&quot;)) + theme(legend.position = &quot;none&quot;) We have found two centroids until minimizing the sum of the squared differences between each centroid and the other points in the cluster. We can access and see how much this value is, given that it is part of the model results. # Sum of squares within each cluster kmeans_model$withinss #&gt; [1] 570.8333 863.6667 # Total kmeans_model$tot.withinss #&gt; [1] 1434.5 Tot.withinss comes from Total within-cluster sum of squares. 12.3.2 Clustering with k &gt;= 3 When we have 3 or more centers the idea is the same, we only change the centers parameter. kmeans_model &lt;- kmeans(players, centers = 3) team &lt;- kmeans_model$cluster players_grouped &lt;- players |&gt; mutate(cluster = team) players_grouped |&gt; ggplot() + aes(x, y, color = factor(cluster)) + geom_point(size = 5) + theme(legend.position = &quot;none&quot;) kmeans_model$tot.withinss #&gt; [1] 881.25 In this case we have found that the sum of squares within the clusters is smaller, so we could indicate that this grouping is more optimal than the grouping into two groups. However, the sum of squares is not necessarily the best indicator for choosing how many clusters to create. 12.3.3 Determination of Optimal Clusters We can mainly use two methods to determine how many clusters we should build, k. The sum of squares method (wss) and the average silhouette method (silhouette). To avoid having to calculate models for different values of k we will use the factoextra library, which was created especially to perform easy multivariate data analysis and elegant visualization, very useful for clustering. install.packages(&quot;factoextra&quot;) library(factoextra) 12.3.3.1 Sum of Squares Method To find the optimal “k” under this method, we will use the elbow plot, where we first calculate the total within-cluster sum of squares for different values of “k”. Then, visually we will identify a point where there seems to be a very strong drop followed by a more gradual drop in the slope. To do this, we will use the function fviz_nbclust(data, type, method) and enter our data, the type of algorithm that will be used to group and the measurement method. fviz_nbclust(players, FUN = kmeans, method = &quot;wss&quot;) In this case the “elbow” is found at the value k = 2, from there the sum of squares reduces but at a slower rate. 12.3.3.2 Average Silhouette Method The method described above is a visual aid that makes recognition difficult when the data points are closer. Therefore, it is much more frequent to perform a silhouette analysis (Rousseeuw 1987). This approach measures the quality of a clustering. That is, it determines how well each object lies within its group. A high average silhouette width indicates a good clustering. The average silhouette method calculates the average silhouette of observations for different values of “k”. The optimal number of groups “k” is the one that maximizes the average silhouette over a range of possible values for “k”. To do this, we change the method parameter in the function and obtain the silhouette analysis. fviz_nbclust(players, FUN = kmeans, method = &quot;silhouette&quot;) Here it is clearly seen that for a value of k=2 we have the best average, making this our optimal number of groups. [!TIP] Interpreting Silhouette Scores Interpreting these scores is straightforward: a score close to 1 indicates that the data point is well-matched to its own cluster and distinct from neighbors, representing a strong grouping. A score near 0 suggests the point lies on the boundary between clusters, while a negative score implies the point may have been assigned to the wrong group. Generally, an average silhouette width above 0.5 signals a solid clustering structure. 12.3.4 k-means for more than 2 variables The method we have learned can be easily extended to more variables. Only in this case it would no longer be possible to visualize it like the soccer team and we would only visualize the results of the grouping and the learned metrics. To do this, we will use the following customer dataset, where we will find a dataset of customers of a wholesale distributor. It includes the annual spending in monetary units on various product categories. url &lt;- &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/00292/Wholesale%20customers%20data.csv&quot; customers &lt;- read_csv(url) #&gt; Rows: 440 Columns: 8 #&gt; ── Column specification ──────────────────────────────── #&gt; Delimiter: &quot;,&quot; #&gt; dbl (8): Channel, Region, Fresh, Milk, Grocery, Frozen, Detergents_Paper, De... #&gt; #&gt; ℹ Use `spec()` to retrieve the full column specification for this data. #&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. We are going to perform a grouping only considering the spending made on frozen foods, groceries and dairy products. customers_filtered &lt;- customers |&gt; select(Milk, Grocery, Frozen) # We scale the data to ensure equal weight for all variables customers_scaled &lt;- as.data.frame(scale(customers_filtered)) Once we have our data we would create a silhouette analysis to determine the best value of “k”. fviz_nbclust(customers_scaled, FUN = kmeans, method = &quot;silhouette&quot;) Again, we get that the recommended number of clusters is 2. Let’s create the model for k = 2 and store the resulting cluster. model &lt;- kmeans(customers_scaled, centers = 2) customers_grouped &lt;- customers_filtered |&gt; mutate(cluster = model$cluster) customers_grouped #&gt; # A tibble: 440 × 4 #&gt; Milk Grocery Frozen cluster #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 9656 7561 214 1 #&gt; 2 9810 9568 1762 1 #&gt; 3 8808 7684 2405 1 #&gt; 4 1196 4221 6404 1 #&gt; 5 5410 7198 3915 1 #&gt; 6 8259 5126 666 1 #&gt; 7 3199 6975 480 1 #&gt; 8 4956 9426 1669 1 #&gt; 9 3648 6192 425 1 #&gt; 10 11093 18881 1159 1 #&gt; # ℹ 430 more rows Once we have grouped our data we can calculate the amount of data in each cluster and the mean of the values for each group and thus identify differences between these two potential customer segments. customers_grouped |&gt; group_by(cluster) |&gt; summarise(total = n(), mean_Milk = mean(Milk), mean_Grocery = mean(Grocery), mean_Frozen = mean(Frozen)) #&gt; # A tibble: 2 × 5 #&gt; cluster total mean_Milk mean_Grocery mean_Frozen #&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 395 4056. 5628. 2864. #&gt; 2 2 45 21070. 28341. 4898. Thus, we have learned to segment customers using machine learning. 12.4 Hierarchical Clustering Hierarchical clustering is another method for grouping data. The word hierarchical comes from the hierarchies that this algorithm creates to determine the clusters. Unlike k-means, we do not start by indicating how many clusters we want to create, but rather the algorithm shows us a list of possible combinations according to the hierarchy of distances between points. Let’s see it with an example. 12.4.1 Clustering with two variables To do this we will use the same soccer team example that we used previously. With the difference that this time we number each player to make visualization easier. num &lt;- 1:12 players &lt;- tibble(x = c(-1, -2, 8, 7, -12, -15, -13, 15, 21, 12, -25, 26), y = c(1, -3, 6, -8, 8, 0, -10, 16, 2, -15, 1, 0)) players |&gt; ggplot() + aes(x, y, label = num) + geom_point(size = 5) + geom_text(nudge_x = 1.3, nudge_y = 1.3) This algorithm searches for the two points with the shortest distance, the closest ones, and groups them. Then it searches for another two points with the smallest distance and asks: is the distance between these two new points less than the distance of these points to the previously created group? If the answer is yes, it groups them, otherwise it groups the closest point to the first created group. Let’s understand the algorithm graphically. Points 1 and 2 have the lowest hierarchy since they have the shortest distance. Then the algorithm searches for the next two closest points (point 9 and 12) and when comparing with the midpoint of 1 and 2 it opts to create a new group with a slightly higher hierarchy and so on. However, now that we have point 7 and 11 and we calculate the distance, it turns out that that distance is not the smallest compared to the distances with the other existing groups. For example, 7 is closer to the midpoint of 1 and 2, and 11 is closer to the midpoint of 5 and 6. Thus, the algorithm creates a higher hierarchy for this grouping. The algorithm continues until it finally creates a group that includes everyone as the highest hierarchy. In the following graph we can not only appreciate this but also on the y-axis the distance between each point or group of points. Up to here we haven’t done more than generate hierarchies from the distances which will serve us later to determine how many clusters to generate. Let’s create in R what has been advanced so far. The first thing we will do is calculate the distances between all points. To do this we will use the dist() function. player_distances &lt;- dist(players) With the calculated distances we can create the hierarchical model using the hclust(distance_matrix) function. hierarchical_model &lt;- hclust(player_distances) Once our model is created we can visualize it using the dendextend library. install.packages(&quot;dendextend&quot;) library(dendextend) The visualization we saw is called a dendrogram. To do this we just have to convert our model to dendrogram format. dend_model &lt;- as.dendrogram(hierarchical_model) plot(dend_model) So far we have only seen the hierarchy, but what interests us is the grouping. The grouping is done by the calculated distance (h parameter). Let’s try with a distance of 60. We will use the color_branches and color_labels functions to make the changes visible. cut_height &lt;- 60 dend_model |&gt; color_branches(h = cut_height, col = c(&quot;red&quot;, &quot;green&quot;, &quot;blue&quot;, &quot;orange&quot;)) |&gt; color_labels(h = cut_height, col = c(&quot;red&quot;, &quot;green&quot;, &quot;blue&quot;, &quot;orange&quot;)) |&gt; plot() #&gt; Warning in get_col(col, k): Length of color vector was longer than the number #&gt; of clusters - first k elements are used Since the highest hierarchy distance is approximately 50, then in this case it groups everyone into one large cluster. Let’s try with a lower number, for example 40. cut_height &lt;- 40 dend_model |&gt; color_branches(h = cut_height, col = c(&quot;red&quot;, &quot;green&quot;, &quot;blue&quot;)) |&gt; color_labels(h = cut_height, col = c(&quot;red&quot;, &quot;green&quot;, &quot;blue&quot;)) |&gt; plot() |&gt; abline(h = cut_height, lty = 2) #&gt; Warning in get_col(col, k): Length of color vector was longer than the number #&gt; of clusters - first k elements are used By making a cut at 40 we now have two clusters, in this case the red color and the green color. Let’s try with a lower number, 28. cut_height &lt;- 28 dend_model |&gt; color_branches(h = cut_height) |&gt; color_labels(h = cut_height) |&gt; plot() |&gt; abline(h = cut_height, lty = 2) #&gt; Loading required namespace: colorspace Now we have three clusters and so we could continue until obtaining the clusters we need. We must have noticed how impractical it is to use the distances of the hierarchical model because they vary according to the data we have. This model allows us to make cuts not only by distances but also by indicating how many clusters we want, parameter k. desired_clusters &lt;- 3 dend_model |&gt; color_branches(k = desired_clusters) |&gt; color_labels(k = desired_clusters) |&gt; plot() We see that it gives us the same grouping whether we use distances or number of desired clusters. 12.4.2 Determination of Optimal Clusters To calculate how many clusters are optimal to create we will use the silhouette analysis again, but this time with the argument FUN = hcut to determine that it be evaluated based on a hierarchical model. fviz_nbclust(players, FUN = hcut, method = &quot;silhouette&quot;) It is not surprising that the value of k is also 2, which coincides with the number obtained in the k-means model. 12.4.3 Obtain the grouping Now that we have validated that the recommended number of clusters is 2, we calculate the grouping from the previously created model. players_grouped &lt;- players |&gt; mutate(cluster = cutree(hierarchical_model, k = 2) ) players_grouped #&gt; # A tibble: 12 × 3 #&gt; x y cluster #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 -1 1 1 #&gt; 2 -2 -3 1 #&gt; 3 8 6 2 #&gt; 4 7 -8 2 #&gt; 5 -12 8 1 #&gt; 6 -15 0 1 #&gt; 7 -13 -10 1 #&gt; 8 15 16 2 #&gt; 9 21 2 2 #&gt; 10 12 -15 2 #&gt; 11 -25 1 1 #&gt; 12 26 0 2 Finally, let’s visualize the grouping performed with this method. players_grouped |&gt; ggplot() + aes(x, y, color = factor(cluster)) + geom_point(size = 5) + theme(legend.position = &quot;none&quot;) We see that the grouping is the same as with the previous method, basically because we are talking about two variables and two clusters. Both methods learned are very flexible, so the creation of models for more variables follows the same logic learned in these sections. 12.5 Dimensionality Reduction We have created clusters with a controlled number of variables. However, we are going to encounter in many cases many more variables that make interpretation difficult and it is important to identify if two variables have the same behavior to be able to take only one of them. For this case we are going to take as an example a credit card customer dataset, adaptation of the public dataset in Kaggle, from the following route. url &lt;- &quot;https://dparedesi.github.io/DS-with-R-datasets/credit-cards.csv&quot; cards_df &lt;- read_csv(url) We have more than 8 thousand customers with 13 attributes. We will analyze if there are strongly correlated variables. To do this we will use the corrplot library. library(corrplot) Next, we will enter the dataset to visualize correlations between the variables, corrplot(cor(cards_df), type=&quot;upper&quot;, method=&quot;ellipse&quot;, tl.cex=0.9) There is a strong correlation between the total purchases variable and the purchases made in the first 3 months. We can visualize these two variables to validate. cards_df |&gt; ggplot() + aes(x=purchases, y=purchases_first_3_months) + geom_point() + labs(title=&quot;Customer Attributes&quot;, subtitle=&quot;Relationship between total purchases and first 3 months&quot;) Given this, we could include within our analysis only one of these two variables. We could also validate the distribution of these variables. # We remove the purchases first 3 months variable cards_df &lt;- cards_df[, !names(cards_df) == &quot;purchases_first_3_months&quot;] cards_df |&gt; pivot_longer(cols = everything(), names_to = &quot;attributes&quot;, values_to = &quot;values&quot;) |&gt; ggplot() + aes(x=values, fill=attributes) + geom_histogram(colour=&quot;black&quot;, show.legend=FALSE) + facet_wrap(~attributes, scales=&quot;free_x&quot;) + labs(x=&quot;Values&quot;, y=&quot;Frequency&quot;, title=&quot;Customer Attributes - Histogram&quot;) We see data concentrations in some variables such as tenure (time our customer has been with us). We can validate it by zooming into that variable. boxplot(cards_df$tenure) prop.table(table(cards_df$tenure)) 85% of our data are from customers who have been with us for 12 months. We could choose to filter the data to analyze customers who have 1 year and thus remove this variable from the grouping. cards_df &lt;- cards_df |&gt; filter(tenure == 12) cards_df &lt;- cards_df[, !names(cards_df) == &quot;tenure&quot;] # We will do the same with the balance_freq variable prop.table(table(cards_df$balance_freq)) cards_df &lt;- cards_df |&gt; filter(balance_freq == 1) cards_df &lt;- cards_df[, !names(cards_df) == &quot;balance_freq&quot;] If we also analyze the distributions of each variable we find the following: summary(cards_df) We see that there are variables that have maximums of 1, as there are others that have a maximum of 30 thousand or 50 thousand. We had already seen previously the importance of normalizing data. Here we will also do it with the scale() function. cards_df_norm &lt;- as.data.frame(scale(cards_df)) We can verify that the distribution does not change, only the scale. cards_df_norm |&gt; pivot_longer(cols = everything(), names_to = &quot;attributes&quot;, values_to = &quot;values&quot;) |&gt; ggplot() + aes(x=values, fill=attributes) + geom_histogram(colour=&quot;black&quot;, show.legend=FALSE) + facet_wrap(~attributes, scales=&quot;free_x&quot;) + labs(x=&quot;Values&quot;, y=&quot;Frequency&quot;, title=&quot;Customer Attributes - Histogram&quot;) Data preparation and variable reduction is a necessary step when we create machine learning models. However, we must do it carefully, given that in this exercise when preparing the data, although we have fewer variables (10), we also have fewer rows. str(cards_df_norm) More advanced techniques such as Principal Component Analysis (PCA) and Singular Value Decomposition (SVD) are used to perform dimensionality reduction more rigorously so as not to lose so much data in our analysis. These techniques are widely used in practice and well-documented, though they require a solid understanding of linear algebra for proper interpretation. 12.6 Exercises In the following exercises we will work on post data from 10 fashion companies that have their pages on Facebook and the reactions of their followers. To do this, we will work with the data in the following repository: url &lt;- &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/00488/Live.csv&quot; posts &lt;- read_csv(url) # We remove columns not relevant to the analysis irrelevant_columns &lt;- c(&quot;status_type&quot;,&quot;status_id&quot;, &quot;status_published&quot;, &quot;Column1&quot;, &quot;Column2&quot;, &quot;Column3&quot;, &quot;Column4&quot;) data_posts &lt;- posts[, !names(posts) %in% irrelevant_columns] With the data_posts object normalized (use scale() function) and create the data_posts_norm object. Build a silhouette plot to determine how many cluster groups are recommended using the k-means algorithm. Solution data_posts_norm &lt;- as.data.frame(scale(data_posts)) fviz_nbclust(data_posts_norm, FUN = kmeans, method = &quot;silhouette&quot;) With the data_posts object build a silhouette plot to determine how many cluster groups are recommended using the hierarchical algorithm. Solution fviz_nbclust(data_posts, FUN = hcut, method = &quot;silhouette&quot;) If you had to remove a variable from the analysis, which variable would it be? Solution # We perform a visualization of the correlation of variables corrplot(cor(data_posts), type=&quot;upper&quot;, method=&quot;ellipse&quot;, tl.cex=0.9) # We can check it by plotting these two variables: data_posts |&gt; ggplot() + aes(x=num_reactions, y=num_likes) + geom_point() Remove the num_reactions variable from the data_posts_norm object and the data_posts object and perform a silhouette analysis again using data_posts_norm. Does the number of clusters change? Solution data_posts &lt;- data_posts[, -1] data_posts_norm &lt;- data_posts_norm[, -1] fviz_nbclust(data_posts_norm, FUN = kmeans, method = &quot;silhouette&quot;) The number of clusters does not change because there exists another variable with the same behavior as this one. Create the k-means model to group using the recommended number of clusters found. Use the data_posts_norm object for the creation of the model. Create the data_posts_grouped object where the original data of data_posts is with the additional column cluster_kmeans indicating the cluster result of this model. Solution kmeans_model &lt;- kmeans(data_posts_norm, centers = 2) data_posts_grouped &lt;- data_posts |&gt; mutate(cluster_kmeans = kmeans_model$cluster) Create the hierarchical model to group using the recommended number of clusters found. Use the data_posts_norm object for the creation of the model. Add to the data_posts_grouped object the column cluster_hier to store the result of the grouping. Solution distances &lt;- dist(data_posts_norm) hier_model &lt;- hclust(distances) data_posts_grouped &lt;- data_posts_grouped |&gt; mutate(cluster_hier = cutree(hier_model, k = 2) ) Calculate the average of each value of the variables for each group of the k-means model. Solution data_posts_grouped |&gt; select(-cluster_hier) |&gt; group_by(cluster_kmeans) |&gt; summarise_all(list(mean)) Calculate the average of each value of the variables for each group of the hierarchical model. Solution data_posts_grouped |&gt; select(-cluster_kmeans) |&gt; group_by(cluster_hier) |&gt; summarise_all(list(mean)) References "],["string-processing-and-text-mining.html", "Chapter 13 String processing and text mining 13.1 Basic functions 13.2 Regular expressions 13.3 From strings to dates 13.4 Exercises 13.5 Text Mining using Tidy Data 13.6 Sentiment Analysis 13.7 Exercises", " Chapter 13 String processing and text mining 13.1 Basic functions We have already learned how to import data and consolidate it. However, we cannot yet work with this data. We have to validate through string processing and ensure a minimum quality to be able to perform our analyses. For example, in the previous chapter we imported data from Wikipedia, however we did not focus on whether we could already perform operations or visualizations with our data. library(rvest) url &lt;- &quot;https://es.wikipedia.org/wiki/Anexo:Pa%C3%ADses_hispanos_por_poblaci%C3%B3n&quot; #url &lt;- &quot;https://es.wikipedia.org/wiki/Distribuci%C3%B3n_geogr%C3%A1fica_del_idioma_espa%C3%B1ol&quot; #as a back up URL html_data &lt;- read_html(url) web_tables &lt;- html_data |&gt; html_nodes(&quot;body&quot;) |&gt; html_nodes(&quot;table&quot;) raw_table &lt;- web_tables[[2]] |&gt; html_table() raw_table &lt;- raw_table |&gt; setNames(c(&quot;N&quot;, &quot;country&quot;, &quot;population&quot;, &quot;prop_population&quot;, &quot;avg_change&quot;, &quot;link&quot;)) raw_table &lt;- raw_table |&gt; as_tibble() raw_table |&gt; head(5) We may not have noticed, but we can observe columns with spaces or commas where there should be numbers. We can validate this not only by analyzing the class of the column, but also if we try to calculate the average of that variable. class(raw_table$population) mean(raw_table$population) We cannot do a direct conversion to number either because white spaces and commas are characters. as.numeric(raw_table$population) There are so frequent and so many possible use cases that there are already multiple functions for processing strings included in the tidyverse library. Likewise, there is more than one way to process strings. It will always depend on how the raw data is found. 13.1.1 Replacing characters One of the basic functions that we will use the most will be replacing characters. We apply this function when we are sure that this change will not compromise the rest of the data. We have spaces and we have commas. So we could start by replacing one of the two to normalize them using the str_replace_all(string, pattern, replacement) function. In the pattern attribute we will use \\\\s, which comes from space. We are going to learn first to modify the data stored in a vector and then we will replicate it to our entire table. library(tidyverse) library(stringr) population_vector &lt;- raw_table$population population_vector &lt;- str_replace_all(population_vector, &quot;\\\\s&quot;, &quot;,&quot;) population_vector We have purposely taken all the values to be separated by commas because now we can easily use the parse_number(vector) function which not only replaces the commas with empty strings, but also removes any non-numeric value before the first number, which facilitates us if we had monetary values, and also converts the value from character type to numeric type. population_vector &lt;- parse_number(population_vector) # Additional example in case we had a monetary value: parse_number(&quot;$345,153&quot;) This vector now allows us to perform mathematical operations or visualization of the distribution. # Convert to millions population_vector &lt;- population_vector/10^6 # We remove the last value which is the world population: length_val &lt;- length(population_vector) population_vector &lt;- population_vector[-length_val] # Visualization boxplot(population_vector) We already know which functions to use to transform the fields of our case. However, we have applied them to vectors. To mutate the columns of our table in raw form we will use the function mutate(across(columns, function)) using the pipeline operator |&gt;. Let’s apply the first change of spaces by commas and not only to column 3, population, but also to column 5, average change. raw_table |&gt; mutate(across(c(3,5), ~str_replace_all(., &quot;\\\\s&quot;, &quot;,&quot;))) We have removed from the str_replace_all function the string attribute and replaced it with a dot .. And that dot . indicates that it will evaluate for each column c(3,5) of our table. Now, let’s apply the parse_number function that we applied previously. raw_table |&gt; mutate(across(c(3,5), ~str_replace_all(., &quot;\\\\s&quot;, &quot;,&quot;))) |&gt; mutate(across(c(3,5), ~parse_number(.))) 13.2 Regular expressions A regular expression11 (or regex as it is known in English) is a pattern that describes a set of strings. We have already used regex in the previous section using only the pattern \\\\s. However, usually we will have many more use cases that will require a pattern that can convert a wider range of cases. Although we could analyze all possible use cases available in the documentation, we learn faster by use cases. Let’s analyze a case that will allow us to learn some patterns little by little. In the dslabs library we found and used previously the height data, heights, of students from a university expressed in inches. library(dslabs) library(tidyverse) data(heights) heights |&gt; head(10) #&gt; sex height #&gt; 1 Male 75 #&gt; 2 Male 70 #&gt; 3 Male 68 #&gt; 4 Male 74 #&gt; 5 Male 61 #&gt; 6 Female 65 #&gt; 7 Female 66 #&gt; 8 Female 62 #&gt; 9 Female 66 #&gt; 10 Male 67 These data were ready to be analyzed. However, that was not how it came from the source. The students had to fill out a survey and even when they were asked for their height in inches, they completed their height in inches, feet, centimeters, writing numbers, letters, etc. We can see the initial data from the form in the reported_heights data frame. reported_heights |&gt; head(10) #&gt; time_stamp sex height #&gt; 1 2014-09-02 13:40:36 Male 75 #&gt; 2 2014-09-02 13:46:59 Male 70 #&gt; 3 2014-09-02 13:59:20 Male 68 #&gt; 4 2014-09-02 14:51:53 Male 74 #&gt; 5 2014-09-02 15:16:15 Male 61 #&gt; 6 2014-09-02 15:16:16 Female 65 #&gt; 7 2014-09-02 15:16:19 Female 66 #&gt; 8 2014-09-02 15:16:21 Female 62 #&gt; 9 2014-09-02 15:16:21 Female 66 #&gt; 10 2014-09-02 15:16:22 Male 67 Although we might think that they entered the data correctly, we do not have to trust and it is always better to validate the quality of our data. There are multiple ways to validate, as we can see below: heights &lt;- reported_heights$height # Validation option 1: Random sample sample(heights, 100) #&gt; [1] &quot;71&quot; &quot;67&quot; &quot;66&quot; &quot;64&quot; &quot;70&quot; &quot;70&quot; #&gt; [7] &quot;76&quot; &quot;1&quot; &quot;59&quot; &quot;67.2&quot; &quot;69&quot; &quot;67&quot; #&gt; [13] &quot;67&quot; &quot;6&quot; &quot;62&quot; &quot;69&quot; &quot;74&quot; &quot;69&quot; #&gt; [19] &quot;178&quot; &quot;69&quot; &quot;74&quot; &quot;169&quot; &quot;67&quot; &quot;68.5&quot; #&gt; [25] &quot;68.5&quot; &quot;71&quot; &quot;68&quot; &quot;158&quot; &quot;6.1&quot; &quot;708,661&quot; #&gt; [31] &quot;6.2&quot; &quot;69&quot; &quot;75&quot; &quot;5&#39;6&quot; &quot;67&quot; &quot;68.4&quot; #&gt; [37] &quot;75&quot; &quot;5.7&quot; &quot;72&quot; &quot;77&quot; &quot;75&quot; &quot;68&quot; #&gt; [43] &quot;69&quot; &quot;72&quot; &quot;62&quot; &quot;65&quot; &quot;73&quot; &quot;67&quot; #&gt; [49] &quot;67&quot; &quot;5.2&quot; &quot;67.71&quot; &quot;67&quot; &quot;5&#39;3&quot; &quot;66&quot; #&gt; [55] &quot;5&#39;7.5&#39;&#39;&quot; &quot;69&quot; &quot;65&quot; &quot;150&quot; &quot;69&quot; &quot;5&#39;11&#39;&#39;&quot; #&gt; [61] &quot;68.11024&quot; &quot;175&quot; &quot;152&quot; &quot;5&#39; 10&quot; &quot;65&quot; &quot;74.5&quot; #&gt; [67] &quot;70&quot; &quot;72&quot; &quot;73.22&quot; &quot;63&quot; &quot;5&#39;9&#39;&#39;&quot; &quot;68.5&quot; #&gt; [73] &quot;74&quot; &quot;74&quot; &quot;5&#39;7\\&quot;&quot; &quot;6&#39;3\\&quot;&quot; &quot;67&quot; &quot;73.22&quot; #&gt; [79] &quot;74&quot; &quot;72.8346&quot; &quot;67.72&quot; &quot;175&quot; &quot;5.69&quot; &quot;69.3&quot; #&gt; [85] &quot;5&#39;10&#39;&#39;&quot; &quot;72&quot; &quot;60&quot; &quot;68&quot; &quot;69&quot; &quot;73&quot; #&gt; [91] &quot;75&quot; &quot;70&quot; &quot;64&quot; &quot;170&quot; &quot;649,606&quot; &quot;73&quot; #&gt; [97] &quot;58&quot; &quot;60&quot; &quot;174&quot; &quot;64.173&quot; # Validation option 2: convert to numbers and count if there are NAs x &lt;- as.numeric(heights) #&gt; Warning: NAs introduced by coercion sum(is.na(x)) #&gt; [1] 81 # Validation option 3: add column of those that cannot be converted to number: reported_heights |&gt; mutate(numeric_height = as.numeric(height)) |&gt; filter(is.na(numeric_height)) |&gt; head(10) #&gt; Warning: There was 1 warning in `mutate()`. #&gt; ℹ In argument: `numeric_height = as.numeric(height)`. #&gt; Caused by warning: #&gt; ! NAs introduced by coercion #&gt; time_stamp sex height numeric_height #&gt; 1 2014-09-02 15:16:28 Male 5&#39; 4&quot; NA #&gt; 2 2014-09-02 15:16:37 Female 165cm NA #&gt; 3 2014-09-02 15:16:52 Male 5&#39;7 NA #&gt; 4 2014-09-02 15:16:56 Male &gt;9000 NA #&gt; 5 2014-09-02 15:16:56 Male 5&#39;7&quot; NA #&gt; 6 2014-09-02 15:17:09 Female 5&#39;3&quot; NA #&gt; 7 2014-09-02 15:18:00 Male 5 feet and 8.11 inches NA #&gt; 8 2014-09-02 15:19:48 Male 5&#39;11 NA #&gt; 9 2014-09-04 00:46:45 Male 5&#39;9&#39;&#39; NA #&gt; 10 2014-09-04 10:29:44 Male 5&#39;10&#39;&#39; NA We might want to choose to eliminate these NA data as they are not significant with respect to the total of 1,095 data points. However, there are several of these data points that follow a determined pattern and instead of being discarded could be converted to the scale we have in the rest of the data. For example, there are people who entered their height as 5’7”, which, for those who remember the conversion, can be converted because 1 foot is 12 inches. So \\(5*12+7=67\\). And so, like that case, we can detect patterns, but we have, again, to be careful in detecting the exact pattern and not a very generic one that can change other use cases. If everyone followed the same pattern \\(x&#39;y&#39;&#39;\\) or \\(x&#39;y\\) it would be much easier to convert it to inches by calculating \\(x*12+y\\). Let’s start by extracting our column to a single character vector with all the values that do not convert automatically to number or were entered in inches. We detect this if they measure more than 5 and up to 7 feet (from 1.5m to 2.1 meters). After that we will create the transformations little by little. problematic_heights &lt;- reported_heights |&gt; filter(is.na(as.numeric(height)) | # Does not convert to number (!is.na(as.numeric(height)) &amp; as.numeric(height) &gt;= 5 &amp; as.numeric(height) &lt;= 7 ) # or entered in feet and not inches ) |&gt; pull(height) length(problematic_heights) #&gt; [1] 168 Adding the condition of having entered in feet we have 168 errors. We cannot ignore 15.3% of errors. We will use str_view() to visualize matches. This function is extremely helpful when debugging regular expressions as it highlights exactly what is matching your pattern. # Let&#39;s visualize entries containing &quot;feet&quot; str_view(problematic_heights, &quot;feet&quot;, match=TRUE) #&gt; [10] │ 5 &lt;feet&gt; and 8.11 inches #&gt; [82] │ 5 &lt;feet&gt; 7inches #&gt; [140] │ 5 &lt;feet&gt; 6 inches We can also use str_detect(string, pattern) to get a logical value (TRUE/FALSE) to filter our vector. index &lt;- str_detect(problematic_heights, &quot;feet&quot;) problematic_heights[index] # Match the pattern #&gt; [1] &quot;5 feet and 8.11 inches&quot; &quot;5 feet 7inches&quot; &quot;5 feet 6 inches&quot; problematic_heights[!index] |&gt; # Do not match the pattern head(40) #&gt; [1] &quot;6&quot; &quot;5&#39; 4\\&quot;&quot; &quot;5.3&quot; #&gt; [4] &quot;165cm&quot; &quot;6&quot; &quot;5&#39;7&quot; #&gt; [7] &quot;&gt;9000&quot; &quot;5&#39;7\\&quot;&quot; &quot;5&#39;3\\&quot;&quot; #&gt; [10] &quot;5.25&quot; &quot;5&#39;11&quot; &quot;5.5&quot; #&gt; [13] &quot;5&#39;9&#39;&#39;&quot; &quot;6&quot; &quot;6.5&quot; #&gt; [16] &quot;5&#39;10&#39;&#39;&quot; &quot;5.8&quot; &quot;5&quot; #&gt; [19] &quot;5.6&quot; &quot;5,3&quot; &quot;6&#39;&quot; #&gt; [22] &quot;6&quot; &quot;5.9&quot; &quot;6,8&quot; #&gt; [25] &quot;5&#39; 10&quot; &quot;5.5&quot; &quot;6.2&quot; #&gt; [28] &quot;Five foot eight inches&quot; &quot;6.2&quot; &quot;5.8&quot; #&gt; [31] &quot;5.1&quot; &quot;5.11&quot; &quot;5&#39;5\\&quot;&quot; #&gt; [34] &quot;5&#39;2\\&quot;&quot; &quot;5.75&quot; &quot;5,4&quot; #&gt; [37] &quot;7&quot; &quot;5.4&quot; &quot;6.1&quot; #&gt; [40] &quot;5&#39;3&quot; 13.2.1 Alternation | is the alternation operator that will choose between one or more possible values. In our case, we have indicated to detect if there is the word “feet”, but we also have “ft” and “foot” to refer to the same thing in our data. Thus, we can create the pattern “feet” or “ft” or “foot”. # Visualize the matches str_view(problematic_heights, &quot;feet|ft|foot&quot;, match=TRUE) #&gt; [10] │ 5 &lt;feet&gt; and 8.11 inches #&gt; [29] │ Five &lt;foot&gt; eight inches #&gt; [82] │ 5 &lt;feet&gt; 7inches #&gt; [124] │ 5&lt;ft&gt; 9 inches #&gt; [125] │ 5 &lt;ft&gt; 9 inches #&gt; [140] │ 5 &lt;feet&gt; 6 inches index &lt;- str_detect(problematic_heights, &quot;feet|ft|foot&quot;) problematic_heights[index] # Match #&gt; [1] &quot;5 feet and 8.11 inches&quot; &quot;Five foot eight inches&quot; &quot;5 feet 7inches&quot; #&gt; [4] &quot;5ft 9 inches&quot; &quot;5 ft 9 inches&quot; &quot;5 feet 6 inches&quot; In the same way we can find the variations for inches and other symbols that we can remove: index &lt;- str_detect(problematic_heights, &quot;inches|in|&#39;&#39;|\\&quot;|cm|and&quot;) problematic_heights[index] # Match #&gt; [1] &quot;5&#39; 4\\&quot;&quot; &quot;165cm&quot; &quot;5&#39;7\\&quot;&quot; #&gt; [4] &quot;5&#39;3\\&quot;&quot; &quot;5 feet and 8.11 inches&quot; &quot;5&#39;9&#39;&#39;&quot; #&gt; [7] &quot;5&#39;10&#39;&#39;&quot; &quot;Five foot eight inches&quot; &quot;5&#39;5\\&quot;&quot; #&gt; [10] &quot;5&#39;2\\&quot;&quot; &quot;5&#39;10&#39;&#39;&quot; &quot;5&#39;3&#39;&#39;&quot; #&gt; [13] &quot;5&#39;7&#39;&#39;&quot; &quot;5&#39;3\\&quot;&quot; &quot;5&#39;6&#39;&#39;&quot; #&gt; [16] &quot;5&#39;7.5&#39;&#39;&quot; &quot;5&#39;7.5&#39;&#39;&quot; &quot;5&#39;2\\&quot;&quot; #&gt; [19] &quot;5&#39; 7.78\\&quot;&quot; &quot;5 feet 7inches&quot; &quot;5&#39;8\\&quot;&quot; #&gt; [22] &quot;5&#39;11\\&quot;&quot; &quot;5&#39;7\\&quot;&quot; &quot;5&#39; 11\\&quot;&quot; #&gt; [25] &quot;6&#39;1\\&quot;&quot; &quot;69\\&quot;&quot; &quot;5&#39; 7\\&quot;&quot; #&gt; [28] &quot;5&#39;10&#39;&#39;&quot; &quot;5ft 9 inches&quot; &quot;5 ft 9 inches&quot; #&gt; [31] &quot;5&#39;11&#39;&#39;&quot; &quot;5&#39;8\\&quot;&quot; &quot;5 feet 6 inches&quot; #&gt; [34] &quot;5&#39;10&#39;&#39;&quot; &quot;6&#39;3\\&quot;&quot; &quot;5&#39;5&#39;&#39;&quot; #&gt; [37] &quot;5&#39;7\\&quot;&quot; &quot;6&#39;4\\&quot;&quot; &quot;170 cm&quot; In this case we have entered '' to detect those who entered that symbol to denote inches and \\\" in case they used double quotes. In this latter case we have used \\ so that it does not generate an error when interpreting as closing the string. We could already start replacing based on the detected patterns: problematic_heights &lt;- str_replace_all(problematic_heights, &quot;feet|ft|foot&quot;, &quot;&#39;&quot;) problematic_heights &lt;- str_replace_all(problematic_heights, &quot;inches|in|&#39;&#39;|\\&quot;|cm|and&quot;, &quot;&quot;) problematic_heights |&gt; head(30) #&gt; [1] &quot;6&quot; &quot;5&#39; 4&quot; &quot;5.3&quot; &quot;165&quot; #&gt; [5] &quot;6&quot; &quot;5&#39;7&quot; &quot;&gt;9000&quot; &quot;5&#39;7&quot; #&gt; [9] &quot;5&#39;3&quot; &quot;5 &#39; 8.11 &quot; &quot;5.25&quot; &quot;5&#39;11&quot; #&gt; [13] &quot;5.5&quot; &quot;5&#39;9&quot; &quot;6&quot; &quot;6.5&quot; #&gt; [17] &quot;5&#39;10&quot; &quot;5.8&quot; &quot;5&quot; &quot;5.6&quot; #&gt; [21] &quot;5,3&quot; &quot;6&#39;&quot; &quot;6&quot; &quot;5.9&quot; #&gt; [25] &quot;6,8&quot; &quot;5&#39; 10&quot; &quot;5.5&quot; &quot;6.2&quot; #&gt; [29] &quot;Five &#39; eight &quot; &quot;6.2&quot; As an additional effort, we could also look to solve that some people have written words instead of numbers. For this we create a function that replaces each word with a number and apply it to the vector: words_to_number &lt;- function(s){ str_to_lower(s) |&gt; str_replace_all(&quot;zero&quot;, &quot;0&quot;) |&gt; str_replace_all(&quot;one&quot;, &quot;1&quot;) |&gt; str_replace_all(&quot;two&quot;, &quot;2&quot;) |&gt; str_replace_all(&quot;three&quot;, &quot;3&quot;) |&gt; str_replace_all(&quot;four&quot;, &quot;4&quot;) |&gt; str_replace_all(&quot;five&quot;, &quot;5&quot;) |&gt; str_replace_all(&quot;six&quot;, &quot;6&quot;) |&gt; str_replace_all(&quot;seven&quot;, &quot;7&quot;) |&gt; str_replace_all(&quot;eight&quot;, &quot;8&quot;) |&gt; str_replace_all(&quot;nine&quot;, &quot;9&quot;) |&gt; str_replace_all(&quot;ten&quot;, &quot;10&quot;) |&gt; str_replace_all(&quot;eleven&quot;, &quot;11&quot;) } problematic_heights &lt;- words_to_number(problematic_heights) problematic_heights |&gt; head(30) #&gt; [1] &quot;6&quot; &quot;5&#39; 4&quot; &quot;5.3&quot; &quot;165&quot; &quot;6&quot; #&gt; [6] &quot;5&#39;7&quot; &quot;&gt;9000&quot; &quot;5&#39;7&quot; &quot;5&#39;3&quot; &quot;5 &#39; 8.11 &quot; #&gt; [11] &quot;5.25&quot; &quot;5&#39;11&quot; &quot;5.5&quot; &quot;5&#39;9&quot; &quot;6&quot; #&gt; [16] &quot;6.5&quot; &quot;5&#39;10&quot; &quot;5.8&quot; &quot;5&quot; &quot;5.6&quot; #&gt; [21] &quot;5,3&quot; &quot;6&#39;&quot; &quot;6&quot; &quot;5.9&quot; &quot;6,8&quot; #&gt; [26] &quot;5&#39; 10&quot; &quot;5.5&quot; &quot;6.2&quot; &quot;5 &#39; 8 &quot; &quot;6.2&quot; 13.2.2 Anchoring Now that it is more standardized we can start with regex with more generic characteristics. For example, there is a person who has entered 6'. It would be convenient to have everything in the form feet plus inches. With which we should have 6'0. To achieve this we have to create a regex according to this generic situation. We will use the symbol ^ to anchor our validation to “start with” and the symbol $ to match with the end of the string. Before replacing, let’s first see who matches. str_view(problematic_heights, &quot;^6&#39;$&quot;, match=TRUE) #&gt; [22] │ &lt;6&#39;&gt; This regex indicates that it starts with 6' and that the expression ends there. We could still make it more generic to address those who, in the future, write 5 inches (1.52m) or 6 inches (1.82m). For this we will use brackets and inside them we will put all the values that we will accept. index &lt;- str_detect(problematic_heights, &quot;^[56]&#39;$&quot;) problematic_heights[index] # Match #&gt; [1] &quot;6&#39;&quot; There is still only one result, but our regex is more generic now and we can already use it to replace. Before replacing in our vector we are going to do a test to learn how to create what we need from a pattern. test_vec &lt;- c(&quot;5&#39;&quot;, &quot;6&#39;&quot;) str_replace_all(test_vec, &quot;^([56])&#39;$&quot;, &quot;\\\\1&#39;0&quot;) #&gt; [1] &quot;5&#39;0&quot; &quot;6&#39;0&quot; We have placed between parentheses to indicate that what is inside is our first value and we use \\\\1 to refer to that first value. So we are indicating to write the first value, then a quote ', and then a zero 0. Now we are ready to apply to our entire vector. We are going to make the change to consider not only 5 and 6, but up to the value of 7 inches (2.1m). Likewise, we are going to take the cases in which there is only a number without the foot symbol '. problematic_heights &lt;- str_replace_all(problematic_heights, &quot;^([5-7])&#39;$&quot;, &quot;\\\\1&#39;0&quot;) problematic_heights &lt;- str_replace_all(problematic_heights, &quot;^([5-7])$&quot;, &quot;\\\\1&#39;0&quot;) problematic_heights |&gt; head(30) #&gt; [1] &quot;6&#39;0&quot; &quot;5&#39; 4&quot; &quot;5.3&quot; &quot;165&quot; &quot;6&#39;0&quot; #&gt; [6] &quot;5&#39;7&quot; &quot;&gt;9000&quot; &quot;5&#39;7&quot; &quot;5&#39;3&quot; &quot;5 &#39; 8.11 &quot; #&gt; [11] &quot;5.25&quot; &quot;5&#39;11&quot; &quot;5.5&quot; &quot;5&#39;9&quot; &quot;6&#39;0&quot; #&gt; [16] &quot;6.5&quot; &quot;5&#39;10&quot; &quot;5.8&quot; &quot;5&#39;0&quot; &quot;5.6&quot; #&gt; [21] &quot;5,3&quot; &quot;6&#39;0&quot; &quot;6&#39;0&quot; &quot;5.9&quot; &quot;6,8&quot; #&gt; [26] &quot;5&#39; 10&quot; &quot;5.5&quot; &quot;6.2&quot; &quot;5 &#39; 8 &quot; &quot;6.2&quot; 13.2.3 Repetitions We can control how many times a pattern matches using repetition operators: We can control how many times a pattern matches using repetition operators. The question mark ? indicates that the preceding element matches 0 or 1 time (making it optional). The plus sign + requires 1 or more matches, ensuring the element is present at least once. The asterisk * allows for 0 or more matches, meaning the element can be absent or repeated indefinitely. For example, to find all cases where instead of using the foot symbol ' they entered a comma, a period, or a space we will use the following pattern: pattern &lt;- &quot;^([4-7])\\\\s*[,\\\\.]\\\\s*(\\\\d*)$&quot; Let’s read the pattern: The string starts with a digit ranging from 4 to 7. \\\\s means that it is followed by a white space, but we use * to indicate that this character appears 0 or more times. After that space we will look for any of the following characters: ,, a period \\\\. (to which we put double backslash because the period alone in a pattern means “any value”). We use \\\\s* again to look for zero or more white spaces. Finally we indicate that the string ends there with a digit, to denote that look for any digit we use \\\\d, d for digit. And we add asterisk so that it keeps one or more digits that it finds. In summary: it starts with a number, then symbols and then a digit. Between the symbols there could be white spaces. That is our pattern. str_view(problematic_heights, pattern, match=TRUE) #&gt; [3] │ &lt;5.3&gt; #&gt; [11] │ &lt;5.25&gt; #&gt; [13] │ &lt;5.5&gt; #&gt; [16] │ &lt;6.5&gt; #&gt; [18] │ &lt;5.8&gt; #&gt; [20] │ &lt;5.6&gt; #&gt; [21] │ &lt;5,3&gt; #&gt; [24] │ &lt;5.9&gt; #&gt; [25] │ &lt;6,8&gt; #&gt; [27] │ &lt;5.5&gt; #&gt; [28] │ &lt;6.2&gt; #&gt; [30] │ &lt;6.2&gt; #&gt; [31] │ &lt;5.8&gt; #&gt; [32] │ &lt;5.1&gt; #&gt; [33] │ &lt;5.11&gt; #&gt; [36] │ &lt;5.75&gt; #&gt; [37] │ &lt;5,4&gt; #&gt; [39] │ &lt;5.4&gt; #&gt; [40] │ &lt;6.1&gt; #&gt; [42] │ &lt;5.6&gt; #&gt; ... and 48 more We already found the values that match the pattern, so we are ready to replace. problematic_heights &lt;- str_replace_all( problematic_heights, &quot;^([4-7])\\\\s*[,\\\\.]\\\\s*(\\\\d*)$&quot;, &quot;\\\\1.\\\\2&#39;0&quot; ) problematic_heights |&gt; head(30) #&gt; [1] &quot;6&#39;0&quot; &quot;5&#39; 4&quot; &quot;5.3&#39;0&quot; &quot;165&quot; &quot;6&#39;0&quot; #&gt; [6] &quot;5&#39;7&quot; &quot;&gt;9000&quot; &quot;5&#39;7&quot; &quot;5&#39;3&quot; &quot;5 &#39; 8.11 &quot; #&gt; [11] &quot;5.25&#39;0&quot; &quot;5&#39;11&quot; &quot;5.5&#39;0&quot; &quot;5&#39;9&quot; &quot;6&#39;0&quot; #&gt; [16] &quot;6.5&#39;0&quot; &quot;5&#39;10&quot; &quot;5.8&#39;0&quot; &quot;5&#39;0&quot; &quot;5.6&#39;0&quot; #&gt; [21] &quot;5.3&#39;0&quot; &quot;6&#39;0&quot; &quot;6&#39;0&quot; &quot;5.9&#39;0&quot; &quot;6.8&#39;0&quot; #&gt; [26] &quot;5&#39; 10&quot; &quot;5.5&#39;0&quot; &quot;6.2&#39;0&quot; &quot;5 &#39; 8 &quot; &quot;6.2&#39;0&quot; Another pattern we see now is when before or after the foot symbol ' there is a white space. Let’s make the change with what we learned and include cases where there are decimals: index &lt;- str_detect(problematic_heights, &quot;^([4-7]\\\\.?\\\\d*)\\\\s*&#39;\\\\s*(\\\\d+\\\\.?\\\\d*)\\\\s*$&quot;) problematic_heights[index] |&gt; # Match head(30) #&gt; [1] &quot;6&#39;0&quot; &quot;5&#39; 4&quot; &quot;5.3&#39;0&quot; &quot;6&#39;0&quot; &quot;5&#39;7&quot; #&gt; [6] &quot;5&#39;7&quot; &quot;5&#39;3&quot; &quot;5 &#39; 8.11 &quot; &quot;5.25&#39;0&quot; &quot;5&#39;11&quot; #&gt; [11] &quot;5.5&#39;0&quot; &quot;5&#39;9&quot; &quot;6&#39;0&quot; &quot;6.5&#39;0&quot; &quot;5&#39;10&quot; #&gt; [16] &quot;5.8&#39;0&quot; &quot;5&#39;0&quot; &quot;5.6&#39;0&quot; &quot;5.3&#39;0&quot; &quot;6&#39;0&quot; #&gt; [21] &quot;6&#39;0&quot; &quot;5.9&#39;0&quot; &quot;6.8&#39;0&quot; &quot;5&#39; 10&quot; &quot;5.5&#39;0&quot; #&gt; [26] &quot;6.2&#39;0&quot; &quot;5 &#39; 8 &quot; &quot;6.2&#39;0&quot; &quot;5.8&#39;0&quot; &quot;5.1&#39;0&quot; problematic_heights &lt;- str_replace_all( problematic_heights, &quot;^([4-7]\\\\.?\\\\d*)\\\\s*&#39;\\\\s*(\\\\d+\\\\.?\\\\d*)\\\\s*$&quot;, &quot;\\\\1&#39;\\\\2&quot; ) problematic_heights |&gt; head(30) #&gt; [1] &quot;6&#39;0&quot; &quot;5&#39;4&quot; &quot;5.3&#39;0&quot; &quot;165&quot; &quot;6&#39;0&quot; &quot;5&#39;7&quot; &quot;&gt;9000&quot; &quot;5&#39;7&quot; #&gt; [9] &quot;5&#39;3&quot; &quot;5&#39;8.11&quot; &quot;5.25&#39;0&quot; &quot;5&#39;11&quot; &quot;5.5&#39;0&quot; &quot;5&#39;9&quot; &quot;6&#39;0&quot; &quot;6.5&#39;0&quot; #&gt; [17] &quot;5&#39;10&quot; &quot;5.8&#39;0&quot; &quot;5&#39;0&quot; &quot;5.6&#39;0&quot; &quot;5.3&#39;0&quot; &quot;6&#39;0&quot; &quot;6&#39;0&quot; &quot;5.9&#39;0&quot; #&gt; [25] &quot;6.8&#39;0&quot; &quot;5&#39;10&quot; &quot;5.5&#39;0&quot; &quot;6.2&#39;0&quot; &quot;5&#39;8&quot; &quot;6.2&#39;0&quot; Likewise, we have the pattern in which they entered: feet + space + inches without any symbol. Let’s make the change with what we learned. index &lt;- str_detect(problematic_heights, &quot;^([4-7])\\\\s+(\\\\d*)\\\\s*$&quot;) problematic_heights[index] # Match #&gt; [1] &quot;5 11&quot; &quot;6 04&quot; problematic_heights &lt;- str_replace_all( problematic_heights, &quot;^([4-7])\\\\s+(\\\\d*)\\\\s*$&quot;, &quot;\\\\1&#39;\\\\2&quot; ) problematic_heights |&gt; head(30) #&gt; [1] &quot;6&#39;0&quot; &quot;5&#39;4&quot; &quot;5.3&#39;0&quot; &quot;165&quot; &quot;6&#39;0&quot; &quot;5&#39;7&quot; &quot;&gt;9000&quot; &quot;5&#39;7&quot; #&gt; [9] &quot;5&#39;3&quot; &quot;5&#39;8.11&quot; &quot;5.25&#39;0&quot; &quot;5&#39;11&quot; &quot;5.5&#39;0&quot; &quot;5&#39;9&quot; &quot;6&#39;0&quot; &quot;6.5&#39;0&quot; #&gt; [17] &quot;5&#39;10&quot; &quot;5.8&#39;0&quot; &quot;5&#39;0&quot; &quot;5.6&#39;0&quot; &quot;5.3&#39;0&quot; &quot;6&#39;0&quot; &quot;6&#39;0&quot; &quot;5.9&#39;0&quot; #&gt; [25] &quot;6.8&#39;0&quot; &quot;5&#39;10&quot; &quot;5.5&#39;0&quot; &quot;6.2&#39;0&quot; &quot;5&#39;8&quot; &quot;6.2&#39;0&quot; We are ready to put all the patterns together and the power of patterns is that they can serve us for future exercises. Thus, we will create a function where we will place each change that we can verify to a string. format_errors &lt;- function(string){ string |&gt; str_replace_all(&quot;feet|ft|foot&quot;, &quot;&#39;&quot;) |&gt; # Change feet for &#39; str_replace_all(&quot;inches|in|&#39;&#39;|\\&quot;|cm|and&quot;, &quot;&quot;) |&gt; # Remove symbols str_replace_all(&quot;^([5-7])&#39;$&quot;, &quot;\\\\1&#39;0&quot;) |&gt; # Adds 0 to 5&#39;, 6&#39; or 7&#39; str_replace_all(&quot;^([5-7])$&quot;, &quot;\\\\1&#39;0&quot;) |&gt; # Adds 0 to 5, 6 or 7 str_replace_all(&quot;^([4-7])\\\\s*[,\\\\.]\\\\s*(\\\\d*)$&quot;, &quot;\\\\1.\\\\2&#39;0&quot;) |&gt; # Change 5.3&#39; to 5.3&#39;0 str_replace_all(&quot;^([4-7]\\\\.?\\\\d*)\\\\s*&#39;\\\\s*(\\\\d+\\\\.?\\\\d*)\\\\s*$&quot;, &quot;\\\\1&#39;\\\\2&quot;) |&gt; #Removes spaces in middle str_replace_all(&quot;^([4-7])\\\\s+(\\\\d*)\\\\s*$&quot;, &quot;\\\\1&#39;\\\\2&quot;) |&gt; # Adds &#39; str_replace(&quot;^([12])\\\\s*,\\\\s*(\\\\d*)$&quot;, &quot;\\\\1.\\\\2&quot;) |&gt; # Changes decimals from commas to dots str_trim() #Removes spaces at start and end } Thus, we have created two functions that could be useful to us if we were to work with surveys of the same type again. Before applying it to our entire table let’s extract the values to a vector again to apply the created functions. problematic_heights &lt;- reported_heights |&gt; filter(is.na(as.numeric(height)) | # Does not convert to number (!is.na(as.numeric(height)) &amp; as.numeric(height) &gt;= 5 &amp; as.numeric(height) &lt;= 7 ) # or entered in feet and not inches ) |&gt; pull(height) Now let’s apply the created functions: formatted_heights &lt;- problematic_heights |&gt; words_to_number() |&gt; format_errors() pattern &lt;- &quot;^([4-7]\\\\.?\\\\d*)\\\\s*&#39;\\\\s*(\\\\d+\\\\.?\\\\d*)\\\\s*$&quot; index &lt;- str_detect(formatted_heights, pattern) formatted_heights[!index] # Do not match the pattern #&gt; [1] &quot;165&quot; &quot;&gt;9000&quot; &quot;2&#39;33&quot; &quot;1.70&quot; &quot;yyy&quot; &quot;6*12&quot; #&gt; [7] &quot;69&quot; &quot;708,661&quot; &quot;649,606&quot; &quot;728,346&quot; &quot;170&quot; &quot;7,283,465&quot; We have managed to reduce from 168 errors of 1095 records, 15.3% of errors, to 12 errors of 1095, 1% of errors. We can now apply to our initial table. # Apply created formulas heights &lt;- reported_heights |&gt; mutate(height) |&gt; mutate(height = words_to_number(height) |&gt; format_errors()) # Get random samples to validate quality random_indices &lt;- sample(1:nrow(heights)) heights[random_indices, ] |&gt; head(15) #&gt; time_stamp sex height #&gt; 155 2014-09-02 15:17:12 Female 63 #&gt; 426 2015-01-06 22:58:54 Male 5&#39;12 #&gt; 1029 2016-07-26 13:02:34 Male 67 #&gt; 326 2014-10-14 05:18:11 Male 71 #&gt; 789 2016-01-25 08:15:45 Female 5&#39;5 #&gt; 985 2016-04-23 17:15:26 Male 67.5 #&gt; 39 2014-09-02 15:16:31 Male 72 #&gt; 822 2016-01-25 21:18:33 Male 68 #&gt; 986 2016-04-25 06:11:45 Male 180 #&gt; 137 2014-09-02 15:17:02 Male 68 #&gt; 455 2015-01-28 03:59:44 Male 5.5&#39;0 #&gt; 589 2015-05-25 16:19:20 Male 69 #&gt; 1089 2017-09-04 07:28:40 Male 69 #&gt; 196 2014-09-02 15:18:30 Female 64.57 #&gt; 680 2015-09-01 22:45:11 Male 68 We still have to do some conversions. However, since they follow a determined pattern we can use the extract(source_column, new_columns, pattern, remove_source) function to confirm creating new columns for each value of our pattern. pattern &lt;- &quot;^([4-7]\\\\.?\\\\d*)\\\\s*&#39;\\\\s*(\\\\d+\\\\.?\\\\d*)\\\\s*$&quot; heights |&gt; extract(height, c(&quot;feet&quot;, &quot;inches&quot;), regex = pattern, remove = FALSE) |&gt; head(15) #&gt; time_stamp sex height feet inches #&gt; 1 2014-09-02 13:40:36 Male 75 &lt;NA&gt; &lt;NA&gt; #&gt; 2 2014-09-02 13:46:59 Male 70 &lt;NA&gt; &lt;NA&gt; #&gt; 3 2014-09-02 13:59:20 Male 68 &lt;NA&gt; &lt;NA&gt; #&gt; 4 2014-09-02 14:51:53 Male 74 &lt;NA&gt; &lt;NA&gt; #&gt; 5 2014-09-02 15:16:15 Male 61 &lt;NA&gt; &lt;NA&gt; #&gt; 6 2014-09-02 15:16:16 Female 65 &lt;NA&gt; &lt;NA&gt; #&gt; 7 2014-09-02 15:16:19 Female 66 &lt;NA&gt; &lt;NA&gt; #&gt; 8 2014-09-02 15:16:21 Female 62 &lt;NA&gt; &lt;NA&gt; #&gt; 9 2014-09-02 15:16:21 Female 66 &lt;NA&gt; &lt;NA&gt; #&gt; 10 2014-09-02 15:16:22 Male 67 &lt;NA&gt; &lt;NA&gt; #&gt; 11 2014-09-02 15:16:22 Male 72 &lt;NA&gt; &lt;NA&gt; #&gt; 12 2014-09-02 15:16:23 Male 6&#39;0 6 0 #&gt; 13 2014-09-02 15:16:23 Male 69 &lt;NA&gt; &lt;NA&gt; #&gt; 14 2014-09-02 15:16:26 Male 68 &lt;NA&gt; &lt;NA&gt; #&gt; 15 2014-09-02 15:16:26 Male 69 &lt;NA&gt; &lt;NA&gt; Now that we have the data that matches the pattern in two other columns, and we know they are numbers, we can convert everything to number. heights |&gt; extract(height, c(&quot;feet&quot;, &quot;inches&quot;), regex = pattern, remove = FALSE) |&gt; mutate(across(c(&quot;height&quot;, &quot;feet&quot;, &quot;inches&quot;), ~as.numeric(.))) |&gt; head(15) #&gt; Warning: There was 1 warning in `mutate()`. #&gt; ℹ In argument: `across(c(&quot;height&quot;, &quot;feet&quot;, &quot;inches&quot;), #&gt; ~as.numeric(.))`. #&gt; Caused by warning: #&gt; ! NAs introduced by coercion #&gt; time_stamp sex height feet inches #&gt; 1 2014-09-02 13:40:36 Male 75 NA NA #&gt; 2 2014-09-02 13:46:59 Male 70 NA NA #&gt; 3 2014-09-02 13:59:20 Male 68 NA NA #&gt; 4 2014-09-02 14:51:53 Male 74 NA NA #&gt; 5 2014-09-02 15:16:15 Male 61 NA NA #&gt; 6 2014-09-02 15:16:16 Female 65 NA NA #&gt; 7 2014-09-02 15:16:19 Female 66 NA NA #&gt; 8 2014-09-02 15:16:21 Female 62 NA NA #&gt; 9 2014-09-02 15:16:21 Female 66 NA NA #&gt; 10 2014-09-02 15:16:22 Male 67 NA NA #&gt; 11 2014-09-02 15:16:22 Male 72 NA NA #&gt; 12 2014-09-02 15:16:23 Male NA 6 0 #&gt; 13 2014-09-02 15:16:23 Male 69 NA NA #&gt; 14 2014-09-02 15:16:26 Male 68 NA NA #&gt; 15 2014-09-02 15:16:26 Male 69 NA NA Now that our columns are numeric we can perform operations to calculate height. heights |&gt; extract(height, c(&quot;feet&quot;, &quot;inches&quot;), regex = pattern, remove = FALSE) |&gt; mutate(across(c(&quot;height&quot;, &quot;feet&quot;, &quot;inches&quot;), ~as.numeric(.))) |&gt; mutate(fixed_heights = feet*12 + inches) |&gt; head(15) #&gt; Warning: There was 1 warning in `mutate()`. #&gt; ℹ In argument: `across(c(&quot;height&quot;, &quot;feet&quot;, &quot;inches&quot;), #&gt; ~as.numeric(.))`. #&gt; Caused by warning: #&gt; ! NAs introduced by coercion #&gt; time_stamp sex height feet inches fixed_heights #&gt; 1 2014-09-02 13:40:36 Male 75 NA NA NA #&gt; 2 2014-09-02 13:46:59 Male 70 NA NA NA #&gt; 3 2014-09-02 13:59:20 Male 68 NA NA NA #&gt; 4 2014-09-02 14:51:53 Male 74 NA NA NA #&gt; 5 2014-09-02 15:16:15 Male 61 NA NA NA #&gt; 6 2014-09-02 15:16:16 Female 65 NA NA NA #&gt; 7 2014-09-02 15:16:19 Female 66 NA NA NA #&gt; 8 2014-09-02 15:16:21 Female 62 NA NA NA #&gt; 9 2014-09-02 15:16:21 Female 66 NA NA NA #&gt; 10 2014-09-02 15:16:22 Male 67 NA NA NA #&gt; 11 2014-09-02 15:16:22 Male 72 NA NA NA #&gt; 12 2014-09-02 15:16:23 Male NA 6 0 72 #&gt; 13 2014-09-02 15:16:23 Male 69 NA NA NA #&gt; 14 2014-09-02 15:16:26 Male 68 NA NA NA #&gt; 15 2014-09-02 15:16:26 Male 69 NA NA NA Finally, we will do a validation of whether the height is in an interval and/or if it was expressed in centimeters or meters. # We assume for a person a minimum 50&quot; (1.2m) and max 84&quot; (2.1m) min &lt;- 50 max &lt;- 84 heights &lt;- heights |&gt; extract(height, c(&quot;feet&quot;, &quot;inches&quot;), regex = pattern, remove = FALSE) |&gt; mutate(across(c(&quot;height&quot;, &quot;feet&quot;, &quot;inches&quot;), ~as.numeric(.))) |&gt; mutate(fixed_heights = feet*12 + inches) |&gt; mutate(final_height = case_when( !is.na(height) &amp; between(height, min, max) ~ height, #inches !is.na(height) &amp; between(height/2.54, min, max) ~ height/2.54, #cm !is.na(height) &amp; between(height*100/2.54, min, max) ~ height*100/2.54, #meters !is.na(fixed_heights) &amp; inches &lt; 12 &amp; between(fixed_heights, min, max) ~ fixed_heights, #feet&#39;inches TRUE ~ as.numeric(NA))) #&gt; Warning: There was 1 warning in `mutate()`. #&gt; ℹ In argument: `across(c(&quot;height&quot;, &quot;feet&quot;, &quot;inches&quot;), #&gt; ~as.numeric(.))`. #&gt; Caused by warning: #&gt; ! NAs introduced by coercion # Random Sample: random_indices &lt;- sample(1:nrow(heights)) heights[random_indices, ] |&gt; select(-time_stamp) |&gt; # Shows all columns except time_stamp head(10) #&gt; sex height feet inches fixed_heights final_height #&gt; 201 Female 67.00000 NA NA NA 67.00000 #&gt; 1006 Male 68.11024 NA NA NA 68.11024 #&gt; 651 Male 70.00000 NA NA NA 70.00000 #&gt; 545 Male 68.00000 NA NA NA 68.00000 #&gt; 617 Male 69.00000 NA NA NA 69.00000 #&gt; 17 Male 75.00000 NA NA NA 75.00000 #&gt; 102 Female 71.00000 NA NA NA 71.00000 #&gt; 71 Male 73.00000 NA NA NA 73.00000 #&gt; 80 Female 72.00000 NA NA NA 72.00000 #&gt; 643 Male 72.00000 NA NA NA 72.00000 We already have our sample validated, we would only have to take the columns we need and start using the object for the analyses we need. final_heights &lt;- heights |&gt; select(gender = sex, heights = final_height) final_heights |&gt; head(10) #&gt; gender heights #&gt; 1 Male 75 #&gt; 2 Male 70 #&gt; 3 Male 68 #&gt; 4 Male 74 #&gt; 5 Male 61 #&gt; 6 Female 65 #&gt; 7 Female 66 #&gt; 8 Female 62 #&gt; 9 Female 66 #&gt; 10 Male 67 13.3 From strings to dates Regularly when we import data, we are not only going to want to transform numeric data. We will also have multiple cases where we need to transform our string to a date in some particular format. For this, we will use the lubridate library, included in tidyverse, which provides us with diverse functions to make date treatment more accessible. library(lubridate) When the text string is in the ISO 8601 date format (YYYY-MM-DD), we can directly use the month(), day(), year() function. dates_char &lt;- c(&quot;2010-05-19&quot;, &quot;2020-05-06&quot;, &quot;2010-02-03&quot;) str(dates_char) #&gt; chr [1:3] &quot;2010-05-19&quot; &quot;2020-05-06&quot; &quot;2010-02-03&quot; month(dates_char) #&gt; [1] 5 5 2 However, we do not always have the date in that format and lubridate() gives other functions that are more flexible when coercing data. Look at this example: dates &lt;- c(20090101, &quot;2009-01-02&quot;, &quot;2009 01 03&quot;, &quot;2009-1-4&quot;, &quot;2009-1, 5&quot;, &quot;Created on 2009 1 6&quot;, &quot;200901 !!! 07&quot;) str(dates) #&gt; chr [1:7] &quot;20090101&quot; &quot;2009-01-02&quot; &quot;2009 01 03&quot; &quot;2009-1-4&quot; &quot;2009-1, 5&quot; ... ymd(dates) #&gt; [1] &quot;2009-01-01&quot; &quot;2009-01-02&quot; &quot;2009-01-03&quot; &quot;2009-01-04&quot; &quot;2009-01-05&quot; #&gt; [6] &quot;2009-01-06&quot; &quot;2009-01-07&quot; The first data entered was a number, but we already know that it coerces it to text. Then, we have different values entered, but all follow the same pattern. First is the year, then the month and then the day. When we know that first is the year, then month and then day we will use the ymd() function to convert all dates to ISO 8601 format. In the same way, we will have the following functions that we can use depending on the form in which we have the date from our source. In all cases it will be convenient for us to convert to ISO 8601 format. For example here we can see when it correctly recognizes the format and when the formatting fails. x &lt;- &quot;28/03/89&quot; ymd(x) #&gt; [1] NA mdy(x) #&gt; [1] NA ydm(x) #&gt; [1] NA myd(x) #&gt; [1] NA dmy(x) #&gt; [1] &quot;1989-03-28&quot; dym(x) #&gt; [1] NA Finally, in the same way that we can use these functions of days, months and years, we can also use to refer to hours, minutes and seconds. # Format with hours, minutes and seconds date_val &lt;- &quot;Feb/2/2012 12:34:56&quot; mdy_hms(date_val) #&gt; [1] &quot;2012-02-02 12:34:56 UTC&quot; # Additional data: Showing system date: now() #&gt; [1] &quot;2025-12-25 01:33:58 GMT&quot; 13.4 Exercises Before solving the following exercise run this Script: sales &lt;- tibble( month = c(&quot;April&quot;, &quot;May&quot;, &quot;June&quot;), revenue = c(&quot;s/32,124&quot;, &quot;s/35,465&quot;, &quot;S/38,332&quot;), profit = c(&quot;s/8,120&quot;, &quot;s/9,432&quot;, &quot;s/10,543&quot;) ) Convert the revenue and profit columns in the sales object to numeric values, removing any currency symbols or formatting characters. Solution # Solution 1 sales |&gt; mutate(across(c(2,3), ~parse_number(.))) # Alternative solution, longer sales |&gt; mutate(across(c(2,3), ~str_replace_all(., &quot;\\\\S/|,&quot;, &quot;&quot;))) |&gt; mutate(across(c(2,3), ~as.numeric(.))) Clean the universities vector so that all university names are standardized. Specifically, replace abbreviations like “Univ.” or “U.” at the beginning of the string with the full word “University”. Solution universities |&gt; str_replace(&quot;^Univ\\\\.?\\\\s|^U\\\\.?\\\\s&quot;, &quot;University &quot;) For the following exercises, we are going to work on the survey data conducted prior to Brexit in the UK. Run the Script first: library(rvest) library(tidyverse) url &lt;- &quot;https://en.wikipedia.org/w/index.php?title=Opinion_polling_for_the_United_Kingdom_European_Union_membership_referendum&amp;oldid=896735054&quot; table_html &lt;- read_html(url) |&gt; html_nodes(&quot;table&quot;) polls &lt;- table_html[[5]] |&gt; html_table(fill = TRUE) Update the polls object by renaming columns to c(\"date\", \"remain\", \"leave\", \"undecided\", \"spread\", \"sample\", \"pollster\", \"type\", \"notes\"). Then, filter the dataset to retain only rows where the remain column contains a percentage symbol (“%”). Solution names(polls) &lt;- c(&quot;date&quot;, &quot;remain&quot;, &quot;leave&quot;, &quot;undecided&quot;, &quot;spread&quot;, &quot;sample&quot;, &quot;pollster&quot;, &quot;type&quot;, &quot;notes&quot;) polls &lt;- polls[str_detect(polls$remain, &quot;%&quot;), ] polls # If we want to validate the number of polls: nrow(polls) Extract the remain column into a vector and convert the text percentages into proper numeric probabilities (e.g., convert “50%” to 0.5). Solution remain &lt;- polls$remain # Solution 1: percentages &lt;- parse_number(remain)/100 # Solution 1: temp &lt;- str_replace(remain, &quot;%&quot;, &quot;&quot;) percentages &lt;- as.numeric(temp)/100 # Solution 2: temp &lt;- str_remove(remain, &quot;%&quot;) percentages &lt;- as.numeric(temp)/100 In the undecided column, the value “N/A” appears when the sum of remain and leave equals 100%. Create a vector for undecided where these “N/A” values are replaced with “0%”. Solution undecided &lt;- polls$undecided str_replace(undecided, &quot;N/A&quot;, &quot;0%&quot;) encapsulate your cleaning logic into a single function named format_percentage(string). Test this function with the vector c(\"13.5%\", \"N/A\", \"10%\") to verify it handles both percentages and “N/A” values correctly. Solution format_percentage &lt;- function(string){ string |&gt; str_replace(&quot;N/A&quot;, &quot;0%&quot;) |&gt; parse_number()/100 } # Function test: test_vec &lt;- c(&quot;13.5%&quot;, &quot;N/A&quot;, &quot;10%&quot;) format_percentage(test_vec) Apply format_percentage to the remain, leave, undecided, and spread columns in the polls dataset. Also, ensure the sample column is converted to a numeric type. Solution polls &lt;- polls |&gt; mutate(across(c(&quot;remain&quot;, &quot;leave&quot;, &quot;undecided&quot;, &quot;spread&quot;), ~format_percentage(.))) |&gt; mutate(across(c(&quot;sample&quot;), ~parse_number(.))) Import the Peruvian COVID-19 dataset from this URL into an object named covid_peru. Convert the birth date column (FECHA_NACIMIENTO) to a proper Date format and calculate the age distribution of the infected individuals using a histogram. Solution url &lt;- &quot;https://www.datosabiertos.gob.pe/sites/default/files/DATOSABIERTOS_SISCOVID.csv&quot; covid_peru &lt;- read_csv(url) # We look for those that do not follow the ISO 8601 standard: index &lt;- str_detect(covid_peru$FECHA_NACIMIENTO, &quot;\\\\d{4}-\\\\d{2}-\\\\d{2}&quot;) covid_peru$FECHA_NACIMIENTO[!index] # We see dates in DD/MM/YYYY format # We replace to ISO 8601 format: covid_peru &lt;- covid_peru |&gt; mutate(across(&quot;FECHA_NACIMIENTO&quot;, ~str_replace(., &quot;(\\\\d{2})/(\\\\d{2})/(\\\\d{4})&quot;, &quot;\\\\3-\\\\2-\\\\1&quot;) )) # We search again for those that do not follow ISO 8601 standard: index &lt;- str_detect(covid_peru$FECHA_NACIMIENTO, &quot;\\\\d{4}-\\\\d{2}-\\\\d{2}&quot;) covid_peru$FECHA_NACIMIENTO[!index] # Convert column to date: covid_peru &lt;- covid_peru |&gt; mutate(across(&quot;FECHA_NACIMIENTO&quot;, ~ymd(.))) # Now that it is date format we create histogram: covid_peru |&gt; mutate(age = year(now()) - year(FECHA_NACIMIENTO)) |&gt; pull(age) |&gt; hist() 13.5 Text Mining using Tidy Data Text mining is the discovery by computer of new information, previously unknown, by automatically extracting information from different written resources. Written resources can be websites, books, chats, comments, emails, reviews, articles, etc. To perform text mining efficiently in R, we will use the tidytext package. The “tidy” text format is defined as a table with one token per row. A token can be a word, a sentence, or a paragraph, but usually, it is single words. This structure allows us to use all the standard tools we’ve learned (dplyr, ggplot2) to analyze text. # Install packages if you haven&#39;t yet # install.packages(&quot;tidytext&quot;) library(tidytext) library(tidyverse) library(stringr) library(syuzhet) # For sentiment analysis library(wordcloud) 13.5.1 Importing data and Tokenization Word maps or word clouds allow us to quickly identify which are the words that are repeated most in a text. We are going to analyze the work “Pride and Prejudice” written by the author Jane Austen. We will obtain the text from the Project Gutenberg12 website. We will use the get_text_as_string() function from syuzhet to import properly. url &lt;- &quot;https://www.gutenberg.org/cache/epub/1342/pg1342.txt&quot; # Import text as a single string pride_book &lt;- get_text_as_string(url) # Convert to a data frame with sentences or just lines # Here we will split by newline to create a rudimentary structure text_df &lt;- tibble( text = str_split(pride_book, &quot;\\n&quot;)[[1]] ) # Remove empty lines text_df &lt;- text_df |&gt; filter(text != &quot;&quot;) head(text_df) #&gt; # A tibble: 1 × 1 #&gt; text #&gt; &lt;chr&gt; #&gt; 1 &quot;The Project Gutenberg eBook of Pride and Prejudice This ebook is for th… 13.5.2 Text cleaning and Tokenization Now we will clean the text and convert it to specific tokens (words). The unnest_tokens() function automatically: 1. Splits text into tokens (words by default). 2. Removes punctuation. 3. Converts to lowercase. Note on AI: This process of breaking text into “tokens” is exactly how Large Language Models like GPT-4 work. In Chapter 14 (Data Science in the Age of AI), we will see that LLMs are essentially probabilistic engines that predict the next token in a sequence. Understanding how to handle tokens here is the foundation for understanding Generative AI. # We eliminate first rows of notes/prologue if needed, though unnest_tokens handles a lot. # Let&#39;s clean some metadata lines roughly start_line &lt;- 115 text_df &lt;- text_df[start_line:nrow(text_df), ] # Tokenize tidy_pride &lt;- text_df |&gt; unnest_tokens(word, text) # See the result head(tidy_pride) #&gt; # A tibble: 6 × 1 #&gt; word #&gt; &lt;chr&gt; #&gt; 1 &lt;NA&gt; #&gt; 2 &lt;NA&gt; #&gt; 3 &lt;NA&gt; #&gt; 4 &lt;NA&gt; #&gt; 5 &lt;NA&gt; #&gt; 6 &lt;NA&gt; Now we have a table where each row is a word. This is the “tidy” format. However, we clearly have words that do not add meaning (stop words), such as “the”, “and”, “of”. We can remove them using a list of stop words. The tm package provides a good list for English. library(tm) english_stop_words &lt;- tibble(word = stopwords(&quot;english&quot;)) # Remove stop words using anti_join tidy_pride_clean &lt;- tidy_pride |&gt; anti_join(english_stop_words, by = &quot;word&quot;) head(tidy_pride_clean) #&gt; # A tibble: 6 × 1 #&gt; word #&gt; &lt;chr&gt; #&gt; 1 &lt;NA&gt; #&gt; 2 &lt;NA&gt; #&gt; 3 &lt;NA&gt; #&gt; 4 &lt;NA&gt; #&gt; 5 &lt;NA&gt; #&gt; 6 &lt;NA&gt; We might also want to remove custom words or numbers that appeared in the extraction. custom_stop_words &lt;- tibble(word = c(&quot;mr&quot;, &quot;mrs&quot;, &quot;miss&quot;, &quot;said&quot;, &quot;will&quot;, &quot;one&quot;, &quot;much&quot;, &quot;may&quot;, &quot;can&quot;, &quot;now&quot;, &quot;sir&quot;, &quot;lady&quot;)) tidy_pride_clean &lt;- tidy_pride_clean |&gt; anti_join(custom_stop_words, by = &quot;word&quot;) |&gt; filter(!str_detect(word, &quot;^\\\\d+$&quot;)) # Remove pure numbers 13.5.3 Word Cloud Now that we have our clean data, calculating word frequency is as simple as using count(). word_counts &lt;- tidy_pride_clean |&gt; count(word, sort = TRUE) head(word_counts) #&gt; # A tibble: 6 × 2 #&gt; word n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 elizabeth 605 #&gt; 2 darcy 383 #&gt; 3 must 322 #&gt; 4 bennet 309 #&gt; 5 jane 274 #&gt; 6 bingley 262 We can create the word cloud directly from this data frame. wordcloud(words = word_counts$word, freq = word_counts$n, min.freq = 5, max.words = 80, random.order = FALSE, colors = brewer.pal(name = &quot;Dark2&quot;, n = 8)) 13.5.4 Word Frequency Plot Since we have the data in a tidy format, plotting a bar chart of the most frequent words is straightforward with ggplot2. word_counts |&gt; head(20) |&gt; ggplot(aes(n, reorder(word, n))) + geom_col(fill = &quot;blue&quot;) + labs(y = NULL, x = &quot;Frequency&quot;, title = &quot;Most common words in Pride and Prejudice&quot;) 13.6 Sentiment Analysis Sentiment analysis allows us to know the tone of the messages. We will use the syuzhet package combined with our tidy data skills. Let’s use the same example of tweets. library(readxl) # Download tweets url &lt;- &quot;https://dparedesi.github.io/DS-with-R-datasets/rmapalacios-tweets.xlsx&quot; temp_file &lt;- tempfile() download.file(url, temp_file) posts &lt;- read_excel(temp_file) file.remove(temp_file) #&gt; [1] TRUE # Filter for tweets only and create a tidy dataframe tweets_df &lt;- posts |&gt; filter(`Tweet Type` == &quot;Tweet&quot;) |&gt; select(text = Text) |&gt; mutate(tweet_id = row_number()) head(tweets_df) #&gt; # A tibble: 6 × 2 #&gt; text tweet_id #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 &quot;Le agradezco mucho regidor.\\nUna visita al niño y a su madre pueden… 1 #&gt; 2 &quot;Esto esta prohibido en tantas normas que no se por donde empezar.\\n… 2 #&gt; 3 &quot;Nadie lo sabe y a los ministros sectoriales parece importarles poco… 3 #&gt; 4 &quot;Ahora se llama \\&quot;trabajo remoto\\&quot; con el auspicio del Estado peruan… 4 #&gt; 5 &quot;¿Y usted esta muy seguro que va a salir a trabajar el lunes 25? Vay… 5 #&gt; 6 &quot;No saben como abundan. https://t.co/r1qMGOhGcR&quot; 6 Now we clean the tweets. unnest_tokens handles most of it, but for tweets, we might want to remove URL links first. # Custom cleaning function for tweets before tokenization clean_tweets &lt;- tweets_df |&gt; mutate(text = str_replace_all(text, &quot;http\\\\S+&quot;, &quot;&quot;)) |&gt; # remove URLs mutate(text = str_replace_all(text, &quot;@\\\\S+&quot;, &quot;&quot;)) # remove mentions # Get Sentiment scores for each tweet # syuzhet works well with the full text vector for scoring tweet_sentiments &lt;- get_nrc_sentiment(clean_tweets$text, language = &quot;spanish&quot;) # Combine with original data tweets_with_sentiment &lt;- bind_cols(clean_tweets, tweet_sentiments) head(tweets_with_sentiment) #&gt; # A tibble: 6 × 12 #&gt; text tweet_id anger anticipation disgust fear joy sadness surprise trust #&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 &quot;Le ag… 1 0 1 0 2 1 3 0 3 #&gt; 2 &quot;Esto … 2 4 0 3 5 0 3 0 0 #&gt; 3 &quot;Nadie… 3 0 0 0 0 0 0 0 0 #&gt; 4 &quot;Ahora… 4 0 1 0 0 1 0 1 1 #&gt; 5 &quot;¿Y us… 5 0 2 0 1 3 2 0 4 #&gt; 6 &quot;No sa… 6 0 0 0 0 0 0 0 0 #&gt; # ℹ 2 more variables: negative &lt;dbl&gt;, positive &lt;dbl&gt; We can now reshape this data to visualize emotions using pivot_longer, just like we do with any tidy dataset. translate_emotions &lt;- function(string){ case_when( string == &quot;anger&quot; ~ &quot;Anger&quot;, string == &quot;anticipation&quot; ~ &quot;Anticipation&quot;, string == &quot;disgust&quot; ~ &quot;Disgust&quot;, string == &quot;fear&quot; ~ &quot;Fear&quot;, string == &quot;joy&quot; ~ &quot;Joy&quot;, string == &quot;sadness&quot; ~ &quot;Sadness&quot;, string == &quot;surprise&quot; ~ &quot;Surprise&quot;, string == &quot;trust&quot; ~ &quot;Trust&quot;, string == &quot;negative&quot; ~ &quot;Negative&quot;, string == &quot;positive&quot; ~ &quot;Positive&quot;, TRUE ~ string ) } # Summarize totals sentiment_totals &lt;- tweets_with_sentiment |&gt; summarise(across(anger:positive, sum)) |&gt; pivot_longer(cols = everything(), names_to = &quot;sentiment&quot;, values_to = &quot;total&quot;) |&gt; mutate(sentiment = translate_emotions(sentiment)) sentiment_totals #&gt; # A tibble: 10 × 2 #&gt; sentiment total #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Anger 805 #&gt; 2 Anticipation 905 #&gt; 3 Disgust 807 #&gt; 4 Fear 1344 #&gt; 5 Joy 549 #&gt; 6 Sadness 1378 #&gt; 7 Surprise 421 #&gt; 8 Trust 1373 #&gt; 9 Negative 2535 #&gt; 10 Positive 2314 Visualizing: # Separate positive/negative from specific emotions general_sentiments &lt;- c(&quot;Positive&quot;, &quot;Negative&quot;) sentiment_totals |&gt; filter(!sentiment %in% general_sentiments) |&gt; ggplot(aes(reorder(sentiment, total), total, fill = sentiment)) + geom_col() + coord_flip() + labs(x = NULL, y = &quot;Total Score&quot;, title = &quot;Emotions in Tweets&quot;) + theme(legend.position = &quot;none&quot;) sentiment_totals |&gt; filter(sentiment %in% general_sentiments) |&gt; ggplot(aes(sentiment, total, fill = sentiment)) + geom_col() + labs(x = NULL, y = &quot;Total Score&quot;, title = &quot;Positive vs Negative Sentiment&quot;) This tidy approach makes it much easier to inspect the data at every step and integrate valid data science workflows (filtering, joining, plotting) without learning a separate system just for text. 13.7 Exercises For these exercises we will use more books from Project Gutenberg using the gutenbergr library. # install.packages(&quot;gutenbergr&quot;) library(gutenbergr) # Tibble: list of books in Gutenberg.org gutenberg_metadata # List of books in Spanish gutenberg_works(languages = &quot;es&quot;) Use gutenberg_download(2000) to download the text of “El ingenioso hidalgo don Quijote de la Mancha” and store the result in an object named download. Solution download &lt;- gutenberg_download(2000) quijote_text &lt;- download$text head(quijote_text) Extract a random sample of 1,000 lines from the text. Clean this sample by tokenizing into words and removing standard Spanish stop words. Solution set.seed(123) sample_lines &lt;- tibble(text = sample(quijote_text, 1000)) tidy_quijote &lt;- sample_lines |&gt; unnest_tokens(word, text) |&gt; anti_join(spanish_stop_words, by = &quot;word&quot;) |&gt; # Remove extra stop words if needed filter(!word %in% c(&quot;don&quot;, &quot;quijote&quot;, &quot;sancho&quot;)) Visualize the most frequent words in your cleaned Quijote sample using a word cloud. Solution quijote_counts &lt;- tidy_quijote |&gt; count(word, sort = TRUE) wordcloud(words = quijote_counts$word, freq = quijote_counts$n, min.freq = 2, max.words = 80, colors = brewer.pal(8, &quot;Dark2&quot;)) Analyze the sentiments present in your text sample to determine the overall emotional tone. Solution # Reconstruct text for syuzhet or do word-by-word sentiment if using tidytext lexicon # Using syuzhet on the original sample lines is often better for context, # but let&#39;s try token-based simply for the exercise or just use the lines: # Extract sentiments from the lines quijote_sentiments &lt;- get_nrc_sentiment(sample_lines$text, language = &quot;spanish&quot;) # Summarize/Plot quijote_sentiments |&gt; summarise(across(everything(), sum)) |&gt; pivot_longer(everything(), names_to = &quot;sentiment&quot;, values_to = &quot;count&quot;) |&gt; ggplot(aes(reorder(sentiment, count), count)) + geom_col(fill = &quot;steelblue&quot;) + coord_flip() + labs(title = &quot;Sentiments in Don Quijote Sample&quot;) https://stringr.tidyverse.org/articles/regular-expressions.html↩︎ www.gutenberg.org↩︎ "],["genai-intro.html", "Chapter 14 Data Science in the Age of AI 14.1 What is a Large Language Model? 14.2 Setting Up Your AI Environment 14.3 AI as the “Pair Programmer” 14.4 The Risks: Hallucinations 14.5 Building a Robust Request 14.6 The Holy Grail: Structured Data extraction 14.7 Batch Processing: The purrr Workflow 14.8 Summary 14.9 Beyond Generation: Embeddings", " Chapter 14 Data Science in the Age of AI The field of Data Science is in a state of constant evolution. We started by learning how to handle vectors and lists in Base R, we moved to the elegance of the tidyverse for data manipulation, and we explored the robustness of tidymodels for machine learning. Now, we are facing a new paradigm shift: Generative AI. Just as the calculator did not replace the mathematician, Large Language Models (LLMs) will not replace the Data Scientist. However, a Data Scientist using AI will likely replace one who does not. In this part of the book, we will demystify these “magic black boxes”. We will learn what they are, how to control them programmatically from R, and how to use them to unlock unstructured data that was previously inaccessible. 14.1 What is a Large Language Model? To work effectively with LLMs, we must stop treating them as “people” and start treating them as probabilistic engines. 14.1.1 It’s all about Probability At its most fundamental level, an LLM like GPT-4, Claude, or Llama is a “next token prediction machine”. It has been trained on a massive corpus of text (books, websites, code repositories) to answer a simple statistical question: Given the sequence of text “The capital of France is…”, what is the most likely next piece of text? The model does not “know” geography. It knows that, statistically, the token “Paris” appears more frequently after that sequence than “London” or “Potato”. 14.1.2 Tokens vs. Words We often think models read words, but they actually process tokens, which can be whole words, fragments, or even spaces. For instance, “apple” might be a single token, while a complex word like “antidisestablishmentarianism” could be split into four or five. A useful rule of thumb is that 1,000 tokens are roughly equivalent to 750 words. This distinction is critical for two reasons: Cost, as you are billed by the token for both input and output; and Context Window, which serves as the model’s short-term memory. A model with a 128k context window can effectively “remember” about 96,000 words of conversation before it begins to lose track of the beginning. 14.1.3 Temperature: Controlling Creativity One of the most important parameters you can control is Temperature, which dictates the randomness of the output. A temperature of 0 makes the model deterministic, always selecting the most probable next token—ideal for tasks requiring precision like data extraction, coding, or math. Conversely, raising the temperature to 1 or higher encourages the model to take risks and choose less likely tokens, making it suitable for creative writing, brainstorming, and poetry. [!TIP] For Data Science, start at 0. When writing code or extracting data, we want reliability, not creativity. 14.2 Setting Up Your AI Environment Before we write code, we must secure our environment. Accessing high-quality models usually requires an API Key (from OpenAI, Anthropic, Google, etc.). [!DANGER] NEVER paste your API key directly into your R script. If you push that script to GitHub, bots will steal your key in seconds and drain your bank account. * Anonymize: If you must use a public tool, rename columns (Client_A, Revenue_X) and inject fake values before prompting. 14.2.1 The Solution: Local LLMs For sensitive data, the best solution is running a Local LLM on your own machine using tools like Ollama or LM Studio. This approach ensures 100% privacy and offline access, though it does come with trade-offs: it requires a capable computer (such as a Mac M-series or NVIDIA GPU), and local models are typically smaller and less capable than massive cloud models like GPT-4. 14.2.2 The .Renviron File The standard way to handle secrets in R is the .Renviron file. This file lives in your project’s root or your home directory and is not tracked by Git (ensure it is in your .gitignore). Open or create the file using R: usethis::edit_r_environ() Add your keys in the following format: OPENAI_API_KEY=&quot;sk-proj-12345...&quot; ANTHROPIC_API_KEY=&quot;sk-ant-12345...&quot; GITHUB_PAT=&quot;ghp_12345...&quot; Restart your R session. Access them in R: Sys.getenv(&quot;OPENAI_API_KEY&quot;) 14.3 AI as the “Pair Programmer” The most immediate value of AI is not replacing your analysis, but accelerating the code you write to perform it. 14.3.1 The Great Refactorer We all have old code: nested for loops, variable names like x1, x2, and manual indexing. AI excels at modernizing legacy code. Scenario: You have this Base R code to filter and clean data: # Old Code data &lt;- read.csv(&quot;sales.csv&quot;) clean_data &lt;- data[data$amount &gt; 100, ] clean_data$date &lt;- as.Date(clean_data$date) final &lt;- clean_data[order(clean_data$date), ] Prompt to AI: &gt; “Refactor this R code to use the tidyverse and the pipe (|&gt;) operator. Ensure variable names are snake_case.” AI Output: library(tidyverse) sales_data &lt;- read_csv(&quot;sales.csv&quot;) |&gt; filter(amount &gt; 100) |&gt; mutate(date = as.Date(date)) |&gt; arrange(date) 14.3.2 The Translator One of the hardest parts of learning R is knowing which package does what you want. You can describe your intent in plain English (or Spanish!) and get the function. Example Prompt: &gt; “I have this R code using purrr::map. Can you explain what it does in simple terms and suggest if there is a more modern way to write it?” 14.3.3 Pro Tip: Prompt Engineering 101 Getting good code from an LLM isn’t magic; it’s engineering. A high-quality prompt typically combines four key components. First, establish a Role (“You are an expert R programmer…”) to frame the model’s perspective. Second, clearly define the Task (“Write a function to…”). Third, set explicit Constraints (“Use dplyr, not base R; do not assume clean data”). Finally, specify the desired Format (“Return the code in a single block with comments”) to ensure the output matches your needs. [!TIP] Iterate. Your first prompt uses vague terms. Your second prompt clarifies them. Your third prompt gets the perfect answer. “I have a date column ‘2023-12-25’. I want to extract the week number of the year. Which lubridate function should I use?” AI Output: &gt; “You should use lubridate::isoweek() or lubridate::week().” 14.3.4 The Regex Master Regular Expressions (Regex) are powerful but notoriously difficult to write. This is arguably the best use case for LLMs. Scenario: You have a column with messy Peru phone numbers: (51) 999-999-999, +51 999 999 999. Prompt: &gt; “I have inconsistent phone numbers. Write a regex compatible with stringr to extract only the 9 digits of the mobile number, ignoring country code.” AI Output: library(stringr) phones &lt;- c(&quot;(51) 987-654-321&quot;, &quot;+51 987654321&quot;, &quot;987 654 321&quot;) # The Pattern: Simple extraction of 9 consecutive digits clean_phones &lt;- str_extract(phones, &quot;\\\\d{9}&quot;) print(clean_phones) #&gt; [1] NA &quot;987654321&quot; NA 14.3.5 The Error Decoder R error messages can be cryptic. * “Error in result[[1]] : subscript out of bounds” * “Error: aesthetics must be either length 1 or the same as the data” Instead of staring at the screen, paste the error and the code chunk into the AI. It will usually pinpoint the exact mismatch in list lengths or ggplot layers. 14.4 The Risks: Hallucinations We cannot finish this introduction without a warning. LLMs are people pleasers. They want to give you an answer, even if they have to invent it. 14.4.1 The “Package” Hallucination It is common for an LLM to invent an R function that should exist but doesn’t. User: “How do I calculate the Gini coefficient in dplyr?” AI: “Just use summarize(gini = gini_coeff(income))!” There is no gini_coeff function in dplyr default exports. It sounded plausible, but running it will crash your script. Always verify functions in the Help tab (?function_name). In the next chapter, we will stop chatting and start coding. We will build an engine to send data to the AI and get structured insights back. # LLMs as an Analysis Engine {#genai-api} In the previous chapter, we treated AI as a chatbot that helps us write code. Now, we are going to flip the script. We will treat the Large Language Model as a **function** within our R code—a function that accepts unstructured text as input and returns structured data as output. This approach turns unstructured text into structured data with minimal code. This is the transition from &quot;Chatting with AI&quot; to &quot;Building with AI&quot;. ## The API Economy To interact with models programmatically, we use **APIs** (Application Programming Interfaces). Instead of a web interface, we send HTTP requests. While there are R packages like `openai`, `ellmer` or `chattr` that wrap these APIs, as a Data Scientist it is critical to understand how to build the connection yourself using `httr2`. This gives you full control over error handling, retries, and costs. ### Prerequisite: The Setup Ensure you have your API key stored in the `.Renviron` file as discussed in the previous chapter. ``` r library(tidyverse) library(httr2) library(jsonlite) # Reload environment if needed readRenviron(&quot;.Renviron&quot;) 14.5 Building a Robust Request A production-quality API request needs more than just a URL. It needs Authentication, Retry Logic, and Error Handling. Let’s build a wrapper function to query OpenAI’s GPT-4o-mini (a cost-effective model). query_openai &lt;- function(prompt, system_prompt = &quot;You are a helpful assistant.&quot;) { api_key &lt;- Sys.getenv(&quot;OPENAI_API_KEY&quot;) if (api_key == &quot;&quot;) stop(&quot;Error: OPENAI_API_KEY not found in environment.&quot;) # 1. Construct the Request req &lt;- request(&quot;https://api.openai.com/v1/chat/completions&quot;) |&gt; req_headers(Authorization = paste(&quot;Bearer&quot;, api_key)) |&gt; req_body_json(list( model = &quot;gpt-4o-mini&quot;, messages = list( list(role = &quot;system&quot;, content = system_prompt), list(role = &quot;user&quot;, content = prompt) ), temperature = 0 # Deterministic for data tasks )) |&gt; # 2. Add Robustness: Retry 3 times if server fails (500) or rate limited (429) req_retry(max_tries = 3, backoff = ~ 2) |&gt; # Exponential backoff req_throttle(rate = 100/60) # 100 requests per minute # 3. Perform Request &amp; Handle Errors response &lt;- req_perform(req) # 4. Parse the content result &lt;- response |&gt; resp_body_json() return(result$choices[[1]]$message$content) } Now we have a function query_openai() that we can use like any other R function. query_openai(&quot;What is the capital of Peru?&quot;) # [1] &quot;The capital of Peru is Lima.&quot; 14.6 The Holy Grail: Structured Data extraction The biggest problem with LLMs is that they love to talk. If you ask for a sentiment score, they might say: “Here is the sentiment score you requested based on my analysis: Positive.” We don’t want that. We want \"Positive\". Or even better, we want a JSON object. 14.6.1 Forcing JSON Output Most modern models support “JSON Mode”. This guarantees the output is machine-readable valid JSON. Let’s say we have a dataset of raw customer reviews and want to extract specific insights. We need to capture the Sentiment (Positive or Negative), a list of mentioned Topics, and the Urgency level—flagging it as ‘High’ if the user is angry or at risk of churning, and ‘Low’ otherwise. extract_review_data &lt;- function(review_text) { system_instructions &lt;- &quot; You are a data extraction engine. Extract the following fields from the user review and return ONLY a JSON object: - sentiment: &#39;Positive&#39;, &#39;Neutral&#39;, or &#39;Negative&#39; - topics: a list of strings (e.g., [&#39;Price&#39;, &#39;UX&#39;]) - urgency: &#39;High&#39; if the user is angry/churning, else &#39;Low&#39; &quot; # Note: To enforce strict JSON, we often need to tell the model in the prompt # AND set response_format = { type: &#39;json_object&#39; } if supported. response_json &lt;- query_openai(review_text, system_label = system_instructions) # Parse JSON string to R list return(fromJSON(response_json)) } 14.7 Batch Processing: The purrr Workflow Now, let’s apply this to a Data Frame. When processing hundreds of rows, we must be careful. Now, let’s apply this to a Data Frame. When processing hundreds of rows, we must be careful. First, we need to respect Rate Limits, as APIs will block you if you send too many requests too quickly (e.g., 1000 in a second). Second, consider Cost by always testing on a small sample like head(df, 5) before running the full job. Finally, ensure Error Safety: if row 99 fails, we want to capture that error gracefully so the entire loop doesn’t crash. We use purrr::map with possibly() (or safely()) generally, but for API calls, adding a small Sys.sleep() is wise. # Sample Data reviews_df &lt;- tibble( id = 1:3, text = c( &quot;I love this product! Best purchase ever.&quot;, &quot;The delivery was late and the item is broken. I want a refund.&quot;, &quot;It&#39;s okay, but a bit expensive for what it is.&quot; ) ) # 1. Create a Safe Function (returns NULL instead of crashing) safe_extract &lt;- possibly(extract_review_data, otherwise = NULL) # 2. Iterate results_df &lt;- reviews_df |&gt; mutate(ai_data = map(text, function(t) { Sys.sleep(0.5) # Be polite to the API safe_extract(t) })) |&gt; # 3. Unnest the JSON structure unnest_wider(ai_data) print(results_df) Resulting Data Frame: id text sentiment topics urgency 1 I love… Positive [“Product”] Low 2 The delivery… Negative [“Shipping”, “Product”] High 3 It’s okay… Neutral [“Price”] Low 14.8 Summary We have turned an unstructured text column into usable columns for filtering and plotting. This is the true power of “LLMs as Data Engines”. We use httr2 for robust connections. We use System Prompts to force JSON structure. We use purrr and unnest_wider to flatten that AI insight back into our Tidyverse workflow. In the next chapter, we will discuss Ethics. But before that, there is one more superpower we need to unlock: Embeddings. 14.9 Beyond Generation: Embeddings So far, we have used LLMs to generate text. But they can also understand text by converting it into numbers. This is called an Embedding. An embedding is a list of numbers (a vector, e.g., 1536 numbers long) that represents the semantic meaning of a text. Consider the difference between a “Dog” and a “Puppy”; their corresponding vectors will be mathematically very close because they share similar semantic meanings. In contrast, “Dog” and “Sandwich” will be far apart. This capability powers Semantic Search. Unlike a standard keyword search that looks for exact matches like “Climate” or “Change”—and potentially misses relevant documents—a semantic search converts your query into a vector. It then finds documents with the closest vectors, allowing you to retrieve a report on “Global warming effect on maize” even if it doesn’t contain the exact words from your query “Climate change impact on corn.” 14.9.1 R Implementation Getting an embedding is just another API call: get_embedding &lt;- function(text) { req &lt;- request(&quot;https://api.openai.com/v1/embeddings&quot;) |&gt; req_headers(Authorization = paste(&quot;Bearer&quot;, Sys.getenv(&quot;OPENAI_API_KEY&quot;))) |&gt; req_body_json(list( model = &quot;text-embedding-3-small&quot;, input = text )) resp &lt;- req_perform(req) # Extract the vector resp |&gt; resp_body_json() |&gt; pluck(&quot;data&quot;, 1, &quot;embedding&quot;) |&gt; unlist() } v1 &lt;- get_embedding(&quot;The dog barked&quot;) v2 &lt;- get_embedding(&quot;The canine made noise&quot;) # cosine_sim &lt;- sum(v1 * v2) / (sqrt(sum(v1^2)) * sqrt(sum(v2^2))) # The result will be very high (close to 1). This vectorization is the foundation of RAG (Retrieval Augmented Generation), which allows you to chat with your own PDFs. "],["genai-embeddings.html", "Chapter 15 Text Analysis with Embeddings 15.1 Beyond Bag-of-Words 15.2 What is an Embedding? 15.3 Getting Embeddings in R 15.4 Visualizing Meaning (Dimensionality Reduction) 15.5 Building a Semantic Search Engine 15.6 Summary: The AI Workflow", " Chapter 15 Text Analysis with Embeddings In the previous chapters, we learned how to generate text and how to extract data from it. But what if we want to understand the relationship between thousands of documents without reading them? This is where Embeddings come in. They are arguably the most powerful yet underutilized tool in the AI toolkit for Data Scientists. 15.1 Beyond Bag-of-Words Traditional text mining techniques, such as word clouds or TF-IDF, treat text as a simple “bag of words,” ignoring context. For example, the sentence “I sat on the bank of the river” and “I went to the bank to deposit money” are treated as identical because they both contain the word “bank,” even though the meaning offers a completely different context. To a human—and to an Embedding model—these distinctions are clear. 15.2 What is an Embedding? An embedding is a translation of text into a vector of numbers. Imagine plotting words on a hypothetical 2D graph based on their meaning. In this space, King might sit at coordinates (5, 5), with Queen located nearby at (5, 7) due to their semantic similarity. Apple, unrelated to royalty, would be positioned far away at (10, 2). Modern embedding models like OpenAI’s text-embedding-3-small scale this concept up massively, placing words not in two dimensions, but in 1,536 dimensions. This high-dimensional space allows them to capture subtle nuances of meaning, tone, and context that simple coordinates cannot. 15.3 Getting Embeddings in R We can request embeddings using the same httr2 workflow we built in the previous chapter, but hitting the /embeddings endpoint. get_embedding &lt;- function(text_input) { api_key &lt;- Sys.getenv(&quot;OPENAI_API_KEY&quot;) req &lt;- request(&quot;https://api.openai.com/v1/embeddings&quot;) |&gt; req_headers(Authorization = paste(&quot;Bearer&quot;, api_key)) |&gt; req_body_json(list( model = &quot;text-embedding-3-small&quot;, input = text_input )) resp &lt;- req_perform(req) result &lt;- resp |&gt; resp_body_json() # The embedding is a list of numbers return(unlist(result$data[[1]]$embedding)) } # Example vector_dog &lt;- get_embedding(&quot;The dog barked&quot;) length(vector_dog) # [1] 1536 15.4 Visualizing Meaning (Dimensionality Reduction) We cannot visualize 1,536 dimensions. But we can use mathematical techniques like PCA (Principal Component Analysis) or UMAP to squash those dimensions down to 2, preserving relative distances. Let’s assume we have a dataframe news_df with headlines and their calculated embeddings. library(tidymodels) # Assume &#39;embeddings_mat&#39; is a matrix where each row is an embedding vector pca_rec &lt;- recipe(~., data = as.data.frame(embeddings_mat)) |&gt; step_pca(all_predictors(), num_comp = 2) pca_prep &lt;- prep(pca_rec) pca_data &lt;- bake(pca_prep, new_data = NULL) # Add back the text labels plot_data &lt;- pca_data |&gt; bind_cols(news_df |&gt; select(headline, category)) # Plot plot_data |&gt; ggplot(aes(x = PC1, y = PC2, color = category)) + geom_point(alpha = 0.8) + theme_minimal() + labs(title = &quot;Map of News Headlines&quot;) If we did this correctly, we would see distinct “clusters”. Sports news would cluster in one corner, politics in another, and technology in a third—even if they never share the exact same keywords! 15.5 Building a Semantic Search Engine The “Hello World” of Embeddings is Semantic Search, which fundamentally differs from traditional approaches. While a Keyword Search for “cheap phone” rigidly looks for the exact words “cheap” AND “phone,” a Semantic Search for “budget friendly mobile” understands the underlying intent—that “budget” relates to “cheap” and “mobile” to “phone.” Mathematically, this is calculated using Cosine Similarity. The closer the angle between two vectors, the more similar their meaning. # Function to calculate Cosine Similarity cosine_sim &lt;- function(a, b) { sum(a * b) / (sqrt(sum(a^2)) * sqrt(sum(b^2))) } search_news &lt;- function(query, data_vectors, data_text) { # 1. Embed the query query_vec &lt;- get_embedding(query) # 2. Compare against all document vectors similarities &lt;- apply(data_vectors, 1, function(doc_vec) { cosine_sim(query_vec, doc_vec) }) # 3. Return top 3 matches results &lt;- tibble( text = data_text, score = similarities ) |&gt; arrange(desc(score)) |&gt; head(3) return(results) } Now you can search for concepts, not just keywords! 15.6 Summary: The AI Workflow We have completed our journey through Generative AI in R. We have completed our journey through Generative AI in R, covering three foundational pillars. We started by understanding that Foundations of models are probabilistic engines predicting tokens, not reasoning beings. Then, we moved to APIs, building robust pipelines to extract structured JSON data from unstructured text. Finally, we explored Embeddings, learning to represent text as numerical vectors to enable powerful search and clustering by meaning. The future of Data Science is hybrid. It combines the statistical rigor of tools like tidymodels with the semantic understanding of Large Language Models. You are now equipped to build that future. "],["introduction-3.html", "Introduction", " Introduction Having mastered the fundamentals of R programming, data transformation, and visualization, it’s time to put these skills into practice with real-world scenarios. The following case studies demonstrate end-to-end data science workflows that you might encounter in professional settings. In this chapter, you will: Connect R to external APIs (Google Analytics) for automated data retrieval. Apply cleaning, transformation, and visualization to real estate market data. Practice the full workflow: problem definition → data access → analysis → insights. Each case introduces complementary libraries such as googleAnalyticsR for API access and lubridate for date manipulation, reinforcing patterns from earlier chapters while adding new practical tools. "],["case-study-real-estate-market-analysis.html", "Chapter 16 Case Study: Real Estate Market Analysis 16.1 Objectives 16.2 Loading Libraries 16.3 Exploring the Data 16.4 Data Cleaning 16.5 Exploratory Analysis 16.6 Creating Indicators 16.7 Try It Yourself 16.8 Conclusions", " Chapter 16 Case Study: Real Estate Market Analysis In this case study, we will apply our data transformation and visualization skills to analyzing the real estate market. We will use the txhousing dataset provided by the ggplot2 package, which contains information about housing sales in Texas. This dataset allows us to explore concepts like tracking value over time, comparing categories, and analyzing transaction volume—skills that transfer directly to financial analysis, sales reporting, and business intelligence. 16.1 Objectives Data Cleaning: Handle missing values and format dates. Transformation: Aggregate data by city and year to find trends. Visualization: Create time-series plots to analyze market health. Analysis: Identify cities with the highest growth and stability. 16.2 Loading Libraries We will use the core tidyverse libraries. library(tidyverse) library(lubridate) library(scales) library(ggthemes) 16.3 Exploring the Data First, let’s load and inspect the data. data(&quot;txhousing&quot;) glimpse(txhousing) #&gt; Rows: 8,602 #&gt; Columns: 9 #&gt; $ city &lt;chr&gt; &quot;Abilene&quot;, &quot;Abilene&quot;, &quot;Abilene&quot;, &quot;Abilene&quot;, &quot;Abilene&quot;, &quot;Abil… #&gt; $ year &lt;int&gt; 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, … #&gt; $ month &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, … #&gt; $ sales &lt;dbl&gt; 72, 98, 130, 98, 141, 156, 152, 131, 104, 101, 100, 92, 75, … #&gt; $ volume &lt;dbl&gt; 5380000, 6505000, 9285000, 9730000, 10590000, 13910000, 1263… #&gt; $ median &lt;dbl&gt; 71400, 58700, 58100, 68600, 67300, 66900, 73500, 75000, 6450… #&gt; $ listings &lt;dbl&gt; 701, 746, 784, 785, 794, 780, 742, 765, 771, 764, 721, 658, … #&gt; $ inventory &lt;dbl&gt; 6.3, 6.6, 6.8, 6.9, 6.8, 6.6, 6.2, 6.4, 6.5, 6.6, 6.2, 5.7, … #&gt; $ date &lt;dbl&gt; 2000.000, 2000.083, 2000.167, 2000.250, 2000.333, 2000.417, … The dataset contains: - city: Name of the city. - year, month: Date components. - sales: Number of sales. - volume: Total value of sales. - median: Median sale price. - listings: Total active listings. - inventory: “Months inventory”: amount of time it would take to sell all current listings at current sales pace. - date: Date in decimal format (e.g., 2000.08). 16.4 Data Cleaning Real-world data often has missing values (NA). Let’s check how many missing values we have in the sales column. sum(is.na(txhousing$sales)) #&gt; [1] 568 We see there are records with no sales data. For our analysis of market volume, we should remove these incomplete records. We will creating a clean dataset housing_clean. We will also create a proper date column using lubridate::make_date(), which is easier to work with than the decimal date. housing_clean &lt;- txhousing |&gt; filter(!is.na(sales)) |&gt; mutate(date_proper = make_date(year, month, 1)) |&gt; select(-date) # Remove the decimal date 16.5 Exploratory Analysis 16.5.1 Market Volume Over Time Let’s look at the total sales volume across all of Texas over time. This gives us a “macro” view of the market, similar to how we might look at total portfolio value in a financial context. # Aggregate by date total_market &lt;- housing_clean |&gt; group_by(date_proper) |&gt; summarise( total_sales = sum(sales), total_volume = sum(volume, na.rm = TRUE) ) # Plot Volume total_market |&gt; ggplot(aes(x = date_proper, y = total_volume)) + geom_line(color = &quot;steelblue&quot;) + scale_y_continuous(labels = label_dollar(scale = 1e-9, suffix = &quot;B&quot;)) + theme_minimal() + labs( title = &quot;Total Texas Housing Market Volume&quot;, subtitle = &quot;Monthly Total Sales Volume (Billions)&quot;, x = &quot;Year&quot;, y = &quot;Volume ($)&quot; ) We can clearly see the seasonality (peaks in summer) and the impact of the 2008 financial crisis (dip around 2008-2010), followed by a strong recovery. 16.5.2 Comparing Cities Just as we might compare different companies or portfolios, let’s compare the median housing prices in the major cities. We’ll focus on the “Big 4” Texas cities: Austin, Dallas, Houston, and San Antonio. major_cities &lt;- c(&quot;Austin&quot;, &quot;Dallas&quot;, &quot;Houston&quot;, &quot;San Antonio&quot;) city_trends &lt;- housing_clean |&gt; filter(city %in% major_cities) city_trends |&gt; ggplot(aes(x = date_proper, y = median, color = city)) + geom_line(alpha = 0.7) + theme_minimal() + scale_y_continuous(labels = label_dollar()) + labs( title = &quot;Median Housing Prices in Major Cities&quot;, x = &quot;Year&quot;, y = &quot;Median Price&quot;, color = &quot;City&quot; ) Austin (green) clearly shows the steepest growth curve, especially post-2012. 16.6 Creating Indicators In financial analysis, we often create ratios. Here, let’s look at Inventory, which is a measure of supply vs. demand. - High Inventory: Buyer’s market (prices might drop). - Low Inventory: Seller’s market (prices might rise). Let’s look at the average inventory per year for these cities. city_inventory &lt;- city_trends |&gt; group_by(city, year) |&gt; summarise(avg_inventory = mean(inventory, na.rm = TRUE), .groups = &quot;drop&quot;) city_inventory |&gt; ggplot(aes(x = year, y = avg_inventory, color = city)) + geom_line(linewidth = 1) + theme_fivethirtyeight() + labs( title = &quot;Market Health: Months of Inventory&quot;, subtitle = &quot;Lower means easier to sell (Seller&#39;s Market)&quot;, color = &quot;City&quot; ) We see a convergence around 2014-2015 where inventory became very tight across all major cities. 16.7 Try It Yourself Expand the Analysis: Add “Fort Worth” to major_cities and re-run the median price comparison. How does it compare to the Big 4? Calculate Growth: Compute the year-over-year percentage change in median price for Austin. Which year had the highest growth? Seasonality Deep Dive: Which month typically has the highest sales volume? Create a boxplot of sales by month to visualize seasonal patterns. 16.8 Conclusions Through this case study, we performed essential Data Science tasks on a real dataset: Cleaning: Handling NA values and formatting dates with lubridate. Aggregation: Summarizing billions of dollars of volume into clear trend lines. Comparison: Benchmarking cities against each other to identify leaders. Indicators: Creating business-relevant metrics like months of inventory. [!TIP] This exact workflow applies to countless domains: stock prices, customer churn, inventory management, or any time-series business data. Master the pattern here, and you can adapt it anywhere. "],["google-analytics-from-r.html", "Chapter 17 Google Analytics from R 17.1 Problem 17.2 Access to data 17.3 Visualization 17.4 Conclusion", " Chapter 17 Google Analytics from R Understanding the audience that enters our website helps us make better decisions, whether these are commercial or content release decisions. We can, thus, insert a visit counter or use Google Analytics to start collecting much more than the total visits. 17.1 Problem We have a website to which we already placed the Google Analytics code to understand visit statistics to my website, but I want reports that today the web does not provide us. We need to access the raw data to represent our own reports and access them even without having to enter the Google Analytics website. [!IMPORTANT] Google sunsetted Universal Analytics in July 2023. The googleAnalyticsR package now supports GA4. The concepts here remain valid, but the specific function parameters may differ. See the package documentation for GA4-specific usage. 17.2 Access to data We are going to assume for this case that we already have a google analytics account and we are already tracking data from our website through some view. For this case I am going to use the statistics to the website that you are currently reading. To access the Google Analytics data we will use the googleAnalyticsR library. In addition, to quickly manipulate dates from or to we will use the lubridate library. install.packages(&quot;googleAnalyticsR&quot;) library(googleAnalyticsR) library(lubridate) library(tidyverse) Then, we have to authenticate. To do this we will use the ga_auth() function, which will open a web page to log in with the account in which we have access to Google Analytics. ga_auth() Now that we are authenticated we can bring all our accounts using the ga_account_list() function. account_list &lt;- ga_account_list() From here we will search for the row of the website that interests us and from there we will obtain the propertyId column. The Property ID in Google Analytics 4 for this website is the following: property_id &lt;- 123456789 # Replace with your GA4 Property ID Finally, we need two variables of the date from when to when we want the data. from_date &lt;- &quot;2024-01-01&quot; to_date &lt;- &quot;2024-03-31&quot; Or if we wish we can only calculate the information of the last two months, or two days, etc. # Two months ago until now # Two months ago until now from_date &lt;- (today() - months(2)) |&gt; as.character() to_date &lt;- today() |&gt; as.character() from_date to_date Thus, we can already make a call to obtain the data we need using the ga_data() function (the standard for GA4). history &lt;- ga_data(property_id, date_range = c(from_date, to_date), metrics = &quot;activeUsers&quot;, dimensions = &quot;date&quot;) With this data frame we could filter it or visualize it, depending on what we need. 17.3 Visualization Now with access to the data we can use the multiple metrics and dimensions available. For this case we are going to exemplify visualizing the city from where they visit this website in the last 90 days, which is related to the information in the paragraph of the main page, the preface, of this website (however, for that other calculation it is performed for another period of time). # Last 90 days to date from_date &lt;- seq(now(), length = 2, by = &quot;-90 days&quot;)[2] |&gt; as_date() |&gt; as.character() to_date &lt;- now() |&gt; as_date() |&gt; as.character() # We add the city as a dimension history &lt;- ga_data(property_id, date_range = c(from_date, to_date), metrics = &quot;activeUsers&quot;, dimensions = &quot;city&quot;) As we see, the dimension also allows a vector as input. We will create a bar chart with the top 5 cities that visited this website in the last 90 days. history |&gt; filter(city != &quot;(not set)&quot;) |&gt; group_by(city) |&gt; summarise(total = sum(activeUsers)) |&gt; mutate(proportion = total / sum(total)) |&gt; top_n(5, wt = proportion) |&gt; mutate(city = reorder(city, proportion, sum)) |&gt; ggplot() + aes(proportion, city) + geom_col() + labs( x = &quot;Proportion of visits&quot;, title = &quot;Proportion of visits by city&quot;, y = &quot;&quot; ) Keep in mind that in this case there is an issue of recognition of IPs coming from Lima, Peru, and that is why they do not appear as the first visitor. At the time of performing this analysis they all appeared as “(not set)”. However, if the same analysis is done by country and not by city, Peru is recognized and appears as one of the top visiting the web. 17.4 Conclusion Accessing Google Analytics data programmatically opens powerful possibilities: Automated Reporting: Schedule R scripts to generate weekly/monthly reports. Custom Metrics: Combine GA data with internal business data for richer analysis. Interactive Dashboards: Use Shiny to create real-time analytics dashboards. With the googleAnalyticsR package, you can query any metric or dimension available in your GA account, transforming raw clickstream data into actionable business insights—all without leaving your R environment. "],["ethics-checklist.html", "Chapter 18 Appendix A: Responsible AI Checklist 18.1 Data Quality &amp; Lineage 18.2 Fairness &amp; Bias 18.3 Transparency &amp; Explainability 18.4 Reproducibility &amp; Integrity 18.5 GenAI Specifics", " Chapter 18 Appendix A: Responsible AI Checklist As we conclude this book, it is crucial to remember that technical skills are only half of the equation. Data science has real-world consequences. Before deploying any model, analysis, or reliable pipeline to production, use this checklist to ensure your work is robust, fair, and transparent. This checklist is designed to be actionable for R users, pointing to specific packages and practices where applicable. 18.1 Data Quality &amp; Lineage “Garbage in, garbage out” applies to ethics as well as accuracy.* Provenance: Do I know exactly where this data came from? Is the source trustworthy? Consent &amp; Privacy: Was the data collected with consent? Does it contain Personally Identifiable Information (PII)? Tip: Use packages like introdat or custom scripts to scan for patterns resembling PII (emails, SSNs) before data leaves your secure environment. Representation: Does the training data match the real-world population it will be applied to? Action: Check distribution of key demographics in your train vs. production sets. Validation: Have I validated the data schema and constraints? Tool: Use the pointblank or validator packages to define and enforce data quality rules (e.g., col_vals_between(age, 0, 120)). 18.2 Fairness &amp; Bias Algorithms can reinforce existing inequalities. Protected Classes: Have I checked performance across different groups (Gender, Age, Ethnicity)? Bias Detection: Have I quantified the bias in my model? Tool: Use fairness, fairmodels, or dalex to calculate metrics like Disparate Impact or Equal Opportunity difference. Example Code: fairness_check(explainer, protected = data$gender, privileged = \"Male\") Proxy Variables: Are there variables (like Zip Code) acting as proxies for protected classes? Impact: Who could be harmed if this model makes a mistake? (e.g., Denying a loan vs. Recommending a bad movie). 18.3 Transparency &amp; Explainability Black boxes should not make high-stakes decisions. Documentation: Is the model card created? (Inputs, Outputs, Limitations, Intended Use). Explainability: Can I explain to a non-technical stakeholder why the model made a specific prediction? Tool: Use dalex, lime, or iml to create feature contribution plots or breakdown plots. Feedback Loop: Is there a mechanism for users to report errors or contest decisions? 18.4 Reproducibility &amp; Integrity Science must be reproducible. Environment Sealing: Is the R environment reproducible? Tool: Use renv to capture package versions in a renv.lock file. Randomness Control: Are random seeds set (set.seed()) for key steps like splitting data or initializing weights? Code Versioning: Is the code committed to version control (Git) with clear messages? 18.5 GenAI Specifics If using Large Language Models (LLMs). Fact-Checking: Have I verified AI-generated code/facts against reliable sources? Hallucinations are common. Security: Have I ensured no sensitive data is being sent to public APIs? Attribution: Am I transparent about which parts of the work were AI-generated? “With great power comes great responsibility.” — Stan Lee (and every Data Scientist) "],["r6-intro.html", "Chapter 19 Appendix B: Object Oriented Programming with R6 19.1 The R6 package: Classes, methods, encapsulation, and inheritance 19.2 Exercises", " Chapter 19 Appendix B: Object Oriented Programming with R6 In R, Object-Oriented Programming (OOP) can be implemented in several ways. Traditionally, R has used systems called S3 and S4 for OOP. S3 is an informal and flexible system. It is based on the idea of generic functions, which can have different methods depending on the class of the object they apply to. For example, the print() function is a generic function having different methods for printing different types of objects, such as vectors, lists, or data frames. # Example of generic function in S3 print(c(1, 2, 3)) # Prints a numeric vector #&gt; [1] 1 2 3 print(list(a = 1, b = 2)) # Prints a list #&gt; $a #&gt; [1] 1 #&gt; #&gt; $b #&gt; [1] 2 S4 is a more formal and structured system than S3. It defines classes and methods more explicitly, using special syntax. S4 is often used in packages requiring a more rigorous object structure, like Bioconductor. # Example of class definition in S4 setClass(&quot;Person&quot;, slots = c(name = &quot;character&quot;, age = &quot;numeric&quot;)) # Example of object creation in S4 my_person &lt;- new(&quot;Person&quot;, name = &quot;John&quot;, age = 30) my_person #&gt; An object of class &quot;Person&quot; #&gt; Slot &quot;name&quot;: #&gt; [1] &quot;John&quot; #&gt; #&gt; Slot &quot;age&quot;: #&gt; [1] 30 However, both S3 and S4 can be somewhat confusing and limited, especially for more complex projects. Luckily, there is a more modern and robust alternative: the R6 package. This package offers a more intuitive and efficient way to implement OOP in R, with features facilitating code organization, reuse, and maintenance. If you are new to OOP, don’t worry about S3 and S4 details for now. With R6, you can learn basic OOP concepts more easily and apply them to your data analysis projects. 19.1 The R6 package: Classes, methods, encapsulation, and inheritance The R6 package implements a class and object system similar to other object-oriented programming languages like Python or Java. It provides a robust and efficient way to create objects with attributes and methods, allowing encapsulation and inheritance. install.packages(&quot;R6&quot;) library(R6) Classes: A class is like a blueprint or template for creating objects. It defines the attributes (data) and methods (functions) that objects of that class will have. In R6, classes are created with the R6Class() function. # Define a &quot;Person&quot; class Person &lt;- R6Class(&quot;Person&quot;, public = list( name = NULL, age = NULL, # Constructor initialize = function(name, age) { self$name &lt;- name self$age &lt;- age }, # Method to greet greet = function() { cat(&quot;Hello, my name is&quot;, self$name, &quot;and I am&quot;, self$age, &quot;years old.\\n&quot;) } ) ) In this example, a Person class is defined with name and age attributes, and greet() method. The public list defines public members of the class, i.e., attributes and methods accessible from outside the object. Objects: An object is an instance of a class. It is a concrete entity having attributes and methods defined by the class. In R6, objects are created with the $new() method. # Create an object of class &quot;Person&quot; juan &lt;- Person$new(name = &quot;Juan&quot;, age = 30) juan #&gt; &lt;Person&gt; #&gt; Public: #&gt; age: 30 #&gt; clone: function (deep = FALSE) #&gt; greet: function () #&gt; initialize: function (name, age) #&gt; name: Juan Methods: Methods are functions operating on an object’s attributes. They allow accessing and modifying object data, as well as performing other actions. In R6, methods are defined within the public list of the class. # Call greet() method of object &quot;juan&quot; juan$greet() #&gt; Hello, my name is Juan and I am 30 years old. Encapsulation: Encapsulation is a mechanism allowing hiding internal details of an object and controlling access to its attributes. This protects object data and facilitates usage. In R6, encapsulation is achieved by distinguishing between public and private members. Public members are defined in public list and can be accessed from outside the object. Private members are defined in private list and can only be accessed from within the object, through methods. # Define a &quot;BankAccount&quot; class with encapsulation BankAccount &lt;- R6Class(&quot;BankAccount&quot;, public = list( holder = NULL, # Constructor initialize = function(holder) { self$holder &lt;- holder private$balance &lt;- 0 }, # Method to deposit money deposit = function(amount) { private$balance &lt;- private$balance + amount }, # Method to withdraw money withdraw = function(amount) { if (amount &lt;= private$balance) { private$balance &lt;- private$balance - amount } else { stop(&quot;Insufficient funds.&quot;) } }, # Method to check balance check_balance = function() { return(private$balance) } ), private = list( balance = NULL ) ) Inheritance: Inheritance is a mechanism allowing creating new classes from existing classes, inheriting their attributes and methods. This facilitates code reuse and creation of class hierarchies. In R6, inheritance is specified with inherit argument of R6Class() function. # Define a &quot;Student&quot; class inheriting from &quot;Person&quot; Student &lt;- R6Class(&quot;Student&quot;, inherit = Person, public = list( major = NULL, # Constructor initialize = function(name, age, major) { super$initialize(name, age) self$major &lt;- major }, # Method to show student info show_info = function() { super$greet() cat(&quot;Major:&quot;, self$major, &quot;\\n&quot;) } ) ) # Create an object of class &quot;Student&quot; maria &lt;- Student$new(name = &quot;Maria&quot;, age = 20, major = &quot;Engineering&quot;) # Call method show_info() maria$show_info() #&gt; Hello, my name is Maria and I am 20 years old. #&gt; Major: Engineering In this example, Student class inherits from Person class. Student constructor calls parent class constructor (super$initialize()) to initialize inherited attributes. show_info() method calls parent class greet() method (super$greet()) and then shows student-specific information. With R6, you can create classes and objects with a high degree of flexibility and control, allowing you to apply OOP effectively in your data analysis projects. 19.2 Exercises Create a class called Pet with attributes name, species and age, and methods introduce() (showing name, species and age of pet) and have_birthday() (incrementing pet age by 1). Solution library(R6) Pet &lt;- R6Class(&quot;Pet&quot;, public = list( name = NULL, species = NULL, age = NULL, initialize = function(name, species, age) { self$name &lt;- name self$species &lt;- species self$age &lt;- age }, introduce = function() { cat(&quot;Hello, I am&quot;, self$name, &quot;, a&quot;, self$species, &quot;of&quot;, self$age, &quot;years old.\\n&quot;) }, have_birthday = function() { self$age &lt;- self$age + 1 } ) ) Create a class called Dog inheriting from Pet class (from previous exercises). Dog class should have an additional attribute called breed and a method called bark(). Solution Dog &lt;- R6Class(&quot;Dog&quot;, inherit = Pet, public = list( breed = NULL, initialize = function(name, age, breed) { super$initialize(name, &quot;dog&quot;, age) self$breed &lt;- breed }, bark = function() { cat(&quot;Woof! Woof!\\n&quot;) } ) ) "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
