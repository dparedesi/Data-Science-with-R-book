[["index.html", "Data Science with R Data Analysis and prediction algorithms with R. Second Edition Preface", " Data Science with R Data Analysis and prediction algorithms with R. Second Edition Author: Mg. Daniel Paredes Inilupu 2025-12-23 Preface Welcome to the second edition of Data Science with R! This book, initially conceived as a compilation of personal notes to facilitate my learning, has evolved to become a comprehensive resource covering everything from fundamentals to advanced data science techniques. I have invested over 700 hours in the creation of this book. You can support this effort by purchasing the PDF version on leanpub. Furthermore, the purchase includes access to future updates and the possibility of making direct inquiries with me for three months regarding the covered topics or their practical application. In this edition, the latest trends and technologies in data science have been incorporated. This includes updates to the most recent versions of R (4.4.2) and RStudio (2024.12.0), as well as the integration of modern libraries such as keras, tensorflow, xgboost, quanteda, and sparklyr. Interactive visualization tools like plotly and leaflet have also been added. New sections address crucial topics such as ethics in data science, reproducibility through R Markdown, and version control with Git and GitHub. Additionally, we have strengthened the content on deep learning, time series analysis, text mining, and sentiment analysis. The web version available at bookdown1 seeks to democratize data science knowledge. Share it and let’s contribute together to freeing knowledge. Just like the first edition, this version is based on exercises designed from practical classroom experiences and activities from the Professional Certificate in Data Science2 by HarvardX. The code used to generate this book is available on GitHub, encouraging transparency and reproducibility. We have improved and updated the exercises, incorporating recent practical cases on social network analysis, climate change, and text mining. This allows for the immediate application of what has been learned to real-world problems. I deeply thank the readers of the first edition for their comments and suggestions, which have been fundamental to improving this version. This book has reached readers in Mexico, Colombia, Spain, Peru, and Chile, among other countries. I hope this second edition serves as a valuable tool to advance your learning and practice of data science. If you have questions or suggestions, you can write to me at dparedesi@uni.pe. I usually respond within a maximum of 48 hours. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. https://bookdown.org/dparedesi/data-science-con-r/↩︎ https://online-learning.harvard.edu/series/professional-certificate-data-science↩︎ "],["acknowledgments.html", "Acknowledgments", " Acknowledgments First and foremost, I want to express my profound gratitude to my wife, Desislava, for her invaluable emotional support during the countless hours I dedicated to organizing ideas and writing code. Her knowledge in R was also key, contributing solutions and perspectives that enriched this work. A special thanks to Rafael Irizarry3, a true benchmark in the R world, whose didactic way of teaching advanced techniques allowed me to advance significantly in my learning. I also extend my gratitude to the developers who, with their dedication, continue to create and maintain this wonderful language. This book is nothing more than a compendium and a guide, made possible thanks to the constant effort of those who expand R’s libraries and packages. My gratitude also goes to Briguit Reinaldo, CEO of Cedhinfo, whose tireless work brings the teaching of computer technologies to more people in Peru, inspiring many to explore new opportunities. Finally, I want to acknowledge the valuable contribution of engineering students from UNI, who participated in grammatical review, paraphrasing, creation of additional exercises, and proposal of new topics to include. Special thanks to Josep Agama4 and Andrés Espinoza5 for their fundamental contributions in the first three chapters. https://hsph.harvard.edu/profile/rafael-a-irizarry/↩︎ https://www.linkedin.com/in/josep-agama-749a61190/↩︎ https://www.linkedin.com/in/aespinozacontreras/↩︎ "],["introduction.html", "Introduction Why R? Installing R Installing RStudio RStudio Sections Testing RStudio", " Introduction Data science requires a multidisciplinary approach that combines statistics, programming, data mining, and business understanding. This book is designed to help you develop those skills through practical examples in R. Why R? R is a language created by statisticians for data analysis. Its versatility makes it a powerful tool for exploring, modeling, and visualizing data. Installing R You can download R from the Comprehensive R Archive Network (CRAN). Search for CRAN on Google: Once on the CRAN page, select the version for your operating system: Linux, Mac OS X, or Windows: Here the steps for Windows are shown, but the steps are similar for Linux or Mac OS X. On the CRAN website, we need to install base which includes all the basic packages you need to get started. Later, in the following chapters, we will see how to install other additional packages. Click on the first link to get the most recent version: Then, open the installer you just downloaded to install R and follow the on-screen instructions. Installing RStudio Although you could already start using R in console mode, we are going to install RStudio, an integrated development environment (IDE), which will facilitate our work with R. Search for RStudio on Google: You should see the website as it appears below. Once there, go to the upper right menu and click on DOWNLOAD Then scroll down until you find the download options. Select the Free RStudio Desktop option. It will show you the button to download according to your operating system. You can also download from the list below the Download button. Once the installer is downloaded, open it and follow the on-screen instructions. RStudio Sections When you start RStudio for the first time, you will see 3 sections: One of the great advantages of R over point-and-click analysis software is that we can save our work as Scripts. To create a new Script, you can click on File, then New File, and then on R Script. This way we will have the 4 sections, or panels, of RStudio: Code editor: The code editor for Scripts where we can save our Scripts. R console: The R console where we will see the result of the execution of our Scripts. Environment/History: Here we will mainly see the variables/functions as we create them. Other panes: Finally a section with additional tabs. Here we will see our created plots, for example. Testing RStudio To test that we have installed it correctly, go to the console section and calculate how much 13 multiplied by 265 is. Click on the console section, type in the console and then press Enter 13 * 265 #&gt; [1] 3445 You should have gotten 3445 as a result in the same way you observe lines above. With the difference of the symbol ##. This symbol in this book will serve to differentiate the result that our R Scripts yield. Thus, if you do not see the symbol ## it is R code, and what is with ## simulates the result seen in the console. You will also see before the number 3445 the number 1 in brackets: [1]. This is because each Script can yield a set of solutions/results. In brackets it tells you which solution number it shows you. In this case the solution is unique, but later we will see when there are more results from your Script. You can get the same result if you write in the Scripts section, and not in the console, that will allow you to make several calculations. For example, on one hand I can do 28 * 27 and then I can calculate a sum 65 + 35 #Calculating 28 multiplied by 27: 28 * 27 #Adding 65 + 35: 65 + 35 To execute from the scripts section we can select all the lines and then click on Run in the upper right part or press Ctrl + Enter. Keep in mind that the # sign makes that line a comment. Try selecting all the text, then selecting only the sum line and click on Run. You will see that it only executes what you select. You are now ready to start learning R. "],["objects.html", "Chapter 1 Objects 1.1 What are objects in R? 1.2 Variables: The first objects on your journey 1.3 Object types for complex data 1.4 The Universe of Objects in R 1.5 Exercises", " Chapter 1 Objects In the world of programming, an object is like a container that holds information. This information can be of different types: numbers, text, complex data, and even code. The important thing is that an object groups everything necessary to represent an entity or concept. In R, practically everything is an object. The variables we will use to store data, the functions we will use to process that data, and even the data itself, are objects. 1.1 What are objects in R? Imagine you are organizing your move to the United States. Each item you pack in a box (clothes, books, appliances) can be considered an object. Each object has characteristics that define it: a name, a type, a size, a weight, etc. In R, objects also have characteristics that define them. These characteristics are called attributes. Some common attributes are: Name: The name we give to the object to be able to refer to it. Type: The type of data the object contains (numeric, character, logical, etc.). Class: The class of the object, which defines its structure and behavior (vector, list, data frame, function, etc.). Length: The amount of elements the object contains (if it is a vector, a list, etc.). 1.1.1 R as an object-oriented language R is an object-oriented programming language. This means it relies on the concept of objects to organize and process information. Object-oriented programming has several advantages: Modularity: Objects allow dividing a program into smaller, manageable parts. Reusability: Objects can be reused in different parts of the program or in other programs. Encapsulation: Objects hide implementation details, facilitating their use and maintenance. 1.1.2 The power of abstraction The concept of an object allows us to abstract the complexity of the real world. Instead of thinking about the details of how data is stored and processed in computer memory, we can think in terms of objects representing real-world entities. For example, instead of thinking of a series of numbers representing the temperatures of different cities, we can think of a “temperatures” object containing all that information. This abstraction facilitates understanding and handling information, allowing us to focus on the logic of the problem we want to solve. 1.2 Variables: The first objects on your journey Before we start packing for our move to the United States, we need to know what things we will take. Each object we decide to take is represented in R as a variable. Think of variables as labels we put on each object. For example, we could use the variable state to save the name of the state we are moving to, or the variable num_suitcases to save the number of suitcases we will take. 1.2.1 Creating variables in R In R, we don’t need to declare a variable before using it. We simply assign it a value using the &lt;- symbol. Example: # Assign the value &quot;California&quot; to the variable &quot;state&quot; state &lt;- &quot;California&quot; # Assign the value 5 to the variable &quot;num_suitcases&quot; num_suitcases &lt;- 5 To see the value we have saved in a variable, we simply type its name in the RStudio console and press Enter. Example: state &lt;- &quot;California&quot; state #&gt; [1] &quot;California&quot; When executing this code, you will see the value \"California\" appear in the console. 1.2.2 Operations with variables We can also use variables to perform operations. For example, if we want to calculate the total cost of our plane trip, we could use the variables ticket_price and num_people. Example: ticket_price &lt;- 300 num_people &lt;- 4 total_cost &lt;- ticket_price * num_people total_cost #&gt; [1] 1200 In this example, we first assign values to the variables ticket_price and num_people. Then, we multiply these variables to calculate the total_cost and display its value in the console. 1.2.3 Best practices for naming variables Watch out for capitalization! R is case-sensitive. If you create a variable called state and then try to access it as State, R will not find it. Descriptive names It is important to use descriptive names for variables, clearly indicating what information they contain. Instead of using variables like x or y, it is better to use names like ticket_price or num_suitcases. Rules for naming variables Variable names can contain letters, numbers, and underscores (_). They cannot start with a number. They cannot contain spaces. R is case-sensitive. 1.2.4 Data types Variables in R can contain different types of data: Numeric: Represent numbers, such as the population of a city or the cost of a plane ticket. chicago_population &lt;- 2700000 ticket_price &lt;- 300 Character: Represent text, such as the name of a state or a city. state &lt;- &quot;California&quot; city &lt;- &quot;Los Angeles&quot; Logical: Represent truth values: TRUE or FALSE. We could use a logical variable to indicate if we want to visit a city or not. visit_chicago &lt;- TRUE visit_miami &lt;- FALSE 1.3 Object types for complex data The variables we have seen so far are very useful for storing individual information, such as the name of a city or the number of suitcases we will carry on our move. However, in the real world, we often need to work with more complex datasets. Imagine you want to save the names of all the cities you plan to visit on your trip to the United States. Would you have to create a variable for each city? That would be very tedious! Fortunately, R offers other types of objects that allow us to organize and manipulate information more efficiently. Let’s look at some of them: 1.3.1 Vectors: organizing information of the same type Vectors are like trains transporting a series of objects of the same type. They can be numbers, text, or logical values, but all elements of a vector must be of the same type. For example, we could use a vector to save the name of each state in the United States, or a vector to save the population of each state. Creating vectors: To create a vector, we can use the c() function (which stands for “combine”) and list the elements we want to include, separated by commas. # Create a vector with the names of some states states &lt;- c(&quot;California&quot;, &quot;Texas&quot;, &quot;Florida&quot;, &quot;New York&quot;) # Create a vector with the population of each state (in millions) population &lt;- c(39.2, 29.0, 21.4, 19.4) If we want to know the amount of data our vector has, its length, we will use the length() function. The class() function tells us the class of the object, that is, what type of data it contains. length(population) #&gt; [1] 4 class(states) #&gt; [1] &quot;character&quot; class(population) #&gt; [1] &quot;numeric&quot; We can use the names() function to assign names to the elements of a vector. This can be useful for identifying each element. names(population) &lt;- states population #&gt; California Texas Florida New York #&gt; 39.2 29.0 21.4 19.4 In addition to c(), there are other useful functions for creating vectors: seq(): Creates a sequence of numbers. We can specify the start value, the end value, and the increment. # Create a vector with numbers from 1 to 10 numbers &lt;- seq(1, 10) # Create a vector with numbers from 2 to 20, by 2 even_numbers &lt;- seq(2, 20, by = 2) rep(): Repeats a value or a vector a specified number of times. # Create a vector with the value 1 repeated 5 times ones &lt;- rep(1, 5) # Create a vector with the sequence &quot;A&quot;, &quot;B&quot; repeated 3 times letters &lt;- rep(c(&quot;A&quot;, &quot;B&quot;), 3) # Output: &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; Accessing vector elements: Each element of a vector has a position, indicated by a number in brackets. The first element is at position 1, the second at position 2, and so on. # Show the first element of the &quot;states&quot; vector states[1] # Output: &quot;California&quot; # Show the third element of the &quot;population&quot; vector population[3] # Output: 21.4 We can also access multiple elements at once using the : operator. For example, to access elements from the second to the fourth of the states vector: states[2:4] #&gt; [1] &quot;Texas&quot; &quot;Florida&quot; &quot;New York&quot; Operations with vectors: We can perform mathematical operations with numeric vectors. For example, if we want to calculate the total population of the four states, we can use the + operator to sum the elements of the population vector. population &lt;- c(39.2, 29.0, 21.4, 19.4) population[1] + population[2] + population[3] + population[4] #&gt; [1] 109 If we want to perform the same operation more concisely, R allows us to sum all elements of a vector directly: population &lt;- c(39.2, 29.0, 21.4, 19.4) sum(population) #&gt; [1] 109 R also offers other tools for performing operations with vectors. For example, if we want to calculate the square root of the population of each state: sqrt(population) #&gt; [1] 6.260990 5.385165 4.626013 4.404543 In this case, the sqrt() function calculates the square root of each element of the population vector individually. This is possible because many functions in R are vectorized, meaning they can operate directly on vectors, element by element. Vectorized functions are very efficient as they avoid the need to write loops to process each element of the vector separately. We will explore functions in R and how to use them for more complex data analysis in greater depth later. Vector coercion: Unlike other programming languages, R tries to interpret or change a value when it encounters an error. For example, if we try to convert a character vector to numeric, R will convert the elements it can and replace the ones it cannot with NA. example &lt;- c(&quot;3&quot;, &quot;b&quot;, &quot;6&quot;, &quot;a&quot;, &quot;bridge&quot;, &quot;4&quot;) as.numeric(example) #&gt; Warning: NAs introduced by coercion #&gt; [1] 3 NA 6 NA NA 4 Sorting vectors: We can sort the elements of a vector using the sort() function. districts &lt;- c(&quot;Comas&quot;, &quot;Lince&quot;, &quot;Miraflores&quot;, &quot;Lurigancho&quot;, &quot;Chorrillos&quot;) sort(districts) #&gt; [1] &quot;Chorrillos&quot; &quot;Comas&quot; &quot;Lince&quot; &quot;Lurigancho&quot; &quot;Miraflores&quot; We can also order a vector using its indices with the order() function. This way, we get a vector with the positions the elements of the original vector would occupy if they were sorted. This can be useful when we want to sort a vector based on another vector or when we want to preserve the original vector without modifying it. indices &lt;- order(districts) # Output: 5 1 2 4 3 districts[indices] #&gt; [1] &quot;Chorrillos&quot; &quot;Comas&quot; &quot;Lince&quot; &quot;Lurigancho&quot; &quot;Miraflores&quot; NA in vectors: If a vector contains NA values, some operations may return NA. We can use the is.na() function to identify NA values and filter them. example_na &lt;- c(28, 3, 19, NA, 89, 45, NA, 86, 5, 18, 28, NA) example_no_na &lt;- example_na[!is.na(example_na)] mean(example_no_na) # Output: 38.66667 #&gt; [1] 35.66667 1.3.2 Lists: grouping objects of different types Lists are like containers that can hold different types of objects. Imagine a box where you can put clothes, books, tools, and any other object you need. In R, lists allow you to group diverse information into a single object. Creating lists: To create a list, we use the list() function and specify the elements we want to include, separated by commas. Each element can have a name, indicated with the = symbol. # Create a list with information about a city city_info &lt;- list(name = &quot;San Francisco&quot;, population = 880000, cost_of_living = 3.8, climate = &quot;Temperate&quot;) Accessing list elements: To access the elements of a list, we can use their names or their positions. # Access the &quot;name&quot; element of the &quot;city_info&quot; list city_info$name # Output: &quot;San Francisco&quot; # Access the second element of the &quot;city_info&quot; list city_info[[2]] # Output: 880000 1.3.3 Matrices: organizing data in rows and columns Matrices are like tables that organize information in rows and columns. All elements of a matrix must be of the same type. Creating matrices: To create a matrix, we use the matrix() function. We must specify the data we want to include, the number of rows (nrow), and the number of columns (ncol). # Create a matrix with distances between cities (in miles) city_distances &lt;- matrix(c(0, 2600, 2100, 950, 2600, 0, 1100, 2700, 2100, 1100, 0, 2100, 950, 2700, 2100, 0), nrow = 4, ncol = 4) city_distances #&gt; [,1] [,2] [,3] [,4] #&gt; [1,] 0 2600 2100 950 #&gt; [2,] 2600 0 1100 2700 #&gt; [3,] 2100 1100 0 2100 #&gt; [4,] 950 2700 2100 0 Accessing matrix elements: To access the elements of a matrix, we use brackets and specify the row and column of the element we want. # Access the element in row 1, column 3 of the &quot;city_distances&quot; matrix city_distances[1, 3] #&gt; [1] 2100 1.3.4 Arrays: multidimensional matrices Arrays are like matrices that have more than two dimensions. Imagine a matrix that, in addition to rows and columns, has depth. In R, arrays allow you to organize data in more complex structures. Creating arrays: To create an array, we use the array() function. # Create an array with maximum and minimum temperatures of # three cities during the summer months (June, July, August) temperatures &lt;- array(c(25, 28, 30, 22, 25, 28, # City 1 28, 20, 32, 25, 18, 30, # City 2 22, 25, 28, 18, 23, 25), # City 3 dim = c(3, 2, 3)) # 3 cities, 2 temperatures (max/min), 3 months temperatures #&gt; , , 1 #&gt; #&gt; [,1] [,2] #&gt; [1,] 25 22 #&gt; [2,] 28 25 #&gt; [3,] 30 28 #&gt; #&gt; , , 2 #&gt; #&gt; [,1] [,2] #&gt; [1,] 28 25 #&gt; [2,] 20 18 #&gt; [3,] 32 30 #&gt; #&gt; , , 3 #&gt; #&gt; [,1] [,2] #&gt; [1,] 22 18 #&gt; [2,] 25 23 #&gt; [3,] 28 25 Accessing array elements: To access the elements of an array, we use brackets and specify the position of the element in each dimension. # Access the maximum temperature of city 2 in July temperatures[2, 1, 2] #&gt; [1] 20 1.3.5 Factors: representing categorical data Factors are a special type of object used to represent categorical data, that is, data that can be classified into groups. For example, the type of climate (“warm”, “temperate”, “cold”), the region of a country (“north”, “south”, “east”, “west”), or the type of housing (“house”, “apartment”). Creating factors: To create a factor, we use the factor() function. # Create a factor with climate types of different cities climate_types &lt;- factor(c(&quot;Temperate&quot;, &quot;Warm&quot;, &quot;Cold&quot;)) Levels of a factor: The different values a factor can take are called levels. In the previous example, the levels of the climate_types factor are “Temperate”, “Warm”, and “Cold”. Utility of factors: Factors are very useful for data analysis, as they allow grouping and comparing information efficiently. For example, we could use the climate_types factor to analyze how the cost of living varies in cities with different climates. 1.4 The Universe of Objects in R Throughout this chapter, we have explored the different types of objects inhabiting the R universe. From the simplest variables to multidimensional arrays, each object plays an important role in building our data analyses. 1.4.1 Philosophy of objects in R In R, everything is an object. This philosophy has profound implications for how code is written and executed. By treating everything as an object, R promotes consistency, modularity, and reuse. Objects allow us to encapsulate information and behavior, facilitating code organization and maintenance. Furthermore, the ability to create our own objects gives us great power to model and solve complex problems. By understanding the philosophy of objects in R, we can make the most of the language’s capabilities for data analysis. 1.4.2 Comparison with other languages While many modern programming languages use the object-oriented paradigm, R has a particular approach. In languages like Python or Java, creating classes and objects is a fundamental part of the language. In R, while it is possible to create classes and objects, the language focuses more on the use of functions to manipulate and transform data. This difference is due in part to R’s history as a language for statistical analysis. In this context, functions are a natural tool for performing calculations and analyses. 1.5 Exercises Now that you know the different types of objects in R, it’s time to put your knowledge to the test. Create the following variables: city_name: Store the name of the US city you would like to move to. population: Store the population of that city (you can search for it online). distance: Store the distance in kilometers from your current city to the US city. want_to_live_there: Store a logical value (TRUE or FALSE) indicating if you really would like to live in that city. Solution city_name &lt;- &quot;Seattle&quot; population &lt;- 724745 distance &lt;- 8340 # Approximate distance from Lima, Peru want_to_live_there &lt;- TRUE Create a vector called nearby_cities containing the names of three cities near the city you chose in the previous exercise. Solution nearby_cities &lt;- c(&quot;Tacoma&quot;, &quot;Bellevue&quot;, &quot;Everett&quot;) Create a list called my_list containing the following elements: Your name. Your age. A vector with the names of your three favorite colors. A logical value indicating if you like chocolate. Solution my_list &lt;- list(name = &quot;Ana&quot;, age = 30, favorite_colors = c(&quot;blue&quot;, &quot;green&quot;, &quot;red&quot;), likes_chocolate = TRUE) Create a matrix called monthly_expenses containing your estimated monthly expenses in the following categories: Category January February March Housing Transport Food Entertainment Complete the matrix with numerical values. Solution monthly_expenses &lt;- matrix(c(1500, 1500, 1500, # Housing 300, 250, 350, # Transport 500, 400, 550, # Food 200, 150, 250), # Entertainment nrow = 4, ncol = 3, dimnames = list(c(&quot;Housing&quot;, &quot;Transport&quot;, &quot;Food&quot;, &quot;Entertainment&quot;), c(&quot;January&quot;, &quot;February&quot;, &quot;March&quot;))) Create a factor called climate_types containing the names of the different climate types in the United States (you can use “Temperate”, “Warm”, “Cold”, etc.). Assign labels to the factor levels to make them more descriptive (for example, “Cold climate”, “Temperate climate”, etc.). Solution climate_types &lt;- factor(c(&quot;Temperate&quot;, &quot;Warm&quot;, &quot;Cold&quot;, &quot;Warm&quot;, &quot;Temperate&quot;), levels = c(&quot;Cold&quot;, &quot;Temperate&quot;, &quot;Warm&quot;), labels = c(&quot;Cold climate&quot;, &quot;Temperate climate&quot;, &quot;Warm climate&quot;)) climate_types Create a vector called cities_to_visit with the names of 5 cities you would like to visit in the United States. Then, create another vector called days_per_city with the number of days you would like to spend in each city. Finally, create a third vector called daily_cost with the estimated daily cost in each city (in dollars). Solution cities_to_visit &lt;- c(&quot;New York&quot;, &quot;Los Angeles&quot;, &quot;Chicago&quot;, &quot;San Francisco&quot;, &quot;Miami&quot;) days_per_city &lt;- c(5, 4, 3, 6, 2) daily_cost &lt;- c(200, 180, 150, 220, 170) Create a vector called max_temperatures with the average maximum temperatures (in Celsius) of the cities you want to visit during the month of July. Then, create a vector called min_temperatures with the average minimum temperatures. Finally, create a matrix containing these two vectors as columns, and name the rows with the names of the cities. Solution max_temperatures &lt;- c(29, 28, 27, 22, 31) # Max temperatures in July min_temperatures &lt;- c(21, 18, 19, 15, 25) # Min temperatures in July # Create the matrix temperatures &lt;- matrix(c(max_temperatures, min_temperatures), nrow = 5, ncol = 2, dimnames = list(cities_to_visit, c(&quot;Maximum&quot;, &quot;Minimum&quot;))) temperatures #&gt; Maximum Minimum #&gt; New York 29 21 #&gt; Los Angeles 28 18 #&gt; Chicago 27 19 #&gt; San Francisco 22 15 #&gt; Miami 31 25 Create a three-dimensional array containing information about the climate of the cities you want to visit. The first dimension should represent the cities, the second dimension should represent the months of the year (“January”, “February”, …, “December”), and the third dimension should represent two variables: “Temperature” and “Precipitation”. You can use dummy values to fill the array. Solution # Create an array with dimensions 5 cities x 12 months x 2 variables climate &lt;- array(dim = c(5, 12, 2), dimnames = list(cities_to_visit, month.name, c(&quot;Temperature&quot;, &quot;Precipitation&quot;))) # Fill the array with dummy values (example) climate[,, &quot;Temperature&quot;] &lt;- sample(10:35, 60, replace = TRUE) # Temperatures between 10 and 35 degrees climate[,, &quot;Precipitation&quot;] &lt;- sample(0:100, 60, replace = TRUE) # Precipitation between 0 and 100 mm climate #&gt; , , Temperature #&gt; #&gt; January February March April May June July August September #&gt; New York 35 32 11 27 22 26 10 30 10 #&gt; Los Angeles 33 21 25 19 16 14 23 33 15 #&gt; Chicago 24 35 28 16 34 29 32 12 25 #&gt; San Francisco 32 28 24 23 14 28 17 22 18 #&gt; Miami 26 18 22 30 22 34 29 15 15 #&gt; October November December #&gt; New York 33 34 28 #&gt; Los Angeles 26 10 29 #&gt; Chicago 35 15 19 #&gt; San Francisco 33 10 14 #&gt; Miami 19 17 23 #&gt; #&gt; , , Precipitation #&gt; #&gt; January February March April May June July August September #&gt; New York 84 45 56 13 65 83 43 43 14 #&gt; Los Angeles 17 10 56 32 59 89 31 75 6 #&gt; Chicago 81 100 60 78 97 60 32 48 35 #&gt; San Francisco 83 58 71 37 22 17 98 62 49 #&gt; Miami 30 3 48 73 29 61 16 56 85 #&gt; October November December #&gt; New York 85 96 2 #&gt; Los Angeles 1 35 45 #&gt; Chicago 11 73 5 #&gt; San Francisco 18 22 37 #&gt; Miami 84 60 13 Imagine you have a vector with the daily maximum temperatures of a US city for a year. Create a program that, using only the concepts learned in this chapter (variables, vectors, matrices, arrays, and factors), identifies the longest streak of consecutive days with maximum temperatures above a given threshold (for example, 25 degrees Celsius). Solution This exercise requires efficient vector handling and algorithmic logic to identify the longest streak. Here is a possible solution: # Create a vector with dummy maximum temperatures for a year temperatures &lt;- sample(10:35, 365, replace = TRUE) # Define the temperature threshold threshold &lt;- 25 # Create a logical vector indicating if the temperature exceeds the threshold hot_days &lt;- temperatures &gt; threshold # Initialize variables to track the longest streak current_streak &lt;- 0 longest_streak &lt;- 0 start_longest_streak &lt;- 0 # Iterate through the hot days vector for (i in 1:length(hot_days)) { if (hot_days[i]) { current_streak &lt;- current_streak + 1 } else { if (current_streak &gt; longest_streak) { longest_streak &lt;- current_streak start_longest_streak &lt;- i - current_streak } current_streak &lt;- 0 } } # Show the longest streak and its position cat(&quot;The longest streak of hot days is:&quot;, longest_streak, &quot;\\n&quot;) #&gt; The longest streak of hot days is: 8 cat(&quot;Starts on day:&quot;, start_longest_streak, &quot;\\n&quot;) #&gt; Starts on day: 116 This code uses a for loop to traverse the hot days vector and two variables (current_streak and longest_streak) to track the longest streak. Imagine you have a vector with the daily stock prices of a company for a year. Create a program that, using only the concepts learned in this chapter, determines the time period in which you could have bought and sold the shares to obtain the maximum profit. Assume you can only buy and sell once. Solution This exercise is a variant of the classic “maximize stock profit” problem. Solving it optimally can be complex, but with the concepts from chapter 1, we can create an algorithm that finds a solution (though not necessarily the optimal one). # Create a vector with dummy stock prices for a year prices &lt;- sample(50:150, 365, replace = TRUE) # Initialize variables to track max profit max_profit &lt;- 0 buy_day &lt;- 1 sell_day &lt;- 1 # Iterate through the prices vector for (i in 1:(length(prices) - 1)) { for (j in (i + 1):length(prices)) { profit &lt;- prices[j] - prices[i] if (profit &gt; max_profit) { max_profit &lt;- profit buy_day &lt;- i sell_day &lt;- j } } } # Show max profit and buy/sell days cat(&quot;Maximum profit:&quot;, max_profit, &quot;\\n&quot;) #&gt; Maximum profit: 100 cat(&quot;Buy day:&quot;, buy_day, &quot;\\n&quot;) #&gt; Buy day: 3 cat(&quot;Sell day:&quot;, sell_day, &quot;\\n&quot;) #&gt; Sell day: 80 This code uses two nested for loops to compare all possible pairs of buy and sell days. "],["functions.html", "Chapter 2 Functions 2.1 Introduction to the world of functions 2.2 Anatomy of a function 2.3 Mastering the use of functions 2.4 Higher-order functions 2.5 Closures: functions with memory 2.6 Debugging and error handling: solving the mysteries of your code 2.7 Exercises", " Chapter 2 Functions 2.1 Introduction to the world of functions In the previous chapter, we explored the different types of objects we can use to store and organize information in R. We learned to create variables, vectors, lists, matrices, and arrays, and saw how to access their elements and perform operations with them. Now, in this chapter, we will go a step further and delve into the world of functions. Functions are one of the fundamental pillars of programming in R, allowing us to perform more complex tasks and automate our work. 2.1.1 What are functions? Imagine a coffee machine. You provide the ingredients (water, coffee, sugar), and the machine performs a series of steps to produce a cup of coffee. Similarly, a function in R is a set of instructions that receives input data (the arguments) and performs a series of operations to produce a result (the return value). Functions allow us to encapsulate a set of instructions into a single block of code, facilitating reuse and code organization. Instead of writing the same instructions over and over again, we can create a function that performs them for us. 2.1.2 Why use functions? Functions offer several advantages: Reusability: We can use the same function in different parts of our code or in different projects. Organization: Functions help us organize our code into logical blocks, making it easier to read and understand. Readability: By using functions, our code becomes more concise and easier to understand. Abstraction: Functions allow us to abstract the complexity of a task, allowing us to focus on the logic of the problem we want to solve. 2.1.3 First functions: exploring basic R functions R already includes a large number of predefined functions that we can use to perform different tasks. Let’s look at some examples: sum(): Calculates the sum of the elements of a vector. numbers &lt;- c(1, 2, 3, 4, 5) sum(numbers) # Output: 15 #&gt; [1] 15 mean(): Calculates the arithmetic mean of the elements of a vector. temperatures &lt;- c(25, 28, 26, 29, 27) mean(temperatures) # Output: 27 #&gt; [1] 27 round(): Rounds a number to a specific number of decimal places. pi # Output: 3.141593 #&gt; [1] 3.141593 round(pi, 2) # Output: 3.14 #&gt; [1] 3.14 length(): Returns the length of a vector (the number of elements it contains). cities &lt;- c(&quot;New York&quot;, &quot;Los Angeles&quot;, &quot;Chicago&quot;) length(cities) # Output: 3 #&gt; [1] 3 These are just a few of the many predefined functions that R offers. As we progress through the book, we will explore more functions and learn how to use them to perform more complex data analysis. 2.2 Anatomy of a function In the previous section, we saw what functions are and why they are so useful in programming. Now, we are going to delve into the structure of a function, so you can create your own functions and automate tasks in your data analysis. 2.2.1 Arguments: the ingredients of the function To make a cup of coffee, you need ingredients: water, coffee, and maybe sugar or milk. Similarly, functions in R need arguments to do their job. Arguments are the input data that the function uses to perform its operations. For example, the sum() function needs a vector of numbers as an argument to calculate the sum of its elements. numbers &lt;- c(1, 2, 3, 4, 5) sum(numbers) # Output: 15 #&gt; [1] 15 A function’s arguments are specified in parentheses after the function name. If a function requires multiple arguments, they are separated by commas. For example, imagine we want to create a function to calculate the total cost of a plane trip. This function might need the following arguments: ticket_price: The price of a plane ticket. num_people: The number of people traveling. discount: An optional discount on the ticket price (for example, for being a student or senior citizen). The function could be called calculate_trip_cost and would be used as follows: calculate_trip_cost(ticket_price = 300, num_people = 2, discount = 0.1) In this case, we are passing three arguments to the function: ticket_price with value 300, num_people with value 2, and discount with value 0.1 (representing a 10% discount). 2.2.2 Body: the instructions of the function The body of a function is the set of instructions that are executed when the function is called. These instructions can be any valid R code: variable assignments, mathematical operations, conditionals, loops, calls to other functions, etc. The body of a function is defined within curly braces {}. For example, the body of the function calculate_trip_cost could be: calculate_trip_cost &lt;- function(ticket_price, num_people, discount = 0) { total_cost &lt;- ticket_price * num_people * (1 - discount) return(total_cost) } In this body, first the total cost of the trip is calculated by multiplying the ticket price by the number of people and by (1 minus the discount). Then, return() is used to return the total_cost. Note that in the function definition, the argument discount has a default value of 0. This means that if we do not specify a value for discount when calling the function, the value 0 will be used. For example, if we do not specify a value for discount, the function uses the default value 0, and the total cost is 600: # Call the function without specifying the discount calculate_trip_cost(ticket_price = 300, num_people = 2) #&gt; [1] 600 If we want to apply a discount, we can specify it when calling the function: calculate_trip_cost(ticket_price = 300, num_people = 2, discount = 0.1) #&gt; [1] 540 In this case, the total cost is 540, since a 10% discount is applied. 2.2.3 Return value: the result of the function The return value is the result the function produces after executing its instructions. It can be a simple value (a number, text, a logical value) or a more complex object (a vector, a list, a dataframe). In R, the return value is specified with the return() function. If return() is not used, the function will return the result of the last expression evaluated in the body. In the calculate_trip_cost example, the return value is the total_cost of the trip, which is a number. 2.2.4 Examples: creating simple functions step by step Let’s see an example of how to create a simple function that converts degrees Celsius to Fahrenheit: celsius_to_fahrenheit &lt;- function(celsius) { fahrenheit &lt;- (celsius * 9 / 5) + 32 return(fahrenheit) } In this example: celsius_to_fahrenheit is the name of the function. celsius is the argument of the function (the temperature in degrees Celsius). The body of the function calculates the temperature in Fahrenheit using the formula (celsius * 9 / 5) + 32 and stores it in the variable fahrenheit. The return() function returns the value of the variable fahrenheit. Now we can use our function to convert temperatures: celsius_to_fahrenheit(0) # Output: 32 #&gt; [1] 32 celsius_to_fahrenheit(100) # Output: 212 #&gt; [1] 212 Congratulations! You just created your first function in R. As we progress through the chapter, you will learn to create more complex functions and use them to solve real-world problems. 2.3 Mastering the use of functions We have already seen how to create simple functions with basic arguments, including the possibility of assigning default values. Now, we will explore even more advanced techniques to master the use of functions and write more flexible and efficient code. 2.3.1 Functions with a variable number of arguments (...): Adapting to different situations Sometimes, we don’t know beforehand how many arguments a function will receive. For these cases, R offers us the possibility of defining functions with a variable number of arguments using the three dots (...). For example, the sum() function can receive any number of arguments: sum(1, 2, 3) #&gt; [1] 6 sum(1, 2, 3, 4, 5) # Output: 15 #&gt; [1] 15 We can use the three dots (...) to create our own functions that accept a variable number of arguments. For example, a function that calculates the average of several numbers: calculate_average &lt;- function(...) { numbers &lt;- c(...) average &lt;- mean(numbers) return(average) } calculate_average(1, 2, 3) #&gt; [1] 2 calculate_average(1, 2, 3, 4, 5) #&gt; [1] 3 In this example, the three dots (...) capture all the arguments passed to the function and store them in the numbers vector. Then, the function calculates the average of the numbers in the vector and returns it as a result. It is important to note that when using ..., we lose the ability to name the arguments individually. However, we gain flexibility by being able to pass a variable number of arguments to the function. 2.3.2 Variable scope: local and global variables The scope of a variable refers to the part of the code where the variable is accessible. In R, variables defined inside a function have a local scope, meaning they are only accessible within the function. Variables defined outside any function have a global scope, meaning they are accessible from anywhere in the code. For example, in the function calculate_average, the variable numbers has a local scope: calculate_average &lt;- function(...) { numbers &lt;- c(...) average &lt;- mean(numbers) return(average) } If we try to access the variable numbers outside the function, we will get an error: numbers # Error: object &#39;numbers&#39; not found This is because numbers only exists inside the calculate_average function. When the function finishes executing, the local variables defined inside it cease to exist. On the other hand, if we define a variable outside any function, it will be a global variable: conversion_rate &lt;- 0.621371 # Conversion rate from kilometers to miles We can access the conversion_rate variable from anywhere in the code, even inside a function: kilometers_to_miles &lt;- function(kilometers) { miles &lt;- kilometers * conversion_rate return(miles) } kilometers_to_miles(100) #&gt; [1] 62.1371 It is important to keep variable scope in mind when writing functions to avoid errors and confusion. If a variable is not defined in the current scope (local), R will look in the global scope. If the variable is not found in any scope, an error will occur. For example, imagine we want to calculate the total cost of a trip, including the cost of the plane ticket, accommodation, and other expenses. We can create a function that receives these expenses as arguments and calculates the total cost: calculate_trip_cost &lt;- function(ticket, accommodation, other_expenses) { total_cost &lt;- ticket + accommodation + other_expenses return(total_cost) } If we call this function with expense values, we get the total cost: calculate_trip_cost(ticket = 300, accommodation = 500, other_expenses = 100) #&gt; [1] 900 Now, imagine we want to apply a tax to the total cost. We could define a global variable tax_rate: tax_rate &lt;- 0.16 And then modify the function to include the tax: calculate_trip_cost &lt;- function(ticket, accommodation, other_expenses) { total_cost &lt;- ticket + accommodation + other_expenses total_cost &lt;- total_cost * (1 + tax_rate) return(total_cost) } When calling the function again, the total cost will include the tax: calculate_trip_cost(ticket = 300, accommodation = 500, other_expenses = 100) #&gt; [1] 1044 In this case, the calculate_trip_cost function can access the global variable tax_rate because it is not defined locally within the function. If we try to use a variable that is not defined in any scope, we will get an error: calculate_trip_cost &lt;- function(ticket, accommodation, other_expenses) { total_cost &lt;- ticket + accommodation + other_expenses + tip return(total_cost) } calculate_trip_cost(ticket = 300, accommodation = 500, other_expenses = 100) # Error: object &#39;tip&#39; not found In this case, the variable tip is not defined either locally or globally, so the function cannot access it. It is important to understand the concept of variable scope to write functions that work correctly and avoid errors. 2.3.3 Examples: functions to calculate taxes, discounts, etc. Functions are very useful for automating repetitive tasks, such as calculating taxes, discounts, or converting units. Let’s look at some examples with different levels of difficulty: Calculating shipping cost for a package calculate_shipping_cost &lt;- function(weight, destination) { if (destination == &quot;local&quot;) { cost &lt;- 5 + 0.1 * weight } else if (destination == &quot;national&quot;) { cost &lt;- 10 + 0.2 * weight } else { # destination == &quot;international&quot; cost &lt;- 20 + 0.5 * weight } return(cost) } # Usage example package_weight &lt;- 2.5 # Weight in kilograms destination &lt;- &quot;national&quot; shipping_cost &lt;- calculate_shipping_cost(package_weight, destination) shipping_cost #&gt; [1] 10.5 In this example, the calculate_shipping_cost() function calculates the shipping cost of a package based on its weight and destination. The function uses a conditional structure (if-else if-else) to apply different shipping rates depending on the destination. Calculating income tax with brackets calculate_income_tax &lt;- function(income) { if (income &lt;= 10000) { rate &lt;- 0.10 } else if (income &lt;= 20000) { rate &lt;- 0.15 } else { rate &lt;- 0.20 } tax &lt;- income * rate return(tax) } # Usage example income &lt;- 15000 tax &lt;- calculate_income_tax(income) tax #&gt; [1] 2250 In this example, the calculate_income_tax() function calculates a person’s income tax based on their income. The function uses a conditional structure (if-else if-else) to apply different tax rates according to the income bracket. Calculating trip cost with multiple options calculate_trip_cost &lt;- function(origin_city, destination_city, transport_type = &quot;plane&quot;, num_people = 1, hotel = NULL, daily_expenses = 100, trip_duration = 7) { # Calculate transport cost if (transport_type == &quot;plane&quot;) { transport_cost &lt;- 300 * num_people # Base price per person } else if (transport_type == &quot;train&quot;) { transport_cost &lt;- 150 * num_people # Base price per person } else { transport_cost &lt;- 0 # Assuming transport is by own car } # Calculate accommodation cost if (!is.null(hotel)) { accommodation_cost &lt;- hotel$price * trip_duration } else { accommodation_cost &lt;- 0 # Assuming staying not at a hotel } # Calculate other expenses other_expenses &lt;- daily_expenses * num_people * trip_duration # Calculate total cost total_cost &lt;- transport_cost + accommodation_cost + other_expenses return(total_cost) } # Usage example trip_cost_1 &lt;- calculate_trip_cost(origin_city = &quot;Lima&quot;, destination_city = &quot;New York&quot;, transport_type = &quot;plane&quot;, num_people = 2) trip_cost_2 &lt;- calculate_trip_cost(origin_city = &quot;Lima&quot;, destination_city = &quot;Los Angeles&quot;, transport_type = &quot;train&quot;, num_people = 3, hotel = list(price = 150), daily_expenses = 120, trip_duration = 10) trip_cost_1 #&gt; [1] 2000 trip_cost_2 #&gt; [1] 5550 2.4 Higher-order functions In previous sections, we explored how to create and use functions in R. Now, let’s delve into a more advanced concept: higher-order functions. What are higher-order functions? Higher-order functions are functions that can: Receive other functions as arguments. Return a function as a result. This type of function allows us to write more flexible and expressive code, and they are a powerful tool for data analysis. 2.4.1 lapply() and sapply(): applying a function to each element Imagine you have a list with information about several US cities, and you want to calculate the population density of each city. You could write a for loop to iterate through the list and calculate the density of each city separately. However, R offers a more efficient and elegant way to do this: the lapply() function. lapply() (which stands for “list apply”) takes two arguments: A list (or a vector). A function to be applied to each element of the list. lapply() applies the function to each element of the list and returns a new list with the results. # Create a list with information about cities cities &lt;- list( New_York = list(population = 8.4e6, area = 783.8), Los_Angeles = list(population = 3.9e6, area = 1302.0), Chicago = list(population = 2.7e6, area = 606.1) ) # Function to calculate population density calculate_density &lt;- function(city) { density &lt;- city$population / city$area return(density) } # Calculate population density of each city densities &lt;- lapply(cities, calculate_density) densities #&gt; $New_York #&gt; [1] 10717.02 #&gt; #&gt; $Los_Angeles #&gt; [1] 2995.392 #&gt; #&gt; $Chicago #&gt; [1] 4454.71 In this example, lapply() applies the calculate_density function to each element of the cities list and returns a new list densities with the population density of each city. The sapply() function is similar to lapply(), but tries to simplify the result. If the result is a list of vectors of the same type and length, sapply() returns a vector or a matrix. # Calculate population density of each city with sapply() densities &lt;- sapply(cities, calculate_density) densities #&gt; New_York Los_Angeles Chicago #&gt; 10717.020 2995.392 4454.710 In this case, sapply() returns a vector with population densities. 2.4.2 apply(): applying a function to rows or columns The apply() function allows us to apply a function to the rows or columns of a matrix or array. It’s like having a tool that allows us to go through each row or column of our data table and perform a specific calculation on each one. For example, if we have a matrix with the maximum and minimum temperatures of different cities, we can use apply() to calculate the average temperature of each city. # Create a matrix with temperatures temperatures &lt;- matrix(c(25, 18, 30, 22, 35, 28), nrow = 3, ncol = 2, dimnames = list(c(&quot;New York&quot;, &quot;Los Angeles&quot;, &quot;Chicago&quot;), c(&quot;Maximum&quot;, &quot;Minimum&quot;))) # Calculate average temperature of each city average_temperatures &lt;- apply(temperatures, 1, mean) average_temperatures #&gt; New York Los Angeles Chicago #&gt; 23.5 26.5 29.0 In this example, apply() applies the mean() function to each row of the temperatures matrix (the argument 1 indicates that the function should be applied to rows) and returns a vector with the average temperatures of each city. If we wanted to calculate the maximum or minimum temperature among all cities, we could use apply() with the max() or min() function, respectively, and apply it to columns (using argument 2). # Calculate maximum temperature among all cities maximum_temperature &lt;- apply(temperatures, 2, max) maximum_temperature #&gt; Maximum Minimum #&gt; 30 35 2.4.3 mapply(): applying a function to multiple arguments The mapply() function allows us to apply a function to multiple arguments in parallel. It’s like having a tool that allows us to take several sets of data and apply the same operation to each corresponding set. For example, imagine we have two vectors: one with the names of different US cities and another with their respective populations. We want to create a new vector containing the phrase “The city of [city name] has a population of [population] inhabitants”. We could use mapply() to apply a function combining the city name and its population to each pair of elements from the vectors. # Create vectors with city names and populations cities &lt;- c(&quot;New York&quot;, &quot;Los Angeles&quot;, &quot;Chicago&quot;) populations &lt;- c(8.4e6, 3.9e6, 2.7e6) # Function to create the phrase create_phrase &lt;- function(city, population) { phrase &lt;- paste(&quot;The city of&quot;, city, &quot;has a population of&quot;, population, &quot;inhabitants.&quot;) return(phrase) } # Create vector with phrases city_phrases &lt;- mapply(create_phrase, cities, populations) city_phrases #&gt; New York #&gt; &quot;The city of New York has a population of 8400000 inhabitants.&quot; #&gt; Los Angeles #&gt; &quot;The city of Los Angeles has a population of 3900000 inhabitants.&quot; #&gt; Chicago #&gt; &quot;The city of Chicago has a population of 2700000 inhabitants.&quot; In this example, mapply() applies the create_phrase function to the cities and populations vectors in parallel, taking one element from each vector at a time, and returns a vector with the resulting phrases. Note that the create_phrase function receives two arguments: city and population. mapply() is responsible for taking one element from each vector and passing them as arguments to the function. In the first iteration, it passes “New York” as city and 8.4e6 as population. In the second iteration, it passes “Los Angeles” and 3.9e6, and so on. Another example of using mapply() would be if we have two vectors with maximum and minimum temperatures of different cities, and we want to calculate the temperature difference between maximum and minimum for each city. # Create vectors with maximum and minimum temperatures maxs &lt;- c(25, 30, 35) mins &lt;- c(18, 22, 28) # Calculate temperature difference for each city temp_difference &lt;- mapply(function(max, min) max - min, maxs, mins) temp_difference #&gt; [1] 7 8 7 In this example, mapply() applies the anonymous function function(max, min) max - min to the maxs and mins vectors in parallel, taking the first element of maxs and the first element of mins, then the second element of each vector, and so on. For each pair of elements, the anonymous function calculates the difference and returns a vector with the results. 2.4.4 Examples: data analysis with higher-order functions Higher-order functions are a powerful tool for data analysis. They allow us to perform complex operations concisely and efficiently. Imagine you have a matrix with information about different states, where each row represents a state and each column a numeric variable, such as population or per capita income. You could use apply() to calculate the mean of each column. # Create a matrix with information about states states &lt;- matrix(c(39.2e6, 29.0e6, 21.4e6, 64500, 56100, 50800), nrow = 3, ncol = 2, dimnames = list(c(&quot;California&quot;, &quot;Texas&quot;, &quot;Florida&quot;), c(&quot;population&quot;, &quot;per_capita_income&quot;))) # Calculate mean of each column means &lt;- apply(states, 2, mean) means #&gt; population per_capita_income #&gt; 29866666.67 57133.33 In this example, apply() applies the mean() function to each column of the states matrix and returns a vector with the means. Another one would be if we have a list with prices of different hotels in several US cities. You could use sapply() to apply a function calculating the tax of each price, or lapply() to convert prices from dollars to euros. You could also use apply() to calculate the average price of hotels in each city, or to find the most expensive and cheapest hotel in each city. As we progress through the book, we will see more examples of how to use higher-order functions to solve real-world problems. The possibilities are endless, and higher-order functions give you great flexibility to manipulate and analyze your data. 2.5 Closures: functions with memory Until now, we have seen that functions in R receive arguments, execute a set of instructions, and return a result. However, functions can also have “memory”, that is, they can remember information between calls. This is possible thanks to a concept called closures. 2.5.1 Concept: functions that “remember” A closure is a function that “remembers” the environment in which it was created. This means the function has access to variables that were defined at the time of its creation, even if those variables are no longer in the current scope. To better understand this concept, let’s see an example. Imagine we want to create a function that counts how many times it has been called. We can do this using a closure: create_counter &lt;- function() { counter &lt;- 0 # Initialize the counter # Define the function that increments the counter increment_counter &lt;- function() { counter &lt;&lt;- counter + 1 return(counter) } return(increment_counter) # Return the function } # Create a counter my_counter &lt;- create_counter() # Call the counter several times my_counter() #&gt; [1] 1 my_counter() #&gt; [1] 2 my_counter() #&gt; [1] 3 In this example, the create_counter() function creates a counter variable and an increment_counter() function. The increment_counter() function has access to the counter variable and increments it by 1 each time it is called. The create_counter() function returns the increment_counter() function. When we call my_counter(), we are calling the increment_counter() function that was created inside create_counter(). This function “remembers” the value of the counter variable and increments it on each call. It is important to note that the counter variable is not a global variable. It is only accessible within the increment_counter() function. This is because counter was defined inside the create_counter() function, so its scope is local to that function. However, the increment_counter() function “captures” the counter variable in its environment, allowing it to access it even after the create_counter() function has finished executing. 2.5.2 Applications: creating counters, functions with internal state Closures have many applications in programming. Some of the most common are: Creating counters: As we saw in the previous example, closures allow us to create functions that maintain an internal state between calls. Creating functions with configurable parameters: We can use closures to create functions that “remember” specific parameters. For example, we could create a function generating functions to convert temperatures from Celsius to Fahrenheit, where the generated function “remembers” the temperature scale it needs to convert. Encapsulating data: Closures allow us to hide data within a function, which can be useful for protecting sensitive information or avoiding naming conflicts. For example, we could create a function generating unique identifiers, where the generated function “remembers” the last generated identifier. 2.5.3 Examples: simulating a game, creating an operation history Let’s see some more concrete examples of using closures: Simulating a game: We can use a closure to simulate a game where the player has to guess a secret number. The closure can “remember” the secret number and keep track of the player’s attempts. Creating an operation history: We can use a closure to create a function that records operations performed on a variable. The closure can “remember” the operation history and show it when requested. Closures are a powerful tool that allows us to write more flexible and expressive code. As you become familiar with them, you will discover new ways to apply them in your data analysis. 2.6 Debugging and error handling: solving the mysteries of your code So far, we have explored the fascinating world of functions in R. We have learned to create, use, and combine them to perform complex tasks. However, on the programming journey, encountering errors is inevitable. Sometimes, our code doesn’t work as we expect, and we encounter cryptic error messages that leave us perplexed. In this section, we will learn to identify, understand, and fix errors in our R code. We will also see how to handle errors gracefully, so our code is more robust and reliable. 2.6.1 Identifying errors: common error messages in R When our code contains an error, R will show us an error message in the console. These messages can seem intimidating at first, but with a little practice, we will learn to interpret them and use them to find the cause of the error. Some common error messages in R are: Error: object 'object_name' not found: This error occurs when we try to use a variable or function that does not exist. It may be that we misspelled the name, or that the variable or function is not defined in the current scope. Error in function_name(arguments): invalid argument: This error occurs when we pass an invalid argument to a function. For example, if we pass a text vector to a function expecting a numeric vector. Error in if (condition) { ... }: argument is of length zero: This error occurs when the condition in an if structure has length zero. This can happen if the condition evaluates to NULL or an empty vector. Error in for (variable in sequence) { ... }: invalid 'for' loop sequence: This error occurs when the sequence in a for loop is invalid. For example, if the sequence is NULL or a vector of length zero. It is important to read error messages carefully and try to understand what they are telling us. Often, the error message will give us a clue about the cause of the problem. 2.6.2 Debugging tools: debug(), traceback() R offers several tools to debug our code and find the cause of errors. Two of the most useful tools are debug() and traceback(). debug(): This function allows us to execute a function step by step, allowing us to inspect the value of variables at each step and understand how the code is executing. To use debug(), we simply call the function with the name of the function we want to debug as an argument. debug(my_function) Then, when we call my_function(), R will enter debug mode and allow us to execute the code line by line. traceback(): This function shows us the sequence of function calls that led to the error. This can be useful for understanding how the error was reached and which functions are involved. To use traceback(), simply call the function after an error has occurred. traceback() R will show a list of the functions that were called, starting with the function where the error occurred and ending with the function that started the code execution. 2.6.3 Error handling: tryCatch() Sometimes, we want our code to continue executing even if an error occurs. For this, we can use the tryCatch() function. tryCatch() allows us to specify a block of code that will be executed if an error occurs. We can also specify a block of code that will be executed if no error occurs. tryCatch( { # Code that might produce an error }, error = function(e) { # Code to be executed if an error occurs }, finally = { # Code to be executed always, whether or not there is an error } ) For example, if we are reading data from a file and the file does not exist, we can use tryCatch() to show an error message and continue with code execution. tryCatch( { data &lt;- read.csv(&quot;my_file.csv&quot;) }, error = function(e) { print(&quot;Error reading file. Please verify the file exists.&quot;) } ) 2.6.4 Examples: debugging functions with errors, handling exceptions Let’s see some examples of how to use debugging tools and error handling in R: Debugging a function with debug(): Imagine we create a function to calculate a person’s Body Mass Index (BMI), but when using it, we get an error. We can use debug() to analyze what happens inside the function. calculate_bmi &lt;- function(weight, height) { bmi &lt;- weight / (height ^ 2) return(bmi) } debug(calculate_bmi) calculate_bmi(weight = 70, height = 1.75) # We call the function to start debugging When executing this code, R will enter debug mode. In the console, we will see a new prompt Browse[1]&gt;. We can use commands like n (next) to execute the next line of code, c (continue) to continue normal execution, or Q to exit debug mode. We can also print the value of variables using their name (e.g. weight, height, bmi). Handling an exception with tryCatch(): Suppose we are creating a function to calculate the annual population growth rate of a city. If the initial population is 0, the division will produce an error. We can use tryCatch() to handle this situation: calculate_growth_rate &lt;- function(initial_population, final_population, years) { tryCatch( { rate &lt;- ((final_population / initial_population)^(1 / years) - 1) * 100 return(rate) }, error = function(e) { message(&quot;Error: Initial population cannot be zero.&quot;) return(NA) } ) } calculate_growth_rate(10000, 12000, 5) # Output: 3.7137... #&gt; [1] 3.713729 calculate_growth_rate(0, 12000, 5) # Output: &quot;Error: Initial population cannot be zero.&quot; #&gt; [1] Inf In this example, if initial_population is 0, tryCatch() captures the error and displays a message. Then, it returns NA to indicate that calculation could not be performed. With practice, you will learn to use these tools to debug your code, handle errors, and write more robust and reliable programs. 2.7 Exercises It’s time to test your skills with functions! Below, you will find a series of exercises with different levels of difficulty. Create a function called miles_to_kilometers() converting miles to kilometers. The function should receive a miles argument and return the equivalent in kilometers. (Remember that 1 mile equals 1.60934 kilometers). Solution miles_to_kilometers &lt;- function(miles) { kilometers &lt;- miles * 1.60934 return(kilometers) } Create a function called triangle_area() calculating the area of a triangle. The function should receive two arguments: base and height, and return the triangle’s area. (Remember that the area of a triangle is equal to (base * height) / 2). Solution triangle_area &lt;- function(base, height) { area &lt;- (base * height) / 2 return(area) } Create a function called price_with_vat() calculating the price of a product including VAT. The function should receive two arguments: price_without_vat and vat_rate (default, 0.16), and return the price with VAT. Solution price_with_vat &lt;- function(price_without_vat, vat_rate = 0.16) { price_with_vat &lt;- price_without_vat * (1 + vat_rate) return(price_with_vat) } Create a function called is_even() determining if a number is even. The function should receive a number argument and return TRUE if the number is even and FALSE if not. (Hint: use the modulo operator %%). Solution is_even &lt;- function(number) { return(number %% 2 == 0) } Create a function called factorial() calculating the factorial of a number. The factorial of a positive integer n, denoted by n!, is the product of all positive integers less than or equal to n. For example, 5! = 5 * 4 * 3 * 2 * 1 = 120. (Hint: use a recursive function). Solution factorial &lt;- function(n) { if (n == 0) { return(1) } else { return(n * factorial(n - 1)) } } Create a function called fibonacci() generating a Fibonacci sequence of a given length. The Fibonacci sequence is a series of numbers where each number is the sum of the two preceding ones. The sequence typically starts with 0 and 1. For example, a Fibonacci sequence of length 10 would be: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34. Solution fibonacci &lt;- function(n) { if (n &lt;= 0) { return(numeric(0)) } else if (n == 1) { return(0) } else if (n == 2) { return(c(0, 1)) } else { fib_seq &lt;- numeric(n) fib_seq[1] &lt;- 0 fib_seq[2] &lt;- 1 for (i in 3:n) { fib_seq[i] &lt;- fib_seq[i - 1] + fib_seq[i - 2] } return(fib_seq) } } fibonacci(10) #&gt; [1] 0 1 1 2 3 5 8 13 21 34 Create a function called gcd() calculating the greatest common divisor (GCD) of two numbers. The GCD of two or more non-zero integers is the largest positive integer that divides them without a remainder. For example, the GCD of 12 and 18 is 6. (Hint: use the Euclidean algorithm). Solution gcd &lt;- function(a, b) { while (b != 0) { temp &lt;- b b &lt;- a %% b a &lt;- temp } return(a) } Create a function called validate_password() validating a password. The function should receive a password argument and return TRUE if the password meets the following conditions, and FALSE otherwise: Has at least 8 characters. Contains at least one uppercase letter. Contains at least one lowercase letter. Contains at least one number. Contains at least one special character (!@#$%^&amp;*). Solution validate_password &lt;- function(password) { if (nchar(password) &lt; 8) { return(FALSE) } if (!grepl(&quot;[A-Z]&quot;, password)) { return(FALSE) } if (!grepl(&quot;[a-z]&quot;, password)) { return(FALSE) } if (!grepl(&quot;[0-9]&quot;, password)) { return(FALSE) } if (!grepl(&quot;[!@#$%^&amp;*]&quot;, password)) { return(FALSE) } return(TRUE) } Create a function called apply_discount() receiving a price calculation function and a discount as arguments. The apply_discount() function should return a new function calculating the price with the discount applied. Solution apply_discount &lt;- function(price_function, discount) { function(original_price) { discounted_price &lt;- price_function(original_price) * (1 - discount) return(discounted_price) } } Create a function called create_temperature_converter() receiving a temperature scale as argument (“Celsius”, “Fahrenheit” or “Kelvin”). The function should return a function converting temperatures to the specified scale. Solution create_temperature_converter &lt;- function(scale) { if (scale == &quot;Celsius&quot;) { return(function(temp) (temp - 32) * 5 / 9) # Fahrenheit to Celsius } else if (scale == &quot;Fahrenheit&quot;) { return(function(temp) (temp * 9 / 5) + 32) # Celsius to Fahrenheit } else if (scale == &quot;Kelvin&quot;) { return(function(temp) temp + 273.15) # Celsius to Kelvin } else { stop(&quot;Invalid temperature scale.&quot;) } } Create a function called guess_number() simulating a guess the number game. The function should generate a random number between 1 and 100 and ask the user to guess it. The function should give hints to the user (higher or lower) and count the number of attempts. (Hint: use a closure to store the secret number and the number of attempts). Solution guess_number &lt;- function() { secret_number &lt;- sample(1:100, 1) attempts &lt;- 0 guess &lt;- function() { attempts &lt;&lt;- attempts + 1 cat(&quot;Attempt&quot;, attempts, &quot;: &quot;) number &lt;- as.numeric(readline()) if (is.na(number)) { cat(&quot;Please enter a valid number.\\n&quot;) } else if (number &lt; secret_number) { cat(&quot;The secret number is higher.\\n&quot;) } else if (number &gt; secret_number) { cat(&quot;The secret number is lower.\\n&quot;) } else { cat(&quot;You guessed it! The secret number was&quot;, secret_number, &quot;\\n&quot;) cat(&quot;It took you&quot;, attempts, &quot;attempts.\\n&quot;) } } return(guess) } game &lt;- guess_number() game() #&gt; Attempt 1 : #&gt; Please enter a valid number. Create a function that, given a vector of integers, finds the contiguous subsequence with the maximum sum. For example, for the vector c(-2, 1, -3, 4, -1, 2, 1, -5, 4), the contiguous subsequence with the maximum sum is c(4, -1, 2, 1), with a sum of 6. Solution max_subsequence &lt;- function(x) { current_max &lt;- 0 global_max &lt;- 0 start &lt;- 1 end &lt;- 1 temp_start &lt;- 1 for (i in 1:length(x)) { current_max &lt;- current_max + x[i] if (current_max &gt; global_max) { global_max &lt;- current_max start &lt;- temp_start end &lt;- i } if (current_max &lt; 0) { current_max &lt;- 0 temp_start &lt;- i + 1 } } return(list(subsequence = x[start:end], sum = global_max)) } test &lt;- c(-2, 1, -3, 4, -1, 2, 1, -5, 4) max_subsequence(test) #&gt; $subsequence #&gt; [1] 4 -1 2 1 #&gt; #&gt; $sum #&gt; [1] 6 Create a function that, given a character vector, determines if it is possible to obtain a palindrome by rearranging its letters. A palindrome is a word or phrase that reads the same left to right as right to left (e.g. “radar”). Solution is_palindrome_possible &lt;- function(text) { letters &lt;- strsplit(tolower(text), &quot;&quot;)[[1]] frequencies &lt;- table(letters) odds &lt;- sum(frequencies %% 2) return(odds &lt;= 1) } test &lt;- c(&quot;radar&quot;, &quot;hello&quot;, &quot;abb&quot;) result &lt;- sapply(test, is_palindrome_possible) result #&gt; radar hello abb #&gt; TRUE FALSE TRUE Create a function that, given a positive integer, determines if it is a prime number. A prime number is a natural number greater than 1 that has no divisors other than 1 and itself. Solution is_prime &lt;- function(n) { if (n &lt;= 1) { return(FALSE) } if (n &lt;= 3) { return(TRUE) } if (n %% 2 == 0 || n %% 3 == 0) { return(FALSE) } i &lt;- 5 while (i * i &lt;= n) { if (n %% i == 0 || n %% (i + 2) == 0) { return(FALSE) } i &lt;- i + 6 } return(TRUE) } The condition i * i &lt;= n in the while loop limits iterations to the square root of n. This optimizes the algorithm, as it is not necessary to check divisors greater than the square root of n. The increment i &lt;- i + 6 is based on the observation that all prime numbers greater than 3 can be expressed in the form 6k ± 1. Therefore, only numbers of the form 6k ± 1 need to be checked as possible divisors. "],["data-frames.html", "Chapter 3 Data Frames 3.1 Introduction to Data Frames 3.2 Creating Data Frames: Building your database for the move 3.3 Exploring Data Frames: Discovering the secrets of your data 3.4 Manipulating Data Frames: Transforming your data 3.5 Exercises 3.6 Data frames in plots 3.7 Data interpretation 3.8 Exercises", " Chapter 3 Data Frames 3.1 Introduction to Data Frames In previous chapters, we explored different types of objects in R, such as variables, vectors, lists, and matrices. These objects allow us to store information in more efficient ways. Now, in this chapter, we will delve into the world of data frames, an essential tool for organizing and analyzing information that will help you make the best decision about your move to the United States. 3.1.1 What are data frames? Imagine a spreadsheet, with rows and columns organizing information in a tabular way. In R, a data frame is precisely that: a data structure that stores information in a tabular format, with rows representing observations (for example, every US city) and columns representing variables (such as population, cost of living, crime rate). Each column of a data frame can contain a different data type: numeric, character, logical, factor, etc. This makes data frames very versatile for storing diverse information. For example, a data frame about US cities could contain the following columns: city: Name of the city (character). state: State the city belongs to (character). population: Population of the city (numeric). area: Area of the city in square kilometers (numeric). has_beach: Logical value indicating if the city has a beach (TRUE or FALSE). 3.1.2 Why data frames? In R, there are various structures for organizing data, such as vectors, lists, and matrices. However, data frames stand out as a fundamental tool in data analysis. Why? Data frames offer a unique combination of features that make them ideal for representing and manipulating complex information: Tabular structure: They organize data in rows and columns, like a spreadsheet, which facilitates visualization and understanding. Flexibility in data types: Each column can contain a different data type (numbers, text, dates, etc.), which allows representing the diversity of real-world information. Efficiency in analysis: Most data analysis functions and packages in R are designed to work with data frames. In summary, data frames are a versatile and powerful data structure that adapts to the needs of modern data analysis. 3.1.3 Data Frames in action: exploring information about the United States In the context of your move to the United States, data frames will be essential for organizing and analyzing the information you need to make the best decision. We can use data frames to store information about: Crime: Crime rates in different states. Cost of living: Cost of housing, food, transportation in different cities. Climate: Average temperatures, precipitation, days of sunshine in different regions. Demographics: Population, average age, education level in different states. With this information organized in data frames, you will be able to perform deeper analyses and make more informed decisions about your move. 3.2 Creating Data Frames: Building your database for the move Now that you know what data frames are and why they are so important in data analysis, it’s time to learn how to create them. In R, we can create data frames in different ways: importing data from external files or creating them manually. 3.2.1 Importing data from files: CSV, Excel A common way to create data frames is by importing data from external files, such as CSV (Comma Separated Values) files or Excel files. R offers us functions to read data from different formats. Importing data from CSV files: To import data from a CSV file, we use the read.csv() function. url &lt;- &quot;https://dparedesi.github.io/DS-con-R/notas-estudiantes.csv&quot; # Import data from a CSV file called &quot;notas-estudiantes.csv&quot; cities &lt;- read.csv(url) cities #&gt; inicio genero tipo P1 P2 P3 P4 P5 P6 #&gt; 1 03/05/2020 mujer Trabajo individual 1 5 5 5 5 5 5 #&gt; 2 03/05/2020 hombre Trabajo individual 1 5 5 5 5 4 5 #&gt; 3 03/05/2020 mujer Trabajo individual 1 5 5 4 5 5 5 #&gt; 4 03/05/2020 hombre Trabajo individual 1 5 5 5 5 5 5 #&gt; 5 03/05/2020 hombre Trabajo individual 1 2 5 5 5 5 5 #&gt; 6 03/05/2020 hombre Trabajo individual 1 5 4 5 1 5 5 #&gt; 7 03/05/2020 hombre Trabajo individual 1 2 1 5 5 2 5 #&gt; 8 03/05/2020 hombre Trabajo individual 1 5 5 5 5 5 5 #&gt; 9 03/05/2020 hombre Trabajo individual 1 4 5 5 5 5 5 #&gt; 10 03/05/2020 hombre Trabajo individual 1 3 4 5 5 5 5 #&gt; 11 03/05/2020 hombre Trabajo individual 1 2 5 5 5 5 5 #&gt; 12 03/05/2020 mujer Trabajo individual 1 1 1 5 5 5 1 #&gt; 13 03/05/2020 hombre Trabajo individual 1 5 5 5 5 5 5 #&gt; 14 03/05/2020 mujer Trabajo individual 1 3 5 5 1 1 1 #&gt; 15 03/05/2020 hombre Trabajo individual 1 4 5 5 5 5 5 #&gt; 16 03/05/2020 mujer Trabajo individual 1 5 5 5 5 5 5 #&gt; 17 03/05/2020 hombre Trabajo individual 1 5 1 5 5 1 1 #&gt; 18 03/05/2020 mujer Trabajo individual 1 4 5 5 5 5 5 #&gt; 19 03/05/2020 hombre Trabajo individual 1 5 3 5 5 5 2 #&gt; 20 03/05/2020 hombre Trabajo individual 1 3 5 5 5 5 5 #&gt; 21 03/05/2020 hombre Trabajo individual 1 1 4 4 4 5 5 The read.csv() function has several optional arguments that allow us to customize data import. Some of the most common arguments are: header: Indicates if the file has a header row (TRUE or FALSE). sep: Specifies the character used to separate columns (by default, the comma “,”). dec: Specifies the character used to separate decimals (by default, the dot “.”). Importing data from Excel files: To import data from an Excel file, we can use the read_excel() function from the readxl package. # Install the readxl package (if you don&#39;t have it installed) install.packages(&quot;readxl&quot;) # Load the readxl package library(readxl) # Import data from an Excel file called &quot;states.xlsx&quot; states &lt;- read_excel(&quot;states.xlsx&quot;) The read_excel() function has several optional arguments, such as sheet to specify the spreadsheet to import. 3.2.2 Creating data frames manually We can also create data frames manually, combining vectors with the data.frame() function. # Create vectors with information about cities cities &lt;- c(&quot;New York&quot;, &quot;Los Angeles&quot;, &quot;Chicago&quot;) states &lt;- c(&quot;New York&quot;, &quot;California&quot;, &quot;Illinois&quot;) population &lt;- c(8.4e6, 3.9e6, 2.7e6) # Create a data frame with city information df_cities &lt;- data.frame(city = cities, state = states, population = population) df_cities #&gt; city state population #&gt; 1 New York New York 8400000 #&gt; 2 Los Angeles California 3900000 #&gt; 3 Chicago Illinois 2700000 In this example, we create a data frame called df_cities with three columns: city, state, and population. Each column is created from a vector. Note that the vectors must have the same length to be combined into a data frame. 3.2.3 Examples We can use data frames to organize diverse information about our move to the United States. For example, we could create a data frame with information about different cities, including their cost of living, crime rate, and climate. We could also create a data frame with information about the different states, including their population, gross domestic product (GDP), and education system. # Create a data frame with information about cities df_cities &lt;- data.frame( city = c(&quot;New York&quot;, &quot;Los Angeles&quot;, &quot;Chicago&quot;, &quot;Houston&quot;), state = c(&quot;New York&quot;, &quot;California&quot;, &quot;Illinois&quot;, &quot;Texas&quot;), cost_of_living = c(3.5, 2.8, 2.5, 2.0), # In thousands of dollars crime_rate = c(400, 350, 500, 450), # Per 100,000 inhabitants climate = c(&quot;Temperate&quot;, &quot;Mediterranean&quot;, &quot;Continental&quot;, &quot;Subtropical&quot;) ) df_cities #&gt; city state cost_of_living crime_rate climate #&gt; 1 New York New York 3.5 400 Temperate #&gt; 2 Los Angeles California 2.8 350 Mediterranean #&gt; 3 Chicago Illinois 2.5 500 Continental #&gt; 4 Houston Texas 2.0 450 Subtropical # Create a data frame with information about states df_states &lt;- data.frame( state = c(&quot;California&quot;, &quot;Texas&quot;, &quot;Florida&quot;, &quot;New York&quot;), population = c(39.2e6, 29.0e6, 21.4e6, 19.4e6), gdp = c(3.2e12, 1.8e12, 1.1e12, 1.7e12), # In dollars education_system = c(&quot;Good&quot;, &quot;Regular&quot;, &quot;Good&quot;, &quot;Excellent&quot;) ) df_states #&gt; state population gdp education_system #&gt; 1 California 39200000 3.2e+12 Good #&gt; 2 Texas 29000000 1.8e+12 Regular #&gt; 3 Florida 21400000 1.1e+12 Good #&gt; 4 New York 19400000 1.7e+12 Excellent These data frames will allow us to analyze the information more efficiently and make more informed decisions about our move. 3.3 Exploring Data Frames: Discovering the secrets of your data We have already learned to create data frames, now it is time to explore their content and discover the information they hide. R offers us various tools to examine and understand our data. 3.3.1 Accessing rows, columns, and cells A data frame is like a map organized in rows and columns. To access the information we need, we must know how to navigate this map. R provides us with different ways to access rows, columns, and cells of a data frame. Accessing columns: We can access a column of a data frame using the $ operator followed by the column name. # Access the &quot;state&quot; column of the &quot;df_cities&quot; data frame df_cities$state #&gt; [1] &quot;New York&quot; &quot;California&quot; &quot;Illinois&quot; &quot;Texas&quot; We can also access a column using the data frame name followed by brackets and the column name in quotes. # Access the &quot;population&quot; column of the &quot;df_states&quot; data frame df_states[&quot;population&quot;] #&gt; population #&gt; 1 39200000 #&gt; 2 29000000 #&gt; 3 21400000 #&gt; 4 19400000 Accessing rows: We can access a row of a data frame using brackets and the row number. # Access the third row of the &quot;df_cities&quot; data frame df_cities[3, ] #&gt; city state cost_of_living crime_rate climate #&gt; 3 Chicago Illinois 2.5 500 Continental Accessing cells: We can access a specific cell of a data frame using brackets and specifying the row and column. # Access the cell in row 2, column 3 of the &quot;df_states&quot; data frame df_states[2, 3] #&gt; [1] 1.8e+12 Filtering rows with conditions: We can use logical conditions to filter the rows of a data frame. For example, if we want to get cities with a cost of living less than 3: df_cities[df_cities$cost_of_living &lt; 3, ] #&gt; city state cost_of_living crime_rate climate #&gt; 2 Los Angeles California 2.8 350 Mediterranean #&gt; 3 Chicago Illinois 2.5 500 Continental #&gt; 4 Houston Texas 2.0 450 Subtropical 3.3.2 Functions for exploring data frames R offers several useful functions for exploring data frames: head(): Shows the first 6 rows of the data frame. head(df_cities) #&gt; city state cost_of_living crime_rate climate #&gt; 1 New York New York 3.5 400 Temperate #&gt; 2 Los Angeles California 2.8 350 Mediterranean #&gt; 3 Chicago Illinois 2.5 500 Continental #&gt; 4 Houston Texas 2.0 450 Subtropical tail(): Shows the last 6 rows of the data frame. tail(df_states) #&gt; state population gdp education_system #&gt; 1 California 39200000 3.2e+12 Good #&gt; 2 Texas 29000000 1.8e+12 Regular #&gt; 3 Florida 21400000 1.1e+12 Good #&gt; 4 New York 19400000 1.7e+12 Excellent str(): Shows the structure of the data frame, including column names, data type of each column, and the first values of each column. str(df_cities) #&gt; &#39;data.frame&#39;: 4 obs. of 5 variables: #&gt; $ city : chr &quot;New York&quot; &quot;Los Angeles&quot; &quot;Chicago&quot; &quot;Houston&quot; #&gt; $ state : chr &quot;New York&quot; &quot;California&quot; &quot;Illinois&quot; &quot;Texas&quot; #&gt; $ cost_of_living: num 3.5 2.8 2.5 2 #&gt; $ crime_rate : num 400 350 500 450 #&gt; $ climate : chr &quot;Temperate&quot; &quot;Mediterranean&quot; &quot;Continental&quot; &quot;Subtropical&quot; summary(): Provides descriptive statistics for each column of the data frame, such as mean, median, minimum and maximum values, etc. summary(df_states) #&gt; state population gdp education_system #&gt; Length:4 Min. :19400000 Min. :1.10e+12 Length:4 #&gt; Class :character 1st Qu.:20900000 1st Qu.:1.55e+12 Class :character #&gt; Mode :character Median :25200000 Median :1.75e+12 Mode :character #&gt; Mean :27250000 Mean :1.95e+12 #&gt; 3rd Qu.:31550000 3rd Qu.:2.15e+12 #&gt; Max. :39200000 Max. :3.20e+12 View(): Opens a window with an interactive view of the data frame, similar to a spreadsheet. View(df_cities) 3.3.3 Examples: exploring data frames with move information By exploring the data frames we created in the previous section, we can obtain valuable information about US cities and states. For example, we could use summary() to get descriptive statistics of the cost of living in different cities, or View() to examine information about each state in detail. # Get descriptive statistics of cost of living in different cities summary(df_cities$cost_of_living) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 2.000 2.375 2.650 2.700 2.975 3.500 # Examine detailed information about each state View(df_states) In addition to the mentioned functions, we can use other tools to explore our data frames. For example, we can use the table() function to get the frequency of each value in a categorical column, such as the climate column in the df_cities data frame. table(df_cities$climate) #&gt; #&gt; Continental Mediterranean Subtropical Temperate #&gt; 1 1 1 1 We can also use the hist() function to create a histogram of a numeric column, such as the population column in the df_states data frame. hist(df_states$population) These are just some ideas of how we can explore our data frames. As you become familiar with R, you will discover new functions and techniques for analyzing and visualizing your data. 3.4 Manipulating Data Frames: Transforming your data In the previous section, we learned to explore data frames and access the information they contain. Now, we will go a step further and learn to manipulate data frames, transforming data to answer specific questions and obtain relevant information for our move. 3.4.1 Introduction to the pipeline operator (|&gt;) Before manipulating data frames, we will introduce a tool to write more readable and efficient code: the native pipeline operator (|&gt;). This operator was introduced in R 4.1 (2021) as a built-in language feature, meaning it works without any additional packages. The pipeline operator allows us to chain several operations sequentially. Instead of writing nested code, we can use the pipeline operator to “pass” the result of one operation to the next. To use additional data manipulation functions, we’ll load the tidyverse package, which includes dplyr - a package with many useful functions for working with data frames. A package in R is like a toolbox with additional functions and data for performing specific tasks. To use a package’s functions, we must first install it and then load it into our working environment. To install the tidyverse package, we can use the following instruction in the R console: install.packages(&quot;tidyverse&quot;) This will install tidyverse and all the packages it contains, including dplyr. Once the package is installed, we can load it with the library() function: library(tidyverse) Now we can use the pipeline operator (|&gt;) and functions from dplyr. For example, if we want to view only the state, population, and total data from the murders data frame (from the dslabs package), we can use a pipeline: install.packages(&quot;dslabs&quot;) # Load library and dataset library(dslabs) data(murders) # Pipeline murders |&gt; select(state, population, total) #&gt; state population total #&gt; 1 Alabama 4779736 135 #&gt; 2 Alaska 710231 19 #&gt; 3 Arizona 6392017 232 #&gt; 4 Arkansas 2915918 93 #&gt; 5 California 37253956 1257 #&gt; 6 Colorado 5029196 65 #&gt; 7 Connecticut 3574097 97 #&gt; 8 Delaware 897934 38 #&gt; 9 District of Columbia 601723 99 #&gt; 10 Florida 19687653 669 #&gt; 11 Georgia 9920000 376 #&gt; 12 Hawaii 1360301 7 #&gt; 13 Idaho 1567582 12 #&gt; 14 Illinois 12830632 364 #&gt; 15 Indiana 6483802 142 #&gt; 16 Iowa 3046355 21 #&gt; 17 Kansas 2853118 63 #&gt; 18 Kentucky 4339367 116 #&gt; 19 Louisiana 4533372 351 #&gt; 20 Maine 1328361 11 #&gt; 21 Maryland 5773552 293 #&gt; 22 Massachusetts 6547629 118 #&gt; 23 Michigan 9883640 413 #&gt; 24 Minnesota 5303925 53 #&gt; 25 Mississippi 2967297 120 #&gt; 26 Missouri 5988927 321 #&gt; 27 Montana 989415 12 #&gt; 28 Nebraska 1826341 32 #&gt; 29 Nevada 2700551 84 #&gt; 30 New Hampshire 1316470 5 #&gt; 31 New Jersey 8791894 246 #&gt; 32 New Mexico 2059179 67 #&gt; 33 New York 19378102 517 #&gt; 34 North Carolina 9535483 286 #&gt; 35 North Dakota 672591 4 #&gt; 36 Ohio 11536504 310 #&gt; 37 Oklahoma 3751351 111 #&gt; 38 Oregon 3831074 36 #&gt; 39 Pennsylvania 12702379 457 #&gt; 40 Rhode Island 1052567 16 #&gt; 41 South Carolina 4625364 207 #&gt; 42 South Dakota 814180 8 #&gt; 43 Tennessee 6346105 219 #&gt; 44 Texas 25145561 805 #&gt; 45 Utah 2763885 22 #&gt; 46 Vermont 625741 2 #&gt; 47 Virginia 8001024 250 #&gt; 48 Washington 6724540 93 #&gt; 49 West Virginia 1852994 27 #&gt; 50 Wisconsin 5686986 97 #&gt; 51 Wyoming 563626 5 Code with pipeline is easier to read and understand, as it follows the natural flow of operations. Pipeline creates a view; we are not editing the murders data frame. We can show the first rows using the head() function: head(murders |&gt; select(state, population, total)) #&gt; state population total #&gt; 1 Alabama 4779736 135 #&gt; 2 Alaska 710231 19 #&gt; 3 Arizona 6392017 232 #&gt; 4 Arkansas 2915918 93 #&gt; 5 California 37253956 1257 #&gt; 6 Colorado 5029196 65 We can also use the pipeline operator to show the first rows: murders |&gt; select(state, population, total) |&gt; head() #&gt; state population total #&gt; 1 Alabama 4779736 135 #&gt; 2 Alaska 710231 19 #&gt; 3 Arizona 6392017 232 #&gt; 4 Arkansas 2915918 93 #&gt; 5 California 37253956 1257 #&gt; 6 Colorado 5029196 65 For better readability, we will use one function per line, obtaining the same result: murders |&gt; select(state, population, total) |&gt; # Select columns head() # Show first 6 rows #&gt; state population total #&gt; 1 Alabama 4779736 135 #&gt; 2 Alaska 710231 19 #&gt; 3 Arizona 6392017 232 #&gt; 4 Arkansas 2915918 93 #&gt; 5 California 37253956 1257 #&gt; 6 Colorado 5029196 65 3.4.2 Transforming a table with mutate() We can create new columns or modify existing ones using the mutate() function. For example, to add a column with the homicide rate per 100,000 inhabitants to the murders data frame: murders |&gt; mutate(ratio = total / population * 100000) |&gt; head() #&gt; state abb region population total ratio #&gt; 1 Alabama AL South 4779736 135 2.824424 #&gt; 2 Alaska AK West 710231 19 2.675186 #&gt; 3 Arizona AZ West 6392017 232 3.629527 #&gt; 4 Arkansas AR South 2915918 93 3.189390 #&gt; 5 California CA West 37253956 1257 3.374138 #&gt; 6 Colorado CO West 5029196 65 1.292453 This creates a view with the additional ratio column. If we want to modify the murders data frame directly, we use the assignment operator &lt;-: murders &lt;- murders |&gt; mutate(ratio = total / population * 100000) 3.4.3 Filtering data: selecting cities that interest you We can filter rows meeting a condition using the filter() function. For example, to get states with less than 1 homicide per 100,000 inhabitants: # Load dataset data(murders) murders |&gt; mutate(ratio = total / population * 100000) |&gt; filter(ratio &lt; 1) #&gt; state abb region population total ratio #&gt; 1 Hawaii HI West 1360301 7 0.5145920 #&gt; 2 Idaho ID West 1567582 12 0.7655102 #&gt; 3 Iowa IA North Central 3046355 21 0.6893484 #&gt; 4 Maine ME Northeast 1328361 11 0.8280881 #&gt; 5 Minnesota MN North Central 5303925 53 0.9992600 #&gt; 6 New Hampshire NH Northeast 1316470 5 0.3798036 #&gt; 7 North Dakota ND North Central 672591 4 0.5947151 #&gt; 8 Oregon OR West 3831074 36 0.9396843 #&gt; 9 South Dakota SD North Central 814180 8 0.9825837 #&gt; 10 Utah UT West 2763885 22 0.7959810 #&gt; 11 Vermont VT Northeast 625741 2 0.3196211 #&gt; 12 Wyoming WY West 563626 5 0.8871131 We can use different operators to create our conditions: &gt;: greater than &lt;: less than &gt;=: greater than or equal to &lt;=: less than or equal to ==: equal to !=: different from We can also combine conditions using logical operators: &amp;: AND |: OR !: NOT For example, to filter by ratio less than 1 and West region: murders |&gt; mutate(ratio = total / population * 100000) |&gt; filter(ratio &lt; 1 &amp; region == &quot;West&quot;) #&gt; state abb region population total ratio #&gt; 1 Hawaii HI West 1360301 7 0.5145920 #&gt; 2 Idaho ID West 1567582 12 0.7655102 #&gt; 3 Oregon OR West 3831074 36 0.9396843 #&gt; 4 Utah UT West 2763885 22 0.7959810 #&gt; 5 Wyoming WY West 563626 5 0.8871131 3.4.4 Sorting data: finding the safest cities The arrange() function from the dplyr package allows us to order the rows of a data frame based on one or more columns. Imagine you have a data frame with information about different cities, and you want to order them from safest to least safe, based on their crime rate. Or perhaps you want to order them by cost of living, from cheapest to most expensive. arrange() allows you to do this easily. For example, to order states by homicide rate (from lowest to highest): murders |&gt; mutate(ratio = total / population * 100000) |&gt; arrange(ratio) |&gt; head() #&gt; state abb region population total ratio #&gt; 1 Vermont VT Northeast 625741 2 0.3196211 #&gt; 2 New Hampshire NH Northeast 1316470 5 0.3798036 #&gt; 3 Hawaii HI West 1360301 7 0.5145920 #&gt; 4 North Dakota ND North Central 672591 4 0.5947151 #&gt; 5 Iowa IA North Central 3046355 21 0.6893484 #&gt; 6 Idaho ID West 1567582 12 0.7655102 If we want to sort in descending order, we use the desc() function: murders |&gt; mutate(ratio = total / population * 100000) |&gt; arrange(desc(ratio)) |&gt; head() #&gt; state abb region population total ratio #&gt; 1 District of Columbia DC South 601723 99 16.452753 #&gt; 2 Louisiana LA South 4533372 351 7.742581 #&gt; 3 Missouri MO North Central 5988927 321 5.359892 #&gt; 4 Maryland MD South 5773552 293 5.074866 #&gt; 5 South Carolina SC South 4625364 207 4.475323 #&gt; 6 Delaware DE South 897934 38 4.231937 We can also sort by multiple columns. For example, if we want to sort first by region and then by state (in alphabetical order): murders |&gt; arrange(region, state) |&gt; head() #&gt; state abb region population total #&gt; 1 Connecticut CT Northeast 3574097 97 #&gt; 2 Maine ME Northeast 1328361 11 #&gt; 3 Massachusetts MA Northeast 6547629 118 #&gt; 4 New Hampshire NH Northeast 1316470 5 #&gt; 5 New Jersey NJ Northeast 8791894 246 #&gt; 6 New York NY Northeast 19378102 517 3.4.5 Aggregating and summarizing data: obtaining general overview The summarize() function from the dplyr package allows us to calculate descriptive statistics for one or more columns of a data frame. It’s like summarizing information from our data frame into a single number or a set of numbers. For example, to calculate the mean population of states: murders |&gt; summarize(mean_population = mean(population)) #&gt; mean_population #&gt; 1 6075769 We can combine summarize() with group_by() to calculate statistics by groups. For example, to calculate average population by region: murders |&gt; group_by(region) |&gt; summarize(mean_population = mean(population)) #&gt; # A tibble: 4 × 2 #&gt; region mean_population #&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 Northeast 6146360 #&gt; 2 South 6804378. #&gt; 3 North Central 5577250. #&gt; 4 West 5534273. 3.4.6 Joining data frames: combining information Imagine you have two data frames: one with information about cities (name, population, etc.) and another with information about the states those cities belong to (state name, governor, etc.). If you want to combine information from both data frames to have a single data frame with all information about cities and their states, you can use dplyr join functions. dplyr offers several functions for joining data frames, such as left_join(), right_join(), inner_join(), and full_join(). Each function performs a different type of join, depending on how data frame rows are combined. The left_join() function joins two data frames keeping all rows from the first data frame (the one on the left) and adding columns from the second data frame that match the first data frame’s rows. If a row from the first data frame has no match in the second data frame, new columns will have NA values. For example, if we have a data frame with city information and another with state information, we can join them by the state column: df_cities_states &lt;- left_join(df_cities, df_states, by = &quot;state&quot;) The resulting data frame df_cities_states will contain information from both data frames combined. If a city in df_cities does not have a corresponding state in df_states, columns from df_states will have NA values for that city. Let’s see a concrete example. Suppose we have the following data frames: df_cities &lt;- data.frame( city = c(&quot;New York&quot;, &quot;Los Angeles&quot;, &quot;Chicago&quot;, &quot;Houston&quot;), state = c(&quot;New York&quot;, &quot;California&quot;, &quot;Illinois&quot;, &quot;Texas&quot;) ) df_states &lt;- data.frame( state = c(&quot;California&quot;, &quot;Texas&quot;, &quot;Florida&quot;), governor = c(&quot;Gavin Newsom&quot;, &quot;Greg Abbott&quot;, &quot;Ron DeSantis&quot;) ) # Join data frames by &quot;state&quot; column df_cities_states &lt;- left_join(df_cities, df_states, by = &quot;state&quot;) df_cities_states #&gt; city state governor #&gt; 1 New York New York &lt;NA&gt; #&gt; 2 Los Angeles California Gavin Newsom #&gt; 3 Chicago Illinois &lt;NA&gt; #&gt; 4 Houston Texas Greg Abbott In this example, left_join() combines df_cities and df_states data frames by the state column. Note that “New York” and “Chicago” cities have NA values in the governor column, since their states (“New York” and “Illinois”) are not present in the df_states data frame. The other join functions (right_join(), inner_join(), and full_join()) work similarly, but with different criteria for combining data frame rows. right_join() does the opposite of left_join(): keeps all rows from the right data frame and only rows from the left data frame that match. inner_join() only keeps rows that have matches in both data frames. full_join() keeps all rows from both data frames, adding NA where there are no matches. You can consult dplyr documentation for more information about these functions. 3.4.7 Examples The dplyr functions we have seen allow us to perform complex data transformations to answer specific questions about our move to the United States. Let’s see some examples with R code: Filter cities with a good education system and low cost of living: # Assume we have a &quot;df_cities&quot; data frame with information # about cities, including &quot;education_system&quot; and &quot;cost_of_living&quot; columns df_cities |&gt; filter(education_system == &quot;Good&quot; &amp; cost_of_living &lt; 2.5) Sort states by their GDP per capita: # Assume we have a &quot;df_states&quot; data frame with information # about states, including &quot;gdp&quot; and &quot;population&quot; columns df_states |&gt; mutate(gdp_per_capita = gdp / population) |&gt; arrange(desc(gdp_per_capita)) Join a city data frame with a climate information data frame: # Assume we have a &quot;df_cities&quot; data frame with information # about cities, including &quot;city&quot; column, and a &quot;df_climate&quot; # data frame with climate information, including &quot;city&quot; column df_cities_climate &lt;- left_join(df_cities, df_climate, by = &quot;city&quot;) With these tools, you will be able to explore and analyze information about the United States to make the best decision about your move. 3.5 Exercises Report the state abbreviation abb and population population columns from the murders data frame Solution murders |&gt; select(abb, population) Report all data frame data that are not from the South region. Solution murders |&gt; filter(region != &quot;South&quot;) If we want to filter all records that are from the South and West region we will use %in% instead of == to compare versus a vector Create the vector south_and_west containing values “South” and “West”. Then filter records that are from those two regions. Solution south_and_west &lt;- c(&quot;South&quot;, &quot;West&quot;) murders |&gt; filter(region %in% south_and_west) Add the ratio column to the murders data frame with the murder ratio per 100,000 inhabitants. Then, filter those with a ratio less than 0.5 and are from “South” and “West” regions. Report state, abb, and ratio columns. Solution data(murders) south_and_west &lt;- c(&quot;South&quot;, &quot;West&quot;) murders &lt;- murders |&gt; mutate(ratio = total/population*100000) |&gt; filter(ratio &lt; 0.9 &amp; region %in% south_and_west) |&gt; select(state, abb, ratio) murders To sort using pipeline we use the arrange(x) function, where x is the name of the column we want to take as reference which will sort in ascending order or arrange(desc(x)) to sort in descending order. Modify the code generated in the previous exercise to sort the result by the ratio field. Solution data(murders) south_and_west &lt;- c(&quot;South&quot;, &quot;West&quot;) murders &lt;- murders |&gt; mutate(ratio = total/population*100000) |&gt; filter(ratio &lt; 0.9 &amp; region %in% south_and_west) |&gt; select(state, abb, ratio) |&gt; arrange(ratio) murders So, finally we can know what state options we have to be able to move and solve the presented case. 3.6 Data frames in plots Now we will see some functions that allow us to visualize our data. Little by little we will build more complex and visually more aesthetic graphs to present. First let’s see the most basic functions R presents us. In the next chapter we will see in more detail graph types and in which situations it is recommended to use one or another graph. 3.6.1 Scatter plots One of the most used plots in R is the scatter plot, which is a type of mathematical diagram using Cartesian coordinates to show values for two variables for a set of data (Jarrell 1994, 492). By default we assume the variables to analyze are independent. Thus, the scatter plot will show the degree of correlation (not causality) between the two variables. The simplest way to plot a scatter plot is with the plot(x,y) function, where x and y are vectors indicating the x-axis coordinates and y-axis coordinates of each point we want to plot. For example, let’s see the relationship between population size and total murders. # Let&#39;s store population data in the x_axis object x_axis &lt;- murders$population # Let&#39;s store total murders data in the y_axis object y_axis &lt;- murders$total # With this code we create the scatter plot plot(x_axis, y_axis) We can see a correlation between population and number of cases. Let’s transform the x_axis dividing by one million (\\({10}^6\\)). Thus we will have the x axis expressed in millions. x_axis &lt;- murders$population/10^6 y_axis &lt;- murders$total plot(x_axis, y_axis) 3.6.2 Histograms We can also create histograms from a vector with the hist function. data(murders) murders &lt;- murders |&gt; mutate(ratio = total/population*100000) hist(murders$ratio) The ease R gives us to create graphs will save us time for analysis. From here we can quickly see that most states have a ratio &lt; 5. 3.6.3 Box plot Finally, R allows us to create box plots easily with the boxplot function. So, if we wanted to analyze the distribution of ratio we would use the following code: boxplot(murders$ratio) 3.7 Data interpretation We have seen graphs that can be generated with a line of code, but we need to interpret them. To do so, we need to learn or remember some statistics. Throughout this book we will learn statistical concepts not going deep into the math part, but from the practical part and leveraging that functions already exist in R. Let’s remember our case/problem. We have a list of murders in each of the 51 states. If we order them by the total column we would have: murders |&gt; arrange(total) |&gt; head() #&gt; state abb region population total ratio #&gt; 1 Vermont VT Northeast 625741 2 0.3196211 #&gt; 2 North Dakota ND North Central 672591 4 0.5947151 #&gt; 3 New Hampshire NH Northeast 1316470 5 0.3798036 #&gt; 4 Wyoming WY West 563626 5 0.8871131 #&gt; 5 Hawaii HI West 1360301 7 0.5145920 #&gt; 6 South Dakota SD North Central 814180 8 0.9825837 R provides us with the summary() function, which gives us a summary of a vector’s data. summary(murders$total) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 2.0 24.5 97.0 184.4 268.0 1257.0 Min.: Minimum value of vector 1st Qu.: First quartile Median: Median or second quartile Mean: Average 3rd Qu.: Third quartile Max.: Maximum value of vector 3.7.1 Quartiles To understand quartiles let’s visualize total data in an ordered way. To only obtain a single column in pipeline we will use .$ before the variable name: murders |&gt; arrange(total) |&gt; pull(total) #&gt; [1] 2 4 5 5 7 8 11 12 12 16 19 21 22 27 32 #&gt; [16] 36 38 53 63 65 67 84 93 93 97 97 99 111 116 118 #&gt; [31] 120 135 142 207 219 232 246 250 286 293 310 321 351 364 376 #&gt; [46] 413 457 517 669 805 1257 Quartiles divide our vector into 4 parts with the same amount of data. Given we have 51 values, we would have groups of 51/4 = 12.75. We would have groups of 13 values (3 groups of 13 elements and one of 12 elements). For example, the first group would be composed of these numbers: #&gt; [1] 2 4 5 5 7 8 11 12 12 16 19 21 22 The second group would be composed of these numbers: #&gt; [1] 27 32 36 38 53 63 65 67 84 93 93 97 97 And so on. In total 4 groups made up of 25% of data each. 3.7.1.1 First quartile Therefore, when we see the 1st quartile, 1st Qu., let’s think that is the cut indicating up to where I can find 25% of the data. summary(murders$total) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 2.0 24.5 97.0 184.4 268.0 1257.0 In our example 24.5 indicates that every number less than or equal to that number will be within the first 25% of data (25% of 51 data points = 12.75, rounded to 13 data points). If we list numbers less than or equal to 24.5 we will have this list: murders |&gt; arrange(total) |&gt; filter(total &lt;= 24.5) |&gt; pull(total) #&gt; [1] 2 4 5 5 7 8 11 12 12 16 19 21 22 Which is exactly the same list we obtained previously for the first group. 3.7.1.2 Second quartile or median The second quartile, also called the median (Median), indicates the cut of the second group. The first group contains the first 25% of data, the second group has additional 25%. So this cut would give us exactly the value found in the middle. summary(murders$total) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 2.0 24.5 97.0 184.4 268.0 1257.0 In our example 97 indicates that below that number we will find 50% of total data (50% of 51 data points = 25.5, rounded to 26 data points). murders |&gt; arrange(total) |&gt; filter(total &lt;= 97) |&gt; pull(total) #&gt; [1] 2 4 5 5 7 8 11 12 12 16 19 21 22 27 32 36 38 53 63 65 67 84 93 93 97 #&gt; [26] 97 3.7.1.3 Third quartile The third quartile is the cut of the third group. Up to the median we already had 50%, if we add another 25% of data we would have 75%. summary(murders$total) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 2.0 24.5 97.0 184.4 268.0 1257.0 In our example 268 indicates that below that number we will find 75% of total data (75% of 51 data points = 26.01, rounded to 26 data points). 3.7.2 Interpretation of box plot We are now ready to create a box plot with total murders and interpret results. boxplot(murders$total) The box starts at value 24.5 (first quartile) and ends at value 268 (third quartile). The thick line represents the median (second quartile), 97 in our example. Between the first quartile and third quartile (between 24.5 and 97 for our example) we will find 50% of the data, also called interquartile range or IQR. Outside the box we see a vertical line upwards and another downwards, showing the range of our data. Outside those lines we see dots which are atypical data very far from the mean, known as outliers. We can quickly find to which states these extreme data belong if we sort the table descendingly using the desc function: murders |&gt; arrange(desc(total)) |&gt; head() #&gt; state abb region population total ratio #&gt; 1 California CA West 37253956 1257 3.374138 #&gt; 2 Texas TX South 25145561 805 3.201360 #&gt; 3 Florida FL South 19687653 669 3.398069 #&gt; 4 New York NY Northeast 19378102 517 2.667960 #&gt; 5 Pennsylvania PA Northeast 12702379 457 3.597751 #&gt; 6 Michigan MI North Central 9883640 413 4.178622 We see that in California 1257 cases were reported. That is one of the extreme data points we see in the box plot. 3.7.3 Examples Create variable pop_log10 and store log base 10 data of population (log10() function). Perform the same log base 10 transformation for total murders and store it in variable tot_log10. Generate a scatter plot of these two variables. pop_log10 &lt;- log10(murders$population) tot_log10 &lt;- log10(murders$total) plot(pop_log10, tot_log10) Create a histogram of population in millions (divided by \\({10}^6\\)). hist(murders$population/10^6) Create a box plot of population. boxplot(murders$population) 3.8 Exercises Below, you will find a series of exercises with different levels of difficulty. It is time to put into practice what you have learned in this chapter. Remember you can use dplyr functions like filter(), arrange(), mutate(), summarize(), group_by() and left_join() to manipulate data frames. Create a data frame called my_expenses with the following columns: category: A factor with categories “Housing”, “Transport”, “Food”, and “Entertainment”. january: Expenses in January for each category. february: Expenses in February for each category. march: Expenses in March for each category. Solution my_expenses &lt;- data.frame( category = factor(c(&quot;Housing&quot;, &quot;Transport&quot;, &quot;Food&quot;, &quot;Entertainment&quot;)), january = c(1500, 300, 500, 200), february = c(1500, 250, 400, 150), march = c(1500, 350, 550, 250) ) my_expenses #&gt; category january february march #&gt; 1 Housing 1500 1500 1500 #&gt; 2 Transport 300 250 350 #&gt; 3 Food 500 400 550 #&gt; 4 Entertainment 200 150 250 Use head(), tail(), str() and summary() functions to explore my_expenses data frame. Solution head(my_expenses) #&gt; category january february march #&gt; 1 Housing 1500 1500 1500 #&gt; 2 Transport 300 250 350 #&gt; 3 Food 500 400 550 #&gt; 4 Entertainment 200 150 250 tail(my_expenses) #&gt; category january february march #&gt; 1 Housing 1500 1500 1500 #&gt; 2 Transport 300 250 350 #&gt; 3 Food 500 400 550 #&gt; 4 Entertainment 200 150 250 str(my_expenses) #&gt; &#39;data.frame&#39;: 4 obs. of 4 variables: #&gt; $ category: Factor w/ 4 levels &quot;Entertainment&quot;,..: 3 4 2 1 #&gt; $ january : num 1500 300 500 200 #&gt; $ february: num 1500 250 400 150 #&gt; $ march : num 1500 350 550 250 summary(my_expenses) #&gt; category january february march #&gt; Entertainment:1 Min. : 200 Min. : 150 Min. : 250.0 #&gt; Food :1 1st Qu.: 275 1st Qu.: 225 1st Qu.: 325.0 #&gt; Housing :1 Median : 400 Median : 325 Median : 450.0 #&gt; Transport :1 Mean : 625 Mean : 575 Mean : 662.5 #&gt; 3rd Qu.: 750 3rd Qu.: 675 3rd Qu.: 787.5 #&gt; Max. :1500 Max. :1500 Max. :1500.0 Access february column of my_expenses data frame using $ operator. Then, access the second row of the data frame using brackets. Solution my_expenses$february #&gt; [1] 1500 250 400 150 my_expenses[2, ] #&gt; category january february march #&gt; 2 Transport 300 250 350 Filter my_expenses data frame to get only rows where expenses in january are greater than 400. Solution my_expenses |&gt; filter(january &gt; 400) #&gt; category january february march #&gt; 1 Housing 1500 1500 1500 #&gt; 2 Food 500 400 550 Sort my_expenses data frame descendingly by expenses in march. Solution my_expenses |&gt; arrange(desc(march)) #&gt; category january february march #&gt; 1 Housing 1500 1500 1500 #&gt; 2 Food 500 400 550 #&gt; 3 Transport 300 250 350 #&gt; 4 Entertainment 200 150 250 Add a column called total to my_expenses data frame containing the sum of January, February, and March expenses for each category. Solution my_expenses &lt;- my_expenses |&gt; mutate(total = january + february + march) Calculate mean and standard deviation of total expenses for each category in my_expenses data frame. Solution my_expenses |&gt; summarize(mean_total = mean(total), std_total = sd(total)) #&gt; mean_total std_total #&gt; 1 1862.5 1793.216 Group my_expenses data frame by category and calculate sum of expenses for each month. Solution my_expenses |&gt; group_by(category) |&gt; summarize(sum_january = sum(january), sum_february = sum(february), sum_march = sum(march)) #&gt; # A tibble: 4 × 4 #&gt; category sum_january sum_february sum_march #&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Entertainment 200 150 250 #&gt; 2 Food 500 400 550 #&gt; 3 Housing 1500 1500 1500 #&gt; 4 Transport 300 250 350 Visually analyze the following chart describing total murder distribution by regions. Just by visualizing it, could you point out which region has the smallest data range, ignoring outliers? Which region has the highest median? Solution West has the smallest data range and has two outliers. South has the highest median among all regions. Analyzing solely by seeing a chart allows us to put ourselves in the final observer’s shoes and understand if decisions can be made just with presented information. Create south vector where you store filtered data of total murders occurred in South region. Then, create a histogram of south vector. Solution south &lt;- murders |&gt; filter(region == &quot;South&quot;) |&gt; pull(total) hist(south) Create a new data frame called df_cities_climate combining information from df_cities and df_climate (you must create df_climate data frame with city climate information). Ensure resulting data frame contains all cities from df_cities, even if they don’t have climate information in df_climate. Solution df_climate &lt;- data.frame( city = c(&quot;New York&quot;, &quot;Los Angeles&quot;, &quot;Chicago&quot;), average_temperature = c(12.8, 17.7, 10.7), # In degrees Celsius annual_precipitation = c(1269, 373, 965) # In millimeters ) df_cities_climate &lt;- left_join(df_cities, df_climate, by = &quot;city&quot;) Create a data frame with some missing values (NA). Then, replace missing values with mean of non-missing values in same column. Solution # Create data frame with missing values df_with_na &lt;- data.frame( x = c(1, 2, NA, 4, 5), y = c(NA, 7, 8, NA, 10) ) df_with_na #&gt; x y #&gt; 1 1 NA #&gt; 2 2 7 #&gt; 3 NA 8 #&gt; 4 4 NA #&gt; 5 5 10 # Replace missing values with mean df_with_na &lt;- df_with_na |&gt; mutate(x = ifelse(is.na(x), mean(x, na.rm = TRUE), x), y = ifelse(is.na(y), mean(y, na.rm = TRUE), y)) df_with_na #&gt; x y #&gt; 1 1 8.333333 #&gt; 2 2 7.000000 #&gt; 3 3 8.000000 #&gt; 4 4 8.333333 #&gt; 5 5 10.000000 Create a function called clean_data_frame() receiving a data frame as argument and replacing missing values with mean of non-missing values in each column. Solution clean_data_frame &lt;- function(df) { for (col in names(df)) { if (is.numeric(df[[col]])) { df[[col]] &lt;- ifelse(is.na(df[[col]]), mean(df[[col]], na.rm = TRUE), df[[col]]) } } return(df) } ## Test created function # Create data frame with missing values to test function df_test &lt;- data.frame( age = c(25, 30, NA, 28, 35), height = c(1.75, 1.80, 1.65, NA, 1.70), weight = c(70, 80, 75, 65, NA) ) df_test #&gt; age height weight #&gt; 1 25 1.75 70 #&gt; 2 30 1.80 80 #&gt; 3 NA 1.65 75 #&gt; 4 28 NA 65 #&gt; 5 35 1.70 NA # Apply function to test data frame df_clean &lt;- clean_data_frame(df_test) # Show clean data frame df_clean #&gt; age height weight #&gt; 1 25.0 1.750 70.0 #&gt; 2 30.0 1.800 80.0 #&gt; 3 29.5 1.650 75.0 #&gt; 4 28.0 1.725 65.0 #&gt; 5 35.0 1.700 72.5 References "],["advanced-techniques.html", "Chapter 4 Advanced Techniques 4.1 Metaprogramming: writing code that writes code 4.2 Functional programming: a new paradigm 4.3 R6: The future of OOP in R 4.4 Exercises", " Chapter 4 Advanced Techniques 4.1 Metaprogramming: writing code that writes code In previous chapters, we explored different object types in R and how to use functions to manipulate them. Now, we are going to delve into a more advanced concept: metaprogramming. Metaprogramming is a technique that allows us to write code that generates other code. It’s like having a code factory where we can create new functions and expressions dynamically. Why is this useful? Metaprogramming can be very useful for: Automating repetitive tasks: If we have to write similar code many times, we can use metaprogramming to generate that code automatically. Creating more flexible functions: We can use metaprogramming to create functions that adapt to different situations and data types. Writing more concise and expressive code: Metaprogramming allows us to express complex ideas more concisely. In R, metaprogramming is based on the manipulation of expressions. An expression is a representation of R code as an object. We can create expressions, modify them, and evaluate them to generate new code. 4.1.1 Manipulating expressions: The art of sculpting code In R, metaprogramming relies on manipulating expressions. An expression is a representation of R code as an object. Instead of simply executing the code, we can manipulate it as if it were a block of clay, shaping and modifying it to create new expressions and functions. Think of an expression like a cooking recipe. The recipe contains a set of instructions (ingredients and steps to follow) to create a dish. Similarly, an expression in R contains instructions to perform a task. R offers us several tools to manipulate expressions, as if they were the hands of a sculptor shaping clay: quote(): This function takes R code and “freezes” it into an expression, without evaluating it. It’s like taking the cooking recipe and saving it in a book, without preparing it yet. my_expression &lt;- quote(x + y) my_expression #&gt; x + y In this example, quote(x + y) creates an expression representing the sum of x and y. The expression is stored in the variable my_expression, but the sum is not performed yet. substitute(): This function allows us to substitute variables in an expression with their values. It’s like in the cooking recipe, replacing the word “sugar” with the amount of sugar we want to use. x &lt;- 10 y &lt;- 5 substitute(x + y) #&gt; x + y In this example, substitute(x + y) replaces variables x and y with their values (10 and 5, respectively), resulting in the expression 10 + 5. eval(): This function “unfreezes” an expression and evaluates it, executing the code it contains. It’s like taking the cooking recipe from the book and using it to prepare the dish. eval(quote(x + y)) #&gt; [1] 15 In this example, eval(quote(x + y)) evaluates the expression x + y, performing the sum and returning the result 15. parse(): This function converts text into an expression. It’s like someone dictating the cooking recipe to us, and we write it down on paper to use later. text &lt;- &quot;x * y&quot; expression &lt;- parse(text = text) eval(expression) #&gt; [1] 50 In this example, parse(text = text) converts the text \"x * y\" into an expression representing the multiplication of x and y. With these tools, we can manipulate expressions to create new functions, modify the behavior of existing functions, and generate code dynamically. 4.1.2 Examples Metaprogramming might seem like an abstract concept at first, but its applications are very concrete and powerful. Let’s look at some examples of how we can use metaprogramming in R to create dynamic functions and generate code automatically. Example 1: Creating a function that generates other functions Imagine you need to create several functions performing similar operations, but with some different parameters. For example, functions adding different constants to a number. Instead of writing each function separately, you can use metaprogramming to create a function that generates these functions dynamically. create_sum_function &lt;- function(n) { expression &lt;- substitute(function(x) x + n) eval(expression) } sum_5 &lt;- create_sum_function(5) sum_10 &lt;- create_sum_function(10) sum_5(10) #&gt; [1] 15 sum_10(10) #&gt; [1] 20 In this example, the create_sum_function() function receives a number n as an argument and generates a new function adding n to its argument. The substitute() function is used to create an expression representing the function we want to generate, and the eval() function is used to evaluate the expression and create the function. Example 2: Generating code for data analysis Suppose you want to perform a data analysis involving several steps, such as filtering data, calculating statistics, and generating a plot. You can use metaprogramming to generate the code for this analysis dynamically, based on specified parameters. analyze_data &lt;- function(data, filter_cond, column_to_analyze, statistic, plot_type) { # Filter data filtered_data &lt;- substitute(data[filter_cond, ][[column_to_analyze]]) filtered_data &lt;- eval(filtered_data) # Calculate statistic calculated_statistic &lt;- substitute(statistic(filtered_data)) calculated_statistic &lt;- eval(calculated_statistic) # Generate plot plot_expression &lt;- substitute(plot_type(filtered_data)) eval(plot_expression) # Return calculated statistic return(calculated_statistic) } # Usage example df &lt;- data.frame( x = c(1, 3, 2, 5.5, 4, 3.5, 8, 7, 9, 10), y = c(10, 8, 9, 6, 7, 5, 3.6, 4, 2, 1) ) # We want to filter data where x &gt; 5, calculate mean of y and generate a histogram result &lt;- analyze_data(df, df$x &gt; 5, &quot;y&quot;, mean, hist) result #&gt; [1] 3.32 Example 3: Creating a function to generate plots with dynamic variable names and advanced options Imagine you need to create a function generating different types of plots (scatter, histograms, boxplots) with custom options like titles, labels, colors, and legends, and that can also handle different datasets and variables. In this case, metaprogramming can be very useful to create a flexible function adapting to these needs. create_plot &lt;- function(data, plot_type, var_x, var_y = NULL, title = NULL, color = &quot;blue&quot;, labels_x = NULL, labels_y = NULL, legend = NULL) { # Create base plot expression if (plot_type == &quot;scatter&quot;) { expression &lt;- substitute(plot(data[[var_x]], data[[var_y]], xlab = labels_x, ylab = labels_y, main = title, col = color)) } else if (plot_type == &quot;histogram&quot;) { expression &lt;- substitute(hist(data[[var_x]], main = title, xlab = labels_x, col = color)) } else if (plot_type == &quot;boxplot&quot;) { expression &lt;- substitute(boxplot(data[[var_x]], main = title, ylab = labels_y, col = color)) } else { stop(&quot;Invalid plot type.&quot;) } # Evaluate base expression eval(expression) # Add legend if specified if (!is.null(legend)) { legend(&quot;topright&quot;, legend = legend, fill = color) } } # Usage example df &lt;- data.frame( x = c(1, 3, 2, 5.5, 4, 3.5, 8, 7, 9, 10), y = c(10, 8, 9, 6, 7, 5, 3.6, 4, 2, 1) ) create_plot(df, &quot;scatter&quot;, &quot;x&quot;, &quot;y&quot;, title = &quot;Scatter Plot&quot;, color = &quot;red&quot;, labels_x = &quot;Variable X&quot;, labels_y = &quot;Variable Y&quot;) create_plot(df, &quot;histogram&quot;, &quot;x&quot;, title = &quot;Histogram of X&quot;, color = &quot;green&quot;, labels_x = &quot;Variable X&quot;) create_plot(df, &quot;boxplot&quot;, &quot;y&quot;, title = &quot;Boxplot of Y&quot;, color = &quot;blue&quot;, labels_y = &quot;Variable X&quot;, legend = c(&quot;Group A&quot;)) In this example, the create_plot() function can generate different types of plots with custom options. The function uses substitute() to construct the base plot expression, and then eval() to evaluate the expression and generate the plot. Additionally, the function can add a legend to the plot if the legend argument is specified. This example illustrates how metaprogramming can be useful for creating more flexible and complex functions that adapt to different needs. 4.2 Functional programming: a new paradigm In previous chapters, we explored different object types in R and how to use functions to manipulate them. We have also seen how metaprogramming allows us to write code that generates other code. Now, we are going to delve into a different programming paradigm: functional programming. Functional programming is a programming style based on the use of pure functions and data immutability. A pure function is a function that always produces the same result for the same arguments, and has no side effects (i.e., it does not modify any data outside the function). Data immutability means data is not modified after being created. Instead of modifying existing data, new data is created with modifications. These principles make functional programming easier to reason about, debug, and maintain. It also facilitates writing concurrent and parallel code, as pure functions have no side effects that can interfere with other processes. 4.2.1 Basic principles of functional programming Functions as first-class citizens: In functional programming, functions are treated like any other data type. They can be passed as arguments to other functions, returned as results from functions, and stored in variables. Pure functions: Pure functions always produce the same result for the same arguments, and have no side effects. This makes code more predictable and easier to debug. Immutability: Data is not modified after being created. Instead of modifying existing data, new data is created with modifications. This avoids errors caused by accidental data modification. Rejection of loops: Functional programming avoids using for and while loops. Instead, higher-order functions like map, reduce, and keep are used to process data collections. 4.2.2 Higher-order functions in R R offers several higher-order functions that are especially useful for functional programming. These functions allow us to manipulate vectors, lists, and other objects concisely and efficiently, avoiding the use of for and while loops. The purrr package offers variants of map() for different types of results: map_dbl() to get a numeric vector, map_chr() to get a character vector, map_lgl() to get a logical vector, etc. map(): Applies a function to each element of a vector or list, and returns a new vector or list with the results. It’s like having a machine taking each element of our data collection, processing it with the function we indicate, and placing the result in a new collection. library(purrr) # Create a vector of numbers numbers &lt;- c(1, 2, 3, 4, 5) # Calculate square of each number using an anonymous function squares &lt;- map(numbers, function(x) x^2) # Show result squares #&gt; [[1]] #&gt; [1] 1 #&gt; #&gt; [[2]] #&gt; [1] 4 #&gt; #&gt; [[3]] #&gt; [1] 9 #&gt; #&gt; [[4]] #&gt; [1] 16 #&gt; #&gt; [[5]] #&gt; [1] 25 # We can also use a predefined function square_roots &lt;- map(numbers, sqrt) # Show result square_roots #&gt; [[1]] #&gt; [1] 1 #&gt; #&gt; [[2]] #&gt; [1] 1.414214 #&gt; #&gt; [[3]] #&gt; [1] 1.732051 #&gt; #&gt; [[4]] #&gt; [1] 2 #&gt; #&gt; [[5]] #&gt; [1] 2.236068 reduce(): Combines elements of a vector or list applying a function cumulatively. It’s like having a machine taking two elements of our collection, combining them using the function we indicate, and then combining the result with the next element, and so on until all elements are combined. # Create a vector of numbers numbers &lt;- c(1, 2, 3, 4, 5) # Calculate sum of numbers using reduce() sum_result &lt;- reduce(numbers, `+`) # Show result sum_result #&gt; [1] 15 # Calculate product of numbers product_result &lt;- reduce(numbers, `*`) # Show result product_result #&gt; [1] 120 keep(): Filters elements of a vector or list meeting a condition. It’s like having a sieve letting pass only elements meeting the condition we indicate. # Create a vector of numbers numbers &lt;- c(1, 2, 3, 4, 5) # Filter even numbers evens &lt;- keep(numbers, ~ . %% 2 == 0) # Show result evens #&gt; [1] 2 4 # Filter numbers greater than 3 greater_than_3 &lt;- keep(numbers, ~ . &gt; 3) # Show result greater_than_3 # Output: 4 5 #&gt; [1] 4 5 The ~ symbol in higher-order functions is used to define an anonymous function. This means you are creating a function “on the fly”, without needing to give it an explicit name. The part following ~ is the body of this function, specifying operations to be performed on each element of the vector or list to which the function is applied. The dot . is used as a placeholder to refer to the current element. These functions, along with other higher-order functions like map2(), pmap(), accumulate(), and every(), give us great flexibility for processing data functionally in R. 4.2.3 Examples Let’s see some examples of how to apply functional programming in R: Calculate the sum of squares of even numbers in a vector: numbers &lt;- c(1, 2, 3, 4, 5) sum_squares_evens &lt;- numbers |&gt; keep(~. %% 2 == 0) |&gt; map_dbl(~. ^2) |&gt; reduce(`+`) sum_squares_evens #&gt; [1] 20 Filter cities with a population greater than 5 million: cities &lt;- list( list(name = &quot;New York&quot;, population = 8.4e6), list(name = &quot;Los Angeles&quot;, population = 3.9e6), list(name = &quot;Chicago&quot;, population = 2.7e6) ) big_cities &lt;- cities |&gt; keep(~.x$population &gt; 5e6) big_cities #&gt; [[1]] #&gt; [[1]]$name #&gt; [1] &quot;New York&quot; #&gt; #&gt; [[1]]$population #&gt; [1] 8400000 In this example, “x” acts as a placeholder to represent each element of the cities list as it iterates over it. That is, in each iteration, “x” will take the value of one of the cities in the list. Why is “x” used? Anonymous function: The expression ~ .x$population &gt; 5e6 defines an anonymous function. This function takes an element of the list as input and returns a logical value (TRUE or FALSE) depending on whether that city’s population is greater than 5 million. Access to elements: The $ symbol is used to access elements of a list. In this case, .x$population accesses the “population” element of the current list element (represented by “x”). Conciseness: Using “x” makes the code more concise and readable, avoiding the need to define a named function explicitly for this operation. You can use any name you want instead of “x”, as long as it is consistent within the anonymous function. Functional programming is a powerful paradigm that can help you write cleaner, more efficient, and maintainable code. As you become familiar with its principles and tools, you will be able to apply them to a wide variety of data analysis problems. 4.3 R6: The future of OOP in R In R, Object-Oriented Programming (OOP) can be implemented in several ways. Traditionally, R has used systems called S3 and S4 for OOP. S3 is an informal and flexible system. It is based on the idea of generic functions, which can have different methods depending on the class of the object they apply to. For example, the print() function is a generic function having different methods for printing different types of objects, such as vectors, lists, or data frames. # Example of generic function in S3 print(c(1, 2, 3)) # Prints a numeric vector #&gt; [1] 1 2 3 print(list(a = 1, b = 2)) # Prints a list #&gt; $a #&gt; [1] 1 #&gt; #&gt; $b #&gt; [1] 2 S4 is a more formal and structured system than S3. It defines classes and methods more explicitly, using special syntax. S4 is often used in packages requiring a more rigorous object structure, like Bioconductor. # Example of class definition in S4 setClass(&quot;Person&quot;, slots = c(name = &quot;character&quot;, age = &quot;numeric&quot;)) # Example of object creation in S4 my_person &lt;- new(&quot;Person&quot;, name = &quot;John&quot;, age = 30) my_person #&gt; An object of class &quot;Person&quot; #&gt; Slot &quot;name&quot;: #&gt; [1] &quot;John&quot; #&gt; #&gt; Slot &quot;age&quot;: #&gt; [1] 30 However, both S3 and S4 can be somewhat confusing and limited, especially for more complex projects. Luckily, there is a more modern and robust alternative: the R6 package. This package offers a more intuitive and efficient way to implement OOP in R, with features facilitating code organization, reuse, and maintenance. If you are new to OOP, don’t worry about S3 and S4 details for now. With R6, you can learn basic OOP concepts more easily and apply them to your data analysis projects. 4.3.1 The R6 package: Classes, methods, encapsulation, and inheritance The R6 package implements a class and object system similar to other object-oriented programming languages like Python or Java. It provides a robust and efficient way to create objects with attributes and methods, allowing encapsulation and inheritance. install.packages(&quot;R6&quot;) library(R6) Classes: A class is like a blueprint or template for creating objects. It defines the attributes (data) and methods (functions) that objects of that class will have. In R6, classes are created with the R6Class() function. # Define a &quot;Person&quot; class Person &lt;- R6Class(&quot;Person&quot;, public = list( name = NULL, age = NULL, # Constructor initialize = function(name, age) { self$name &lt;- name self$age &lt;- age }, # Method to greet greet = function() { cat(&quot;Hello, my name is&quot;, self$name, &quot;and I am&quot;, self$age, &quot;years old.\\n&quot;) } ) ) In this example, a Person class is defined with name and age attributes, and greet() method. The public list defines public members of the class, i.e., attributes and methods accessible from outside the object. Objects: An object is an instance of a class. It is a concrete entity having attributes and methods defined by the class. In R6, objects are created with the $new() method. # Create an object of class &quot;Person&quot; juan &lt;- Person$new(name = &quot;Juan&quot;, age = 30) juan #&gt; &lt;Person&gt; #&gt; Public: #&gt; age: 30 #&gt; clone: function (deep = FALSE) #&gt; greet: function () #&gt; initialize: function (name, age) #&gt; name: Juan Methods: Methods are functions operating on an object’s attributes. They allow accessing and modifying object data, as well as performing other actions. In R6, methods are defined within the public list of the class. # Call greet() method of object &quot;juan&quot; juan$greet() #&gt; Hello, my name is Juan and I am 30 years old. Encapsulation: Encapsulation is a mechanism allowing hiding internal details of an object and controlling access to its attributes. This protects object data and facilitates usage. In R6, encapsulation is achieved by distinguishing between public and private members. Public members are defined in public list and can be accessed from outside the object. Private members are defined in private list and can only be accessed from within the object, through methods. # Define a &quot;BankAccount&quot; class with encapsulation BankAccount &lt;- R6Class(&quot;BankAccount&quot;, public = list( holder = NULL, # Constructor initialize = function(holder) { self$holder &lt;- holder private$balance &lt;- 0 }, # Method to deposit money deposit = function(amount) { private$balance &lt;- private$balance + amount }, # Method to withdraw money withdraw = function(amount) { if (amount &lt;= private$balance) { private$balance &lt;- private$balance - amount } else { stop(&quot;Insufficient funds.&quot;) } }, # Method to check balance check_balance = function() { return(private$balance) } ), private = list( balance = NULL ) ) Inheritance: Inheritance is a mechanism allowing creating new classes from existing classes, inheriting their attributes and methods. This facilitates code reuse and creation of class hierarchies. In R6, inheritance is specified with inherit argument of R6Class() function. # Define a &quot;Student&quot; class inheriting from &quot;Person&quot; Student &lt;- R6Class(&quot;Student&quot;, inherit = Person, public = list( major = NULL, # Constructor initialize = function(name, age, major) { super$initialize(name, age) self$major &lt;- major }, # Method to show student info show_info = function() { super$greet() cat(&quot;Major:&quot;, self$major, &quot;\\n&quot;) } ) ) # Create an object of class &quot;Student&quot; maria &lt;- Student$new(name = &quot;Maria&quot;, age = 20, major = &quot;Engineering&quot;) # Call method show_info() maria$show_info() #&gt; Hello, my name is Maria and I am 20 years old. #&gt; Major: Engineering In this example, Student class inherits from Person class. Student constructor calls parent class constructor (super$initialize()) to initialize inherited attributes. show_info() method calls parent class greet() method (super$greet()) and then shows student-specific information. With R6, you can create classes and objects with a high degree of flexibility and control, allowing you to apply OOP effectively in your data analysis projects. 4.4 Exercises Below, you will find a series of exercises with different levels of difficulty. It is time to put into practice what you have learned in this chapter. Create an expression representing the sum of two variables a and b. Solution expression &lt;- quote(a + b) Create an expression representing the multiplication of two variables x and y, and then evaluate it. Solution x &lt;- 5 y &lt;- 10 expression &lt;- quote(x * y) eval(expression) #&gt; [1] 50 Create a vector of numbers and use map() function to calculate the square of each number. Solution numbers &lt;- c(1, 2, 3, 4, 5) squares &lt;- map(numbers, function(x) x^2) squares #&gt; [[1]] #&gt; [1] 1 #&gt; #&gt; [[2]] #&gt; [1] 4 #&gt; #&gt; [[3]] #&gt; [1] 9 #&gt; #&gt; [[4]] #&gt; [1] 16 #&gt; #&gt; [[5]] #&gt; [1] 25 Create a vector of numbers and use filter() function to get only even numbers. Solution numbers &lt;- c(1, 2, 3, 4, 5) evens &lt;- keep(numbers, ~ . %% 2 == 0) evens #&gt; [1] 2 4 Create a function called create_power_function() receiving a number n as argument and returning a function raising its argument to power n. Solution create_power_function &lt;- function(n) { function(x) x^n } Create a vector of numbers and use reduce() function to calculate the product of all numbers. Solution numbers &lt;- c(1, 2, 3, 4, 5) product &lt;- reduce(numbers, `*`) product #&gt; [1] 120 Create a class called Pet with attributes name, species and age, and methods introduce() (showing name, species and age of pet) and have_birthday() (incrementing pet age by 1). Solution library(R6) Pet &lt;- R6Class(&quot;Pet&quot;, public = list( name = NULL, species = NULL, age = NULL, initialize = function(name, species, age) { self$name &lt;- name self$species &lt;- species self$age &lt;- age }, introduce = function() { cat(&quot;Hello, I am&quot;, self$name, &quot;, a&quot;, self$species, &quot;of&quot;, self$age, &quot;years old.\\n&quot;) }, have_birthday = function() { self$age &lt;- self$age + 1 } ) ) Create a function called create_flexible_sum_function() receiving a number n as argument and returning a function adding n to the sum of all arguments passed to it. Solution create_flexible_sum_function &lt;- function(n) { function(...) { sum(c(...)) + n } } # Tests # Create a function adding 5 to any set of numbers sum_5 &lt;- create_flexible_sum_function(5) # Usage examples and verification sum_5(2, 3, 4) #&gt; [1] 14 sum_5(10, 20) #&gt; [1] 35 sum_5() #&gt; [1] 5 Create a function called create_dynamic_plot() receiving a data frame, a plot type (“scatter”, “histogram” or “boxplot”), and a list of options for the plot (like title, color, labels, etc.). The function should generate the specified plot with given options. Solution create_dynamic_plot &lt;- function(data, plot_type, options) { # Create base plot expression if (plot_type == &quot;scatter&quot;) { expression &lt;- quote(plot(data[[options$var_x]], data[[options$var_y]], xlab = options$labels_x, ylab = options$labels_y, main = options$title, col = options$color)) } else if (plot_type == &quot;histogram&quot;) { expression &lt;- quote(hist(data[[options$var_x]], main = options$title, xlab = options$labels_x, col = options$color)) } else if (plot_type == &quot;boxplot&quot;) { expression &lt;- quote(boxplot(data[[options$var_x]], main = options$title, ylab = options$labels_y, col = options$color)) } else { stop(&quot;Invalid plot type.&quot;) } # Evaluate base expression eval(expression) } # Create sample data data &lt;- data.frame(x = rnorm(100), y = rnorm(100)) # Tests # Scatter plot options_scatter &lt;- list(var_x = &quot;x&quot;, var_y = &quot;y&quot;, title = &quot;Scatter Plot&quot;, labels_x = &quot;Variable X&quot;, labels_y = &quot;Variable Y&quot;, color = &quot;blue&quot;) create_dynamic_plot(data, &quot;scatter&quot;, options_scatter) # Histogram options_histogram &lt;- list(var_x = &quot;x&quot;, title = &quot;Histogram&quot;, labels_x = &quot;Values&quot;, color = &quot;green&quot;) create_dynamic_plot(data, &quot;histogram&quot;, options_histogram) # Boxplot options_boxplot &lt;- list(var_x = &quot;y&quot;, title = &quot;Boxplot&quot;, labels_y = &quot;Values&quot;, color = &quot;red&quot;) create_dynamic_plot(data, &quot;boxplot&quot;, options_boxplot) Create a class called Dog inheriting from Pet class (from previous exercises). Dog class should have an additional attribute called breed and a method called bark(). Solution Dog &lt;- R6Class(&quot;Dog&quot;, inherit = Pet, public = list( breed = NULL, initialize = function(name, age, breed) { super$initialize(name, &quot;dog&quot;, age) self$breed &lt;- breed }, bark = function() { cat(&quot;Woof! Woof!\\n&quot;) } ) ) "],["ggplot-and-dplyr.html", "Chapter 5 Ggplot and dplyr 5.1 Creating the ggplot object 5.2 Aesthetic mapping layer 5.3 Geoms layer 5.4 Scale layer 5.5 Label, title and legend layer 5.6 Reference lines 5.7 Changing the plot style 5.8 Summarizing data with dplyr 5.9 Exercises", " Chapter 5 Ggplot and dplyr We have learned so far how to perform simple and fast visualizations. Now that we have a notion of plots, we will use more complex and aesthetically better plots to present. To do this, we will use the ggplot object included in the tidyverse library (the tidyverse package includes a package called ggplot2 which allows us to use the ggplot object). Previously we have already installed the tidyverse package, so now we only have to load the library: library(tidyverse) We are going to learn visualization techniques using our previous case/problem, so that we learn to use the ggplot object gradually with an already known case/problem. 5.1 Creating the ggplot object We will start by creating the ggplot object from the murders data using the pipeline operator |&gt;. Let’s also remember to have loaded the murders data from the dslabs library. library(dslabs) data(murders) murders |&gt; ggplot() This code only shows us an empty box. This is because we haven’t specified which variables to take from the data frame nor what type of plot we want. To add each component of the plot we are creating we will use layers. The ggplot object allows us to add layer by layer which component of the plot we want to add. The symbol to add layers to the ggplot object is the + symbol. 5.2 Aesthetic mapping layer First we will focus on the basic aesthetics, that is: what goes on the x-axis and what we put on the y-axis. To do this we will use the aesthetic function which in R is aes(). For example, let’s add the population data on the x-axis and the total data on the y-axis. We don’t have to use the $ accessor because the aes function takes the murders table before the pipeline as a reference. murders |&gt; ggplot() + aes(x = population, y = total) Now we have a box with the axes marked, but still without any data inside the box. 5.3 Geoms layer Let’s add one more layer that indicates what type of plot we want. To do this we will use the so-called geoms. There are different types of geoms. For example, a scatter plot is shown with points, therefore we will use the geom_point() function. For more detail we can see the documentation of geom_point() here. murders |&gt; ggplot() + aes(x = population, y = total) + geom_point() In the same way, we can show lines connecting the data instead of points with the geom_line() function. murders |&gt; ggplot() + aes(x = population, y = total) + geom_line() Up to this point we have created the same scatter plot that we saw in the previous chapter. The power of ggplot lies in the ease of adding components. For example, if we want each point in the scatter plot to have the state name, or even better the abbreviation abb, we just have to add it as a label attribute inside aes and add the geom_text() layer. murders |&gt; ggplot() + aes(x = population, y = total, label=abb) + geom_point() + geom_text() In this plot we can already see that the upper right point corresponds to CA which is the abbreviation for the state of California. 5.3.1 Tweaking aes and geoms We can tweak our plots in multiple ways by adding attributes to our functions. For example, if we want to identify which region each point belongs to (if it is from the US North, South, etc.) we would have to edit aes() and make color take into account the region variable as follows: murders |&gt; ggplot() + aes(x = population, y = total, label=abb, color=region) + geom_point() + geom_text() Then, we can also edit the attributes of the geoms. For example, let’s make the size of the points larger. To do this we edit inside geom_points(): murders |&gt; ggplot() + aes(x = population, y = total, label=abb, color=region) + geom_point(size=3) + geom_text() Having increased the size of the points, we can no longer see the text of the state abbreviations well. We can nudge the text on the x-axis or on the y-axis. Since we are talking about several million people, let’s nudge the letters 1.5 million to the right. murders |&gt; ggplot() + aes(x = population, y = total, label=abb, color=region) + geom_point(size=3) + geom_text(nudge_x = 1500000) To avoid entering such large numbers we can transform the population on the x-axis in the aes() function. Thus, once we express the data without counting the millions we would have to nudge the text only 1.5 points to the right: murders |&gt; ggplot() + aes(x = population/10^6, y = total, label=abb, color=region) + geom_point(size=3) + geom_text(nudge_x = 1.5) This transformation gives us the same result as before and the x-axis is now easier to understand now that we can see the numbers. 5.4 Scale layer Visually we can still improve our plot further. We see several data points concentrated in lower values and only a few extremes. In those cases it is better to have a view scaling the axes using logarithms. To do this, we will use the layers scale_x_continuous() and scale_y_continuous(). For example, if we want to transform the scale to base 2 logarithm we would have to add layers, but also change the value of nudge_x, due to the scale change: murders |&gt; ggplot() + aes(x = population/10^6, y = total, label=abb, color=region) + geom_point(size=3) + geom_text(nudge_x = 0.23) + scale_x_continuous(trans = &quot;log2&quot;) + scale_y_continuous(trans = &quot;log2&quot;) In the same way, we could do the transformation to base 10 logarithm: murders |&gt; ggplot() + aes(x = population/10^6, y = total, label=abb, color=region) + geom_point(size=3) + geom_text(nudge_x = 0.075) + scale_x_continuous(trans = &quot;log10&quot;) + scale_y_continuous(trans = &quot;log10&quot;) The transformation of the scale to base 10 logarithm is widely used in statistics and R provides us with a faster function to proceed with this scale transformation, the function scale_x_log10(), which gives us the same result as the previous plot. murders |&gt; ggplot() + aes(x = population/10^6, y = total, label=abb, color=region) + geom_point(size=3) + geom_text(nudge_x = 0.075) + scale_x_log10() + scale_y_log10() 5.5 Label, title and legend layer We can also change the labels (label in English) of the plot. So far on the x-axis we see that population/10^6 appears and we can change it with the function xlab(). In the same way we can change on the y-axis using ylab(). To add a title to the plot we will use the function ggtitle(). To change the name of the legend we will use the function scale_color_discrete(). murders |&gt; ggplot() + aes(x = population/10^6, y = total, label=abb, color=region) + geom_point(size=3) + geom_text(nudge_x = 0.075) + scale_x_log10() + scale_y_log10() + xlab(&quot;Population expressed in millions (logarithmic scale)&quot;) + ylab(&quot;Total number of murders (logarithmic scale)&quot;) + ggtitle(&quot;Gun murders in the US in 2010&quot;) + scale_color_discrete(name = &quot;Regions&quot;) 5.6 Reference lines We can add reference lines, whether vertical with geom_vline(xintercept = ), horizontal with geom_hline(yintercept = ...) or diagonal with geom_abline(intercept = ), the latter asks us at what point it cuts the y-axis and draws a line with a default slope of 1. For example, we could calculate the average of total murders and draw a horizontal reference line. #Calculate the average of the total avg_total &lt;- mean(murders$total) #And add the horizontal reference line murders |&gt; ggplot() + aes(x = population/10^6, y = total, label=abb, color=region) + geom_point(size=3) + geom_text(nudge_x = 0.075) + scale_x_log10() + scale_y_log10() + xlab(&quot;Population expressed in millions (logarithmic scale)&quot;) + ylab(&quot;Total number of murders (logarithmic scale)&quot;) + ggtitle(&quot;Gun murders in the US in 2010&quot;) + scale_color_discrete(name = &quot;Regions&quot;) + geom_hline(yintercept = avg_total) Or we could calculate the murder rate per million inhabitants throughout the US and draw a reference diagonal. In the case of the diagonal we have to express it in the same scale of the axis, therefore we have to convert it to log10. # Calculate the average rate ratio &lt;- sum(murders$total)/sum(murders$population) * 10^6 # Calculate base 10 logarithm to obtain the intercept on the &quot;y-axis&quot; ratio_log10 &lt;- log10(ratio) #And add the diagonal reference line murders |&gt; ggplot() + aes(x = population/10^6, y = total, label=abb, color=region) + geom_point(size=3) + geom_text(nudge_x = 0.075) + scale_x_log10() + scale_y_log10() + xlab(&quot;Population expressed in millions (logarithmic scale)&quot;) + ylab(&quot;Total number of murders (logarithmic scale)&quot;) + ggtitle(&quot;Gun murders in the US in 2010&quot;) + scale_color_discrete(name = &quot;Regions&quot;) + geom_abline(intercept = ratio_log10) We can improve this reference line by making it dashed and gray. To do this, simply edit the geom_abline() function as follows: murders |&gt; ggplot() + aes(x = population/10^6, y = total, label=abb, color=region) + geom_point(size=3) + geom_text(nudge_x = 0.075) + scale_x_log10() + scale_y_log10() + xlab(&quot;Population expressed in millions (logarithmic scale)&quot;) + ylab(&quot;Total number of murders (logarithmic scale)&quot;) + ggtitle(&quot;Gun murders in the US in 2010&quot;) + scale_color_discrete(name = &quot;Regions&quot;) + geom_abline(intercept = ratio_log10, lty = 2, color = &quot;darkgrey&quot;) 5.7 Changing the plot style The plot style using ggplot() can be easily changed. There are multiple themes we can use by loading the ggthemes library. We can, for example, use a widely used theme: the economist theme by adding the theme_economist() layer. library(ggthemes) murders |&gt; ggplot() + aes(x = population/10^6, y = total, label=abb, color=region) + geom_point(size=3) + geom_text(nudge_x = 0.075) + scale_x_log10() + scale_y_log10() + xlab(&quot;Population expressed in millions (logarithmic scale)&quot;) + ylab(&quot;Total number of murders (logarithmic scale)&quot;) + ggtitle(&quot;Gun murders in the US in 2010&quot;) + scale_color_discrete(name = &quot;Regions&quot;) + geom_abline(intercept = ratio_log10, lty = 2, color = &quot;darkgrey&quot;) + theme_economist() We still see overlapping abbreviations. We can make the names repel each other using the geom_text_repel() function instead of geom_text() that we are currently using. To use this function we need to call the ggrepel library. library(ggthemes) library(ggrepel) murders |&gt; ggplot() + aes(x = population/10^6, y = total, label=abb, color=region) + geom_point(size=3) + geom_text_repel() + scale_x_log10() + scale_y_log10() + xlab(&quot;Population expressed in millions (logarithmic scale)&quot;) + ylab(&quot;Total number of murders (logarithmic scale)&quot;) + ggtitle(&quot;Gun murders in the US in 2010&quot;) + scale_color_discrete(name = &quot;Regions&quot;) + geom_abline(intercept = ratio_log10, lty = 2, color = &quot;darkgrey&quot;) + theme_economist() #&gt; Warning: ggrepel: 1 unlabeled data points (too many overlaps). Consider #&gt; increasing max.overlaps This plot is visually much easier to understand and aesthetically much better than the default plot we created in previous chapters. We can explore more examples at this link. 5.8 Summarizing data with dplyr An important part of exploratory analysis is knowing how to summarize a variable into data that is easy to understand. The tidyverse package includes several packages, among them dplyr which makes it easier for us to summarize data from a variable. When we call the tidyvere library we are also calling all the functions of dplyr. To start using the functions of dplyr we are going to load the heights data frame which is in the dslabs library. library(tidyverse) library(dslabs) First let’s understand the heights data frame, we can apply pipeline and then use the head() function: heights |&gt; head() #&gt; sex height #&gt; 1 Male 75 #&gt; 2 Male 70 #&gt; 3 Male 68 #&gt; 4 Male 74 #&gt; 5 Male 61 #&gt; 6 Female 65 This data frame corresponds to the list of attributes of students in a university, where the height column indicates the height of each one in inches. Finally, let’s add the column height_m where we will transform the height to meters. Remember that a meter has 39.37 inches. Let’s store the result in the variable heights_m. heights_m &lt;- heights |&gt; mutate(height_m = height/39.37) heights_m |&gt; head() #&gt; sex height height_m #&gt; 1 Male 75 1.905004 #&gt; 2 Male 70 1.778004 #&gt; 3 Male 68 1.727203 #&gt; 4 Male 74 1.879604 #&gt; 5 Male 61 1.549403 #&gt; 6 Female 65 1.651003 The fastest way to summarize a list of data is indicating what the average is and how much its standard deviation6 is. If we wanted to obtain the average we would use the mean() function and sd() to obtain the standard deviation. For example: avg &lt;- mean(heights_m$height_m) std_dev &lt;- sd(heights_m$height_m) However, this summarizes all students for us without considering whether men could be on average taller than women. If we wanted to calculate the average and std. dev. we would have to filter first, then store in a variable and finally calculate the average and standard deviation. This is impractical and for that dplyr grants us the summarize() function. 5.8.1 Summarize function We can use the summarize function using the pipeline operator. Thus, we could calculate the average and std. dev. in this way: heights_m |&gt; filter(sex == &quot;Male&quot;) |&gt; summarize(avg = mean(height_m), std_dev = sd(height_m)) #&gt; avg std_dev #&gt; 1 1.760598 0.09172018 This function also generates a data frame for us. We can validate it if we store the result in the variable heights_m_male and then report the class of the variable: heights_m_male &lt;- heights_m |&gt; filter(sex == &quot;Male&quot;) |&gt; summarize(avg = mean(height_m), std_dev = sd(height_m)) class(heights_m_male) #&gt; [1] &quot;data.frame&quot; We can report the data frame heights_m_male and use it for future analyzes accessing with the accessor $. heights_m_male #&gt; avg std_dev #&gt; 1 1.760598 0.09172018 We see that the average height of men is 1.76 meters with a standard deviation of 0.09 meters. In the same way, we can use summarize to calculate other functions such as: heights_m |&gt; filter(sex == &quot;Male&quot;) |&gt; summarize(min_val = min(height_m), max_val = max(height_m), median_val = median(height_m)) #&gt; min_val max_val median_val #&gt; 1 1.270003 2.100004 1.752604 The tallest student measures more than 2.1 meters. Half of the male students measure more than 1.75 meters. However, we would now have to change to “Female” to calculate the data for women. We need to group and then summarize the data taking into account the grouping. For this there is the function group_by() 5.8.2 Group By Function This function allows us to create grouped data frames which makes it easier for us to summarize the data. We would only have to select based on what we want to group and no longer filter by sex. In this case the grouping would be based on the sex column: heights_m |&gt; group_by(sex) |&gt; summarize(avg = mean(height_m), std_dev = sd(height_m)) #&gt; # A tibble: 2 × 3 #&gt; sex avg std_dev #&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Female 1.65 0.0955 #&gt; 2 Male 1.76 0.0917 On average, women are shorter than men. If we now remember our danger case in the US. We can calculate the ratio of total crimes regarding the population and then compare it by region in this way: murders |&gt; mutate(ratio = total / population * 100000) |&gt; group_by(region) |&gt; summarize(avg_ratio = mean(ratio)) #&gt; # A tibble: 4 × 2 #&gt; region avg_ratio #&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 Northeast 1.85 #&gt; 2 South 4.42 #&gt; 3 North Central 2.18 #&gt; 4 West 1.83 On average, the South region is more dangerous. 5.9 Exercises This time we are going to perform exercises within the field of biology and for this we must remember the parts of a flower. This way we will give more sense to the problem: Load the iris data frame to your variable environment (data(iris)). In this data frame we find a list of characteristics of 150 flowers of 3 different types of species. Report a scatter plot of the sepal and petal length of the iris data frame. Solution iris |&gt; ggplot() + aes(x = Sepal.Length, y = Petal.Length) + geom_point() From the visualization created in the previous question, color the points according to the species to which it belongs. Solution iris |&gt; ggplot() + aes(x = Sepal.Length, y = Petal.Length, color = Species) + geom_point() Modify your previous plot so that the following elements appear: Chart title: Relationship between sepal and petal size of different flowers X-axis label: Sepal length (in cm) Y-axis label: Petal length (in cm) Legend title: Species Solution iris |&gt; ggplot() + aes(x = Sepal.Length, y = Petal.Length, color = Species) + geom_point() + xlab(&quot;Sepal length (in cm)&quot;) + ylab(&quot;Petal length (in cm)&quot;) + ggtitle(&quot;Relationship between sepal and petal size of different flowers&quot;) + scale_color_discrete(name = &quot;Species&quot;) Calculate a summary of the ratio between sepal length and petal length. This summary should show a data frame with the average, standard deviation and median of the ratio of these two dimensions mentioned. Solution iris |&gt; mutate(ratio = Sepal.Length / Petal.Length) |&gt; summarize(promedio = mean(ratio), std_dev = sd(ratio), median_val = median(ratio)) Make a summary of the ratio between sepal and petal length showing one row per species. Solution iris |&gt; mutate(ratio = Sepal.Length / Petal.Length) |&gt; group_by(Species) |&gt; summarize(promedio = mean(ratio), std_dev = sd(ratio), median_val = median(ratio)) What is the standard deviation?↩︎ "],["gapminder.html", "Chapter 6 Gapminder 6.1 Initial gapminder plots 6.2 Facets 6.3 Time series 6.4 Exercises 6.5 Histograms with ggplot 6.6 Box plots with ggplot 6.7 Comparison of distributions 6.8 Exercises", " Chapter 6 Gapminder The Gapminder Foundation7 is a Swedish non-profit organization that promotes global development through the use of statistics that can help reduce common myths and sensationalist stories about global health and economics. An important selection of data is already loaded in the dslabs library in the gapminder data frame. Our case/problem now will be to answer these two questions: Is it still reasonable to divide the world between Western countries* and developing countries? Is it true that every day we are worse off and rich countries get richer while poor countries get poorer? (*): Samuel Huntington in 1993 published an article called Clash of Civilizations8 where he defined Western countries as those located in the regions of North America, Northern/Southern/Western Europe and Australia and New Zealand. The process we are going to follow to solve the mentioned questions will be: Data Exploration Data Understanding Data analysis until finding the relevant ones to solve questions Visualization and summary First let’s explore the structure of the data frame with str(): gapminder |&gt; str() #&gt; &#39;data.frame&#39;: 10545 obs. of 9 variables: #&gt; $ country : Factor w/ 185 levels &quot;Albania&quot;,&quot;Algeria&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... #&gt; $ year : int 1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ... #&gt; $ infant_mortality: num 115.4 148.2 208 NA 59.9 ... #&gt; $ life_expectancy : num 62.9 47.5 36 63 65.4 ... #&gt; $ fertility : num 6.19 7.65 7.32 4.43 3.11 4.55 4.82 3.45 2.7 5.57 ... #&gt; $ population : num 1636054 11124892 5270844 54681 20619075 ... #&gt; $ gdp : num NA 1.38e+10 NA NA 1.08e+11 ... #&gt; $ continent : Factor w/ 5 levels &quot;Africa&quot;,&quot;Americas&quot;,..: 4 1 1 2 2 3 2 5 4 3 ... #&gt; $ region : Factor w/ 22 levels &quot;Australia and New Zealand&quot;,..: 19 11 10 2 15 21 2 1 22 21 ... We have a data frame with more than 10 thousand data points and 9 variables. Now let’s take a look at the data with head(): gapminder |&gt; head() #&gt; country year infant_mortality life_expectancy fertility #&gt; 1 Albania 1960 115.40 62.87 6.19 #&gt; 2 Algeria 1960 148.20 47.50 7.65 #&gt; 3 Angola 1960 208.00 35.98 7.32 #&gt; 4 Antigua and Barbuda 1960 NA 62.97 4.43 #&gt; 5 Argentina 1960 59.87 65.39 3.11 #&gt; 6 Armenia 1960 NA 66.86 4.55 #&gt; population gdp continent region #&gt; 1 1636054 NA Europe Southern Europe #&gt; 2 11124892 13828152297 Africa Northern Africa #&gt; 3 5270844 NA Africa Middle Africa #&gt; 4 54681 NA Americas Caribbean #&gt; 5 20619075 108322326649 Americas South America #&gt; 6 1867396 NA Asia Western Asia Remember that for library data frames we can usually find the documentation and understand each attribute faster: ?gapminder Going directly to the questions would be not leaving that curiosity free to see what else is in the data. Thus, we are going to start with other variables such as infant mortality, fertility or population. We can filter all the data that are from Peru and select the column country, year, infant mortality and population: gapminder |&gt; filter(country == &quot;Peru&quot;) |&gt; select(country, year, infant_mortality, population) #&gt; country year infant_mortality population #&gt; 1 Peru 1960 135.9 10061519 #&gt; 2 Peru 1961 132.6 10350239 #&gt; 3 Peru 1962 129.1 10650672 #&gt; 4 Peru 1963 125.4 10961539 #&gt; 5 Peru 1964 121.8 11281015 #&gt; 6 Peru 1965 118.2 11607684 #&gt; 7 Peru 1966 114.8 11941327 #&gt; 8 Peru 1967 111.6 12282081 #&gt; 9 Peru 1968 108.7 12629333 #&gt; 10 Peru 1969 106.0 12982444 #&gt; 11 Peru 1970 103.4 13341071 #&gt; 12 Peru 1971 100.9 13704333 #&gt; 13 Peru 1972 98.3 14072476 #&gt; 14 Peru 1973 95.8 14447649 #&gt; 15 Peru 1974 93.3 14832839 #&gt; 16 Peru 1975 91.0 15229951 #&gt; 17 Peru 1976 88.9 15639898 #&gt; 18 Peru 1977 87.0 16061327 #&gt; 19 Peru 1978 85.5 16491087 #&gt; 20 Peru 1979 83.9 16924758 #&gt; 21 Peru 1980 82.4 17359118 #&gt; 22 Peru 1981 80.7 17792551 #&gt; 23 Peru 1982 78.7 18225727 #&gt; 24 Peru 1983 76.3 18660443 #&gt; 25 Peru 1984 73.7 19099575 #&gt; 26 Peru 1985 70.7 19544950 #&gt; 27 Peru 1986 67.6 19996250 #&gt; 28 Peru 1987 64.6 20451712 #&gt; 29 Peru 1988 61.7 20909897 #&gt; 30 Peru 1989 58.9 21368856 #&gt; 31 Peru 1990 56.3 21826658 #&gt; 32 Peru 1991 53.7 22283130 #&gt; 33 Peru 1992 51.0 22737056 #&gt; 34 Peru 1993 48.2 23184222 #&gt; 35 Peru 1994 45.4 23619358 #&gt; 36 Peru 1995 42.5 24038761 #&gt; 37 Peru 1996 39.7 24441076 #&gt; 38 Peru 1997 36.9 24827409 #&gt; 39 Peru 1998 34.3 25199744 #&gt; 40 Peru 1999 31.8 25561297 #&gt; 41 Peru 2000 29.6 25914875 #&gt; 42 Peru 2001 27.6 26261363 #&gt; 43 Peru 2002 25.7 26601463 #&gt; 44 Peru 2003 24.1 26937737 #&gt; 45 Peru 2004 22.6 27273188 #&gt; 46 Peru 2005 21.3 27610406 #&gt; 47 Peru 2006 20.1 27949958 #&gt; 48 Peru 2007 19.0 28292768 #&gt; 49 Peru 2008 18.0 28642048 #&gt; 50 Peru 2009 17.1 29001563 #&gt; 51 Peru 2010 16.3 29373644 #&gt; 52 Peru 2011 15.6 29759891 #&gt; 53 Peru 2012 14.9 30158768 #&gt; 54 Peru 2013 14.2 30565461 #&gt; 55 Peru 2014 13.6 30973148 #&gt; 56 Peru 2015 13.1 31376670 #&gt; 57 Peru 2016 NA NA Let’s add a filter to obtain only the data from 2015: gapminder |&gt; filter(country == &quot;Peru&quot; &amp; year == 2015) |&gt; select(country, year, infant_mortality, population) #&gt; country year infant_mortality population #&gt; 1 Peru 2015 13.1 31376670 We can make a comparison between Peru and Chile if we create a vector and instead of the == operator we use the %in% operator that allows evaluating that our data are in that vector. vector_countries = c(&quot;Peru&quot;, &quot;Chile&quot;) gapminder |&gt; filter(country %in% vector_countries &amp; year == 2015) |&gt; select(country, year, infant_mortality, population) #&gt; country year infant_mortality population #&gt; 1 Chile 2015 7.0 17948141 #&gt; 2 Peru 2015 13.1 31376670 Infant mortality is measured in number of children who die per 1,000 infants. This means that it already takes into account the population. In 2015 Peru had a higher infant mortality rate than Chile. 6.1 Initial gapminder plots However, if we want to analyze global data, comparing country by country would take us much more time. Let’s use ggplot to see if there is a relationship in our data. Let’s create a scatter plot with data from the year 1990 of the fertility variable (fertility), which is the average number of children per woman, and the life expectancy variable (life_expectancy). gapminder |&gt; filter(year == 1990) |&gt; ggplot() + aes(x = fertility, y = life_expectancy) + geom_point() From this graph we can see that countries where families have 7.5 children have a lower life expectancy. On the other hand, in countries with high life expectancy the average number of children is less than 2 children per family. As we have done previously, we can color the points according to some other variable. In this case, knowing which continent they belong to could give us a better idea of the data. gapminder |&gt; filter(year == 1990) |&gt; ggplot() + aes(x = fertility, y = life_expectancy, color = continent) + geom_point() In this graph, groups begin to be seen. Several European countries are in the upper left quadrant, while several African countries are in the lower right quadrant. 6.2 Facets Although the previous graph already shows us a correlation of variables, we cannot see how it has changed from one year to another. For this we will use the facet layer (facet_). In the layer facet_grid(row_variable ~ column_variable) we replace “row_variable” with the name of our variable or replace it with a . if we don’t want any of them. For example, from the previous example let’s compare how the distribution changed by comparing the year 1960 with the year 2013. vector_years &lt;- c(1960, 2013) gapminder |&gt; filter(year %in% vector_years) |&gt; ggplot() + aes(x = fertility, y = life_expectancy, color = continent) + geom_point() + facet_grid(year ~ .) We can make it even clearer which continent changed the most if we add the continent variable as a column. vector_years &lt;- c(1960, 2013) gapminder |&gt; filter(year %in% vector_years) |&gt; ggplot() + aes(x = fertility, y = life_expectancy, color = continent) + geom_point() + facet_grid(year ~ continent) Having several columns for each continent makes it harder to understand because the columns become smaller. It is recommended to have few columns. So we invert the order between year and continent. vector_years &lt;- c(1960, 2013) gapminder |&gt; filter(year %in% vector_years) |&gt; ggplot() + aes(x = fertility, y = life_expectancy, color = continent) + geom_point() + facet_grid(continent ~ year) Here the change by regions is much more evident and how the majority of countries have reduced fertility per family while increasing life expectancy. We are living longer than in the 60s and in turn having fewer children per family. These phenomena have occurred globally. We don’t always have to show all variables, in this case continents. We can continue applying filters so that it shows us a subset of continents that we want to compare. For example: vector_years &lt;- c(1960, 2013) vector_continents &lt;- c(&quot;Europe&quot;, &quot;Asia&quot;) gapminder |&gt; filter(year %in% vector_years &amp; continent %in% vector_continents) |&gt; ggplot() + aes(x = fertility, y = life_expectancy, color = continent) + geom_point() + facet_grid(continent ~ year) In this case it would be visually better if the continents were not in separate rows, but could still be appreciated in the graph. To do this, we will use the wrap facet (facet_wrap(~ x)), where x is the variable we want to wrap. In our case it would be the year, instead of appearing in separate rows we can join and transpose them. vector_años &lt;- c(1960, 2013) vector_continents &lt;- c(&quot;Europe&quot;, &quot;Asia&quot;) gapminder |&gt; filter(year %in% vector_years &amp; continent %in% vector_continents) |&gt; ggplot() + aes(x = fertility, y = life_expectancy, color = continent) + geom_point() + facet_wrap( ~ year) We can add more data by adding more data to the vectors. For example, let’s add a cut in the middle between 1960 and 2013. vector_years &lt;- c(1960, 1985, 2013) vector_continents &lt;- c(&quot;Europe&quot;, &quot;Asia&quot;) gapminder |&gt; filter(year %in% vector_years &amp; continent %in% vector_continents) |&gt; ggplot() + aes(x = fertility, y = life_expectancy, color = continent) + geom_point() + facet_wrap( ~ year) 6.3 Time series Time series are sequences of data measured at determined moments and ordered chronologically. R allows us to easily plot time series, we only need our data frames to include some time variable. 6.3.1 Individual time series In an individual time series we only analyze how a single variable has evolved, for example the evolution of the fertility rate in Peru. For this we can use a scatter plot with points or with lines. As we will remember, we use geom_point() for points: gapminder |&gt; filter(country == &quot;Peru&quot;) |&gt; ggplot() + aes(x = year, y = fertility) + geom_point() #&gt; Warning: Removed 1 row containing missing values or values #&gt; outside the scale range (`geom_point()`). We get a “warning” indicating that there are values that cannot be drawn because they are NA and are not available. This does not prevent showing the graph. If we want a line graph, which is the most used in time series, we use geom_line(): gapminder |&gt; filter(country == &quot;Peru&quot;) |&gt; ggplot() + aes(x = year, y = fertility) + geom_line() #&gt; Warning: Removed 1 row containing missing values or values #&gt; outside the scale range (`geom_line()`). 6.3.2 Multiple time series In multiple time series we seek comparison to analyze in a time series how the data evolved. For example, this would be the time series if we compare Peru, Bolivia and Chile: countries &lt;- c(&quot;Peru&quot;, &quot;Bolivia&quot;, &quot;Chile&quot;) gapminder |&gt; filter(country %in% countries) |&gt; ggplot() + aes(x = year, y = fertility, color = country) + geom_line() #&gt; Warning: Removed 3 rows containing missing values or values #&gt; outside the scale range (`geom_line()`). We can also remove the legend and show the name of the countries as labels on the same graph. To do this we will first have to create a data frame using the function data.frame() that indicates the coordinates where we want each label to appear: countries &lt;- c(&quot;Peru&quot;, &quot;Bolivia&quot;, &quot;Chile&quot;) labels &lt;- data.frame(country_names = countries, x = c(1975, 1965, 1962), y = c(6, 7, 4)) labels #&gt; country_names x y #&gt; 1 Peru 1975 6 #&gt; 2 Bolivia 1965 7 #&gt; 3 Chile 1962 4 We will use this to indicate that we want, for example, Bolivia to be written at the intersection of the year 1972 and with a fertility rate of 6.8. To use these labels in ggplot we will edit the arguments in the geom_text layer. We will use the data attributes to indicate that we want to obtain the data from an external source, and we will include the aes layer inside geom_text to correlate the data frame we have created with the graph. We must keep in mind that the column name in both data frames must be the same, in this case country: countries &lt;- c(&quot;Peru&quot;, &quot;Bolivia&quot;, &quot;Chile&quot;) labels &lt;- data.frame(country = countries, x = c(1976, 1972, 1965), y = c(5.2, 6.8, 5.5)) gapminder |&gt; filter(country %in% countries) |&gt; ggplot() + aes(year, fertility, col = country) + geom_line() + geom_text(data = labels, aes(x, y, label = country)) + theme(legend.position = &quot;none&quot;) #&gt; Warning: Removed 3 rows containing missing values or values #&gt; outside the scale range (`geom_line()`). 6.4 Exercises For these exercises we will continue using the gapminder data frame. Create a scatter plot of fertility versus life expectancy for the American continent in 2000. Color the points according to the region they belong to. Solution gapminder |&gt; filter( continent == &quot;Americas&quot; &amp; year == 2000) |&gt; ggplot() + aes(fertility, life_expectancy, color = region) + geom_point() To create a vector of sequences we can use X:Y. This creates a vector that goes from number X to number Y The Vietnam War caused deaths in both the US (United States) and Vietnam. Create a chart that allows visualizing how life expectancy changed in each country. Hint: Filter these two countries, as well as the population from year 1955 to 1990 and this time series in a line chart. Solution countries &lt;- c(&quot;Vietnam&quot;, &quot;United States&quot;) year_sequence &lt;- 1955:1990 gapminder |&gt; filter(country %in% countries &amp; year %in% year_sequence) |&gt; ggplot() + aes(year, life_expectancy, color = country) + geom_line() After the Vietnam War, Pol Pot and Khmer Rouge took control of Cambodia from 1975 to 1979 in one of the bloodiest dictatorships in history. Add Cambodia to the previous chart to visualize the impact of this government on life expectancy in that country. Solution countries &lt;- c(&quot;Vietnam&quot;, &quot;United States&quot;, &quot;Cambodia&quot;) year_sequence &lt;- 1955:1990 gapminder |&gt; filter(country %in% countries &amp; year %in% year_sequence) |&gt; ggplot() + aes(year, life_expectancy, color = country) + geom_line() 6.5 Histograms with ggplot We could continue exploring the data until we understand it much better. Eventually we would get to the GDP (gdp) data and in turn we would understand that comparing only GDP alone makes no sense since there are countries with much more population than others. Data transformation is not something new, but we will see that it is something recurrent in our analyzes. We are going to use a transformation that allows us to obtain how much is the GDP per capita per day in each country in each year gapminder &lt;- gapminder |&gt; mutate(gdp_per_capita_per_day &lt;- gdp/population/365) We could visualize this variable first by creating a histogram of it. A histogram in ggplot is nothing more than one of the geoms we have available, in this case it would be geom_histogram(binwidth = x), where x is the width of the bar. For example, let’s calculate the distribution of our created variable in the year 2010: gapminder |&gt; mutate(gdp_per_capita_per_day = gdp/population/365) |&gt; filter(year == 2010) |&gt; ggplot() + aes(gdp_per_capita_per_day) + geom_histogram(binwidth = 5) #&gt; Warning: Removed 9 rows containing non-finite outside the #&gt; scale range (`stat_bin()`). We can filter out the NA so that we no longer get the low “warnings” with the function we saw previously is.na(). In this case since we don’t want the NA we will negate the function by placing the symbol ! at the beginning. gapminder |&gt; mutate(gdp_per_capita_per_day = gdp/population/365) |&gt; filter(year == 2010 &amp; !is.na(gdp_per_capita_per_day)) |&gt; ggplot() + aes(gdp_per_capita_per_day) + geom_histogram(binwidth = 5) At this point it should be quick to detect that there is a concentration of data from countries with low GDP per capita and we could be tempted to apply a scale transformation on the x-axis. Let’s try with logarithm in base 2: gapminder |&gt; mutate(gdp_per_capita_per_day = gdp/population/365) |&gt; filter(year == 2010 &amp; !is.na(gdp_per_capita_per_day)) |&gt; ggplot() + aes(gdp_per_capita_per_day) + geom_histogram(binwidth = 0.5) + #Change the width to 0.5 due to logarithmic scale scale_x_continuous(trans = &quot;log2&quot;) Let’s be careful interpreting this data. We cannot say that it is a symmetric distribution, even when with this scale we are tempted to do so. Remember the scale and use it appropriately. 6.6 Box plots with ggplot In the same way, box plots are one more geom within the available ones, for this we will use the geom_boxplot() layer. For example, let’s create a box plot to analyze GDP per capita per day by continent: gapminder |&gt; mutate(gdp_per_capita_per_day = gdp/population/365) |&gt; filter(year == 2010 &amp; !is.na(gdp_per_capita_per_day)) |&gt; ggplot() + aes(continent, gdp_per_capita_per_day) + geom_boxplot() Now let’s zoom in. Within each continent we have regions, for example in the Americas we have South America, Central America, North America, and so on with each continent. Let’s change the continent variable to region. gapminder |&gt; mutate(gdp_per_capita_per_day = gdp/population/365) |&gt; filter(year == 2010 &amp; !is.na(gdp_per_capita_per_day)) |&gt; ggplot() + aes(region, gdp_per_capita_per_day) + geom_boxplot() As we can verify: this visualization allows us to infer very little. Before discarding a graph let’s think if we can change the configuration to improve the visualization. The first thing we can improve is the names of the regions. They are in horizontal form, but we could rotate it 45 degrees using the theme() layer. gapminder |&gt; mutate(gdp_per_capita_per_day = gdp/population/365) |&gt; filter(year == 2010 &amp; !is.na(gdp_per_capita_per_day)) |&gt; ggplot() + aes(region, gdp_per_capita_per_day) + geom_boxplot() + theme(axis.text.x = element_text(angle = 45, hjust = 1) ) The names are understood, but if we want to find the top 3 (either by median or average) we would have to look for them one by one. Let’s reorder it, but first let’s be aware of some previous considerations: The region column is a Factor type variable, it is not a character string. Even when visually we did not find a difference, factors are used to categorize data. For example, bronze, silver, platinum customers, etc. Factors are useful because internally they are replaced by numbers and numbers, at a computational level, are faster to sort. The default sorting is alphabetical, as we can appreciate if we use the levels function. levels(gapminder$region) #&gt; [1] &quot;Australia and New Zealand&quot; &quot;Caribbean&quot; #&gt; [3] &quot;Central America&quot; &quot;Central Asia&quot; #&gt; [5] &quot;Eastern Africa&quot; &quot;Eastern Asia&quot; #&gt; [7] &quot;Eastern Europe&quot; &quot;Melanesia&quot; #&gt; [9] &quot;Micronesia&quot; &quot;Middle Africa&quot; #&gt; [11] &quot;Northern Africa&quot; &quot;Northern America&quot; #&gt; [13] &quot;Northern Europe&quot; &quot;Polynesia&quot; #&gt; [15] &quot;South America&quot; &quot;South-Eastern Asia&quot; #&gt; [17] &quot;Southern Africa&quot; &quot;Southern Asia&quot; #&gt; [19] &quot;Southern Europe&quot; &quot;Western Africa&quot; #&gt; [21] &quot;Western Asia&quot; &quot;Western Europe&quot; We will use the reorder() function to change the order of the factors and since we are altering the dataframe we would have to use it inside the mutate() function. The reorder() function asks us as the first attribute the factor to reorder, then the vector that we will take into account and finally a grouping function. For example, order based on the median of each region (visually remember that it is the thick line inside each box): gapminder |&gt; mutate(gdp_per_capita_per_day = gdp/population/365) |&gt; filter(year == 2010 &amp; !is.na(gdp_per_capita_per_day)) |&gt; mutate(region = reorder(region, gdp_per_capita_per_day, FUN = median)) |&gt; ggplot() + aes(region, gdp_per_capita_per_day) + geom_boxplot() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) Note that a mutate has been placed after filtering the data. This is to guarantee that we are removing the NA. Otherwise, we risk that all values are NA and the reordering is not performed and remains default. We see at the far left some regions in Africa, and at the far right Europe and USA. Remember that we can add color according to some variable. In this case let’s add color based on the continent: gapminder |&gt; mutate(gdp_per_capita_per_day = gdp/population/365) |&gt; filter(year == 2010 &amp; !is.na(gdp_per_capita_per_day)) |&gt; mutate(region = reorder(region, gdp_per_capita_per_day, FUN = median)) |&gt; ggplot() + aes(region, gdp_per_capita_per_day, color = continent) + geom_boxplot() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) Although we can already differentiate it, in a box plot it is usually the fill (fill in English) of the box that is painted. So, let’s change the color attribute to the fill attribute. And let’s remove the legend on the x-axis. It is not necessary in this case where the regions are self-explanatory. gapminder |&gt; mutate(gdp_per_capita_per_day = gdp/population/365) |&gt; filter(year == 2010 &amp; !is.na(gdp_per_capita_per_day)) |&gt; mutate(region = reorder(region, gdp_per_capita_per_day, FUN = median)) |&gt; ggplot() + aes(region, gdp_per_capita_per_day, fill = continent) + geom_boxplot() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + xlab(&quot;&quot;) This graph helps us see the top 5, but since there are several regions concentrated in small values of GDP per capita we visually lose those regions. We need a scale transformation. If you are thinking of adding a logarithmic scale layer for the y-axis you are on the right track. Let’s try: gapminder |&gt; mutate(gdp_per_capita_per_day = gdp/population/365) |&gt; filter(year == 2010 &amp; !is.na(gdp_per_capita_per_day)) |&gt; mutate(region = reorder(region, gdp_per_capita_per_day, FUN = median)) |&gt; ggplot() + aes(region, gdp_per_capita_per_day, fill = continent) + geom_boxplot() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + xlab(&quot;&quot;) + scale_y_continuous(trans = &quot;log2&quot;) Sometimes it is necessary not only to show the boxes, but also where each of the data points is located. For this we can add the geom_point() layer that we had previously used to show the points of each data. gapminder |&gt; mutate(gdp_per_capita_per_day = gdp/population/365) |&gt; filter(year == 2010 &amp; !is.na(gdp_per_capita_per_day)) |&gt; mutate(region = reorder(region, gdp_per_capita_per_day, FUN = median)) |&gt; ggplot() + aes(region, gdp_per_capita_per_day, fill = continent) + geom_boxplot() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + xlab(&quot;&quot;) + scale_y_continuous(trans = &quot;log2&quot;) + geom_point(size = 0.5) 6.7 Comparison of distributions To be able to solve the first question of the case we would have to compare the distributions of the “Western” countries versus the developing countries. For this, since we do not have a column that indicates which are from the West, we are going to create a western_countries with the list of regions that fall into this category: western_countries &lt;- c(&quot;Western Europe&quot;, &quot;Northern Europe&quot;, &quot;Southern Europe&quot;, &quot;Northern America&quot;, &quot;Australia and New Zealand&quot;) We will also use the ifelse(test, yes, no) function to create a new column such that if the region is in the West it stores a value, and if it is not in the West it stores another value. It is recommended to read the documentation in ?ifelse. Let’s add the column for the group each country belongs to: western_countries &lt;- c(&quot;Western Europe&quot;, &quot;Northern Europe&quot;, &quot;Southern Europe&quot;, &quot;Northern America&quot;, &quot;Australia and New Zealand&quot;) gapminder |&gt; mutate(gdp_per_capita_per_day = gdp/population/365) |&gt; filter(year == 2010 &amp; !is.na(gdp_per_capita_per_day)) |&gt; mutate(group = ifelse(region %in% western_countries, &quot;Western&quot;, &quot;Developing&quot;)) |&gt; head() #&gt; country year infant_mortality life_expectancy fertility #&gt; 1 Albania 2010 14.8 77.2 1.74 #&gt; 2 Algeria 2010 23.5 76.0 2.82 #&gt; 3 Angola 2010 109.6 57.6 6.22 #&gt; 4 Antigua and Barbuda 2010 7.7 75.8 2.13 #&gt; 5 Argentina 2010 13.0 75.8 2.22 #&gt; 6 Armenia 2010 16.1 73.0 1.55 #&gt; population gdp continent region gdp_per_capita_per_day #&gt; 1 2901883 6137563946 Europe Southern Europe 5.794597 #&gt; 2 36036159 79164339611 Africa Northern Africa 6.018638 #&gt; 3 21219954 26125663270 Africa Middle Africa 3.373106 #&gt; 4 87233 836686777 Americas Caribbean 26.277814 #&gt; 5 41222875 434405530244 Americas South America 28.871158 #&gt; 6 2963496 4102285513 Asia Western Asia 3.792527 #&gt; group #&gt; 1 Western #&gt; 2 Developing #&gt; 3 Developing #&gt; 4 Developing #&gt; 5 Developing #&gt; 6 Developing Now that we have how to differentiate the countries we can see their distribution until we find how to answer our question. We start by creating a histogram with logarithmic scale in the x-axis and separate it using facet_grid based on the group it belongs to: western_countries &lt;- c(&quot;Western Europe&quot;, &quot;Northern Europe&quot;, &quot;Southern Europe&quot;, &quot;Northern America&quot;, &quot;Australia and New Zealand&quot;) gapminder |&gt; mutate(gdp_per_capita_per_day = gdp/population/365) |&gt; filter(year == 2010 &amp; !is.na(gdp_per_capita_per_day)) |&gt; mutate(group = ifelse(region %in% western_countries, &quot;Western&quot;, &quot;Developing&quot;)) |&gt; ggplot() + aes(gdp_per_capita_per_day) + geom_histogram(binwidth = 1) + scale_x_continuous(trans = &quot;log2&quot;) + facet_grid(. ~ group) We see that the daily GDP per capita has a distribution with higher values compared to developing countries. However, the picture in one year is not everything. We are ready to see if the separation was the same 40 years back from the date in the example (2010). We are also going to add the geom_histogram() layer the color attribute to see the border of the bars which by default are grey. western_countries &lt;- c(&quot;Western Europe&quot;, &quot;Northern Europe&quot;, &quot;Southern Europe&quot;, &quot;Northern America&quot;, &quot;Australia and New Zealand&quot;) gapminder |&gt; mutate(gdp_per_capita_per_day = gdp/population/365) |&gt; filter(year %in% c(1970, 2010) &amp; !is.na(gdp_per_capita_per_day)) |&gt; mutate(group = ifelse(region %in% western_countries, &quot;Western&quot;, &quot;Developing&quot;)) |&gt; ggplot() + aes(gdp_per_capita_per_day) + geom_histogram(binwidth = 1, color = &quot;black&quot;) + scale_x_continuous(trans = &quot;log2&quot;) + facet_grid(year ~ group) Both groups, both “Western” and “Developing” have improved in that 40-year span, but developing countries have advanced more than Western countries. So far we have assumed something: that all countries that reported in 2010 also reported data in 1970. To make the comparison finer we have to look for the distribution of countries that have data reported both in 1970 and in 2010. To do this, we are going to create a vector that lists the countries with data in 1970 and another of those that have data in 2010 and then look for the intersection. Remember that to extract a column we use the pull() function. western_countries &lt;- c(&quot;Western Europe&quot;, &quot;Northern Europe&quot;, &quot;Southern Europe&quot;, &quot;Northern America&quot;, &quot;Australia and New Zealand&quot;) list_1 &lt;- gapminder |&gt; mutate(gdp_per_capita_per_day = gdp/population/365) |&gt; filter(year %in% c(1970) &amp; !is.na(gdp_per_capita_per_day)) |&gt; pull(country) list_2 &lt;- gapminder |&gt; mutate(gdp_per_capita_per_day = gdp/population/365) |&gt; filter(year %in% c(2010) &amp; !is.na(gdp_per_capita_per_day)) |&gt; pull(country) To find the intersection of these two vectors we will use the function intersect(vector_1, vector_2), which will give us the vector we are looking for. intersection_vector &lt;- intersect(list_1, list_2) So, we recreate our histogram including only the countries on this list. gapminder |&gt; mutate(gdp_per_capita_per_day = gdp/population/365) |&gt; filter(year %in% c(1970, 2010) &amp; !is.na(gdp_per_capita_per_day)) |&gt; filter(country %in% intersection_vector) |&gt; mutate(group = ifelse(region %in% western_countries, &quot;Western&quot;, &quot;Developing&quot;)) |&gt; ggplot() + aes(gdp_per_capita_per_day) + geom_histogram(binwidth = 1, color = &quot;black&quot;) + scale_x_continuous(trans = &quot;log2&quot;) + facet_grid(year ~ group) We see now more clearly with comparable data how there are more countries within the developing region that increased per capita GDP, much more than developing countries. But this first inference is still visual, we need to compare how the median, range, etc. changed. For this we will use a box plot very similar to the previous one, but this time we will edit geom_boxplot() so that it shows us in a single graph how each region has changed from 1970 to 2010. gapminder |&gt; mutate(gdp_per_capita_per_day = gdp/population/365) |&gt; filter(year %in% c(1970, 2010) &amp; !is.na(gdp_per_capita_per_day)) |&gt; filter(country %in% intersection_vector) |&gt; mutate(region = reorder(region, gdp_per_capita_per_day, FUN = median)) |&gt; ggplot() + aes(region, gdp_per_capita_per_day) + geom_boxplot(aes(region, gdp_per_capita_per_day, fill=factor(year))) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + xlab(&quot;&quot;) + scale_y_continuous(trans = &quot;log2&quot;) We see how there are regions within Asia that have grown substantially. As we know from general culture, some countries in Asia are already powers, but today with these graphs we can understand well how much each region has changed until becoming a power. Therefore, we can now answer both questions of the case: It is not reasonable to continue using the categorization of “Western” and “developing” since there are more and more regions that are poorly represented by those categories, such as East Asia. It is not true that rich countries get richer while poor countries get poorer. We have seen that developing countries have even higher growth than the growth that Western countries have. 6.8 Exercises For this series of exercises we will use the stars data frame which tells us attributes of a series of stars such as their temperature, type and magnitude. The magnitude column is the absolute magnitude, where a more negative value of a star compared to another means it is more luminous. Make sure to load the data frame with data(stars) before starting. The temperature is in degrees Kelvin. Add the temp_celsius column using the following formula: C = K - 273.15. Report a scatter plot of temperature versus magnitude coloring according to star type. Also, change the scale of the x-axis to logarithmic base 10. Solution stars |&gt; mutate(temp_celsius = temp - 273.15) |&gt; ggplot() + aes(temp_celsius, magnitude, color = type) + scale_x_log10() + geom_point() Since positive values indicate less brightness, invert the values of the previous graph using the scale_y_reverse() layer. Solution stars |&gt; mutate(temp_celsius = temp - 273.15) |&gt; ggplot() + aes(temp_celsius, magnitude, color = type) + scale_x_log10() + geom_point() + scale_y_reverse() The Sun is classified as a G-type star. Are G-type stars the most luminous? Create a box plot to compare medians of magnitude and determine which type of stars are the most luminous. No, G-type stars are not the most luminous. For this we can elaborate this graph: Solution stars |&gt; ggplot() + aes(type, magnitude) + geom_boxplot() + scale_y_reverse() https://www.gapminder.org/↩︎ Full text of Huntington’s article (in English)↩︎ "],["introduction-to-probabilities.html", "Introduction to Probabilities", " Introduction to Probabilities Understanding probability theory is indispensable for every Data Scientist. In the following chapters we are going to learn how to apply probability theory in R. It is expected that the reader has basic knowledge in statistics given that we will not go into depth in each of the statistical concepts. For example, we are not going to explain formulas for calculating probabilities of independent or dependent events. Instead, we are going to apply directly probabilities of independent or dependent events in R. "],["discrete-probabilities.html", "Chapter 7 Discrete Probabilities 7.1 Calculation using the mathematical definition 7.2 Monte Carlo Simulation for Discrete Variables 7.3 Exercises 7.4 Combinations and Permutations 7.5 Sufficient Experiments with Monte Carlo Simulation 7.6 Case: Birthdays in Classrooms 7.7 Exercises 7.8 Integrative Exercise", " Chapter 7 Discrete Probabilities We will start with some basic principles of categorical data. Probabilities of this type are called discrete probabilities. Understanding the basic principles of discrete probabilities will help us understand continuous probabilities which are the most common in data science applications. Recall that a discrete variable is a variable that cannot take some values within a minimum countable set, that is, it does not accept any value, only those that belong to the set. For example, if we have 4 women and 6 men seated in a room and we were to raffle 1 prize, intuitively we would know that the probability that the winner is a man is 60%. 7.1 Calculation using the mathematical definition The probability we obtained by intuition in the previous example can be expressed as follows: \\(P(A) = probability\\ of\\ event\\ A = \\frac{Times\\ event\\ A\\ can\\ occur}{Total\\ possible\\ outcomes}\\) \\(P(Winner\\ is\\ man) = \\frac{6}{10} = 60\\%\\) 7.2 Monte Carlo Simulation for Discrete Variables Monte Carlo simulation or method is a statistical method used to solve complex mathematical problems through the generation of random variables. In this case the problem is not complex, but Monte Carlo can be used to familiarize ourselves with a method that we will use constantly. We will use Monte Carlo simulation to estimate the proportion we would obtain if we repeated this experiment randomly a determined number of times. That is, the probability of the event using this estimation would be the proportion of times that event occurred in our simulation. In R we can easily create random samples using the sample() function. For example, let’s create a vector of students and then use the sample() function to choose one at random. students &lt;- c(&quot;woman&quot;, &quot;woman&quot;, &quot;woman&quot;, &quot;woman&quot;, &quot;man&quot;, &quot;man&quot;, &quot;man&quot;, &quot;man&quot;, &quot;man&quot;, &quot;man&quot;) sample(students, 1) We could also use the rep() function to create the students vector faster. To do this we would enter as the first argument a vector and as the second another vector indicating how many times we want them to be created. Thus, we would create the students vector faster. students &lt;- rep(c(&quot;woman&quot;, &quot;man&quot;), times = c(4, 6)) students #&gt; [1] &quot;woman&quot; &quot;woman&quot; &quot;woman&quot; &quot;woman&quot; &quot;man&quot; &quot;man&quot; &quot;man&quot; &quot;man&quot; &quot;man&quot; #&gt; [10] &quot;man&quot; Now we have to simulate a determined number of times the experiment of picking a random element. For this we will use the replicate() function. Let’s replicate this experiment 100 times: students &lt;- rep(c(&quot;woman&quot;, &quot;man&quot;), times = c(4, 6)) n_times &lt;- 100 results &lt;- replicate(n_times, { sample(students, 1) }) results #&gt; [1] &quot;man&quot; &quot;man&quot; &quot;man&quot; &quot;man&quot; &quot;woman&quot; &quot;woman&quot; &quot;man&quot; &quot;woman&quot; &quot;woman&quot; #&gt; [10] &quot;man&quot; &quot;woman&quot; &quot;man&quot; &quot;man&quot; &quot;man&quot; &quot;man&quot; &quot;man&quot; &quot;man&quot; &quot;man&quot; #&gt; [19] &quot;woman&quot; &quot;man&quot; &quot;woman&quot; &quot;woman&quot; &quot;man&quot; &quot;woman&quot; &quot;man&quot; &quot;man&quot; &quot;man&quot; #&gt; [28] &quot;man&quot; &quot;woman&quot; &quot;woman&quot; &quot;woman&quot; &quot;man&quot; &quot;man&quot; &quot;man&quot; &quot;man&quot; &quot;man&quot; #&gt; [37] &quot;man&quot; &quot;man&quot; &quot;man&quot; &quot;man&quot; &quot;man&quot; &quot;woman&quot; &quot;man&quot; &quot;man&quot; &quot;man&quot; #&gt; [46] &quot;woman&quot; &quot;man&quot; &quot;man&quot; &quot;man&quot; &quot;man&quot; &quot;woman&quot; &quot;woman&quot; &quot;woman&quot; &quot;man&quot; #&gt; [55] &quot;man&quot; &quot;man&quot; &quot;man&quot; &quot;man&quot; &quot;woman&quot; &quot;man&quot; &quot;woman&quot; &quot;man&quot; &quot;man&quot; #&gt; [64] &quot;man&quot; &quot;woman&quot; &quot;man&quot; &quot;man&quot; &quot;man&quot; &quot;woman&quot; &quot;woman&quot; &quot;man&quot; &quot;man&quot; #&gt; [73] &quot;man&quot; &quot;man&quot; &quot;man&quot; &quot;man&quot; &quot;woman&quot; &quot;man&quot; &quot;woman&quot; &quot;man&quot; &quot;man&quot; #&gt; [82] &quot;woman&quot; &quot;woman&quot; &quot;woman&quot; &quot;man&quot; &quot;woman&quot; &quot;man&quot; &quot;woman&quot; &quot;woman&quot; &quot;man&quot; #&gt; [91] &quot;woman&quot; &quot;man&quot; &quot;woman&quot; &quot;man&quot; &quot;man&quot; &quot;man&quot; &quot;man&quot; &quot;man&quot; &quot;woman&quot; #&gt; [100] &quot;man&quot; We can see what the result was for each of the 100 draws we simulated. Now we will use the table() function to transform our results vector into a summary table that shows us how many times each value appeared. table(results) #&gt; results #&gt; man woman #&gt; 67 33 If we store this result in a vector tabla_results, we can then use the prop.table() function to know the proportion of each value: tabla_results &lt;- table(results) prop.table(tabla_results) #&gt; results #&gt; man woman #&gt; 0.67 0.33 We should not worry if the probability that it is a man has not come out exactly 60%. Recall that we are estimating the probability using a method that depends on the number of times we simulate the experiment. The more times we repeat the experiment the closer we will be to the value. For example, let’s replicate this experiment now 10,000 times. students &lt;- rep(c(&quot;woman&quot;, &quot;man&quot;), times = c(4, 6)) n_times &lt;- 10000 results &lt;- replicate(n_times, { sample(students, 1) }) tabla_results &lt;- table(results) prop.table(tabla_results) #&gt; results #&gt; man woman #&gt; 0.5954 0.4046 We see how the value converges to 60%. We should not worry if the value varies by a few digits from the one presented in this book given that we are simulating a random event. Finally, for this simple example we could also have used the mean() function. Although this calculates the average of a set of numbers, we could convert our students vector to numerical values, where each value is converted to 1 or 0 depending on some condition. To do this, R makes the conversion of vectors to 1 and 0 very simple using the comparator operator ==: students == &quot;man&quot; #&gt; [1] FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE When we apply the mean() function to this result, it coerces TRUE values to 1 and FALSE values to 0. Thus, if we apply the average of this list, we would have the percentage of men and with it the probability that when choosing a person it is a man: mean(students == &quot;man&quot;) #&gt; [1] 0.6 7.2.1 Other functions to create vectors We have already learned the rep() function to create vectors faster. Another function we find in R is the expand.grid(x, y) function which creates a data frame of all combinations between vectors x and y. greetings &lt;- c(&quot;Hola&quot;, &quot;Chau&quot;) names_list &lt;- c(&quot;Andrés&quot;, &quot;Josep&quot;, &quot;Alonso&quot;, &quot;Andrés&quot;, &quot;Cesar&quot;, &quot;Jeremy&quot;) result &lt;- expand.grid(saludo = greetings, nombre = names_list) result #&gt; saludo nombre #&gt; 1 Hola Andrés #&gt; 2 Chau Andrés #&gt; 3 Hola Josep #&gt; 4 Chau Josep #&gt; 5 Hola Alonso #&gt; 6 Chau Alonso #&gt; 7 Hola Andrés #&gt; 8 Chau Andrés #&gt; 9 Hola Cesar #&gt; 10 Chau Cesar #&gt; 11 Hola Jeremy #&gt; 12 Chau Jeremy Finally, we have the paste(x,y) function, which concatenates two strings or vectors of strings adding a space in the middle. paste(result$saludo, result$nombre) #&gt; [1] &quot;Hola Andrés&quot; &quot;Chau Andrés&quot; &quot;Hola Josep&quot; &quot;Chau Josep&quot; &quot;Hola Alonso&quot; #&gt; [6] &quot;Chau Alonso&quot; &quot;Hola Andrés&quot; &quot;Chau Andrés&quot; &quot;Hola Cesar&quot; &quot;Chau Cesar&quot; #&gt; [11] &quot;Hola Jeremy&quot; &quot;Chau Jeremy&quot; Thus, we can easily generate, for example, a deck of cards distributed in 4 suits: hearts, diamonds, spades, and clubs. The cards of each suit are numbered from 1 to 10, where 1 is the Ace, and followed by Jack, Queen, and King. To do this, we would have to create a vector of suits and a vector of numbers to then create the combinationl and have the complete deck. numbers &lt;- c(&quot;Ace&quot;, &quot;Two&quot;, &quot;Three&quot;, &quot;Four&quot;, &quot;Five&quot;, &quot;Six&quot;, &quot;Seven&quot;, &quot;Eight&quot;, &quot;Nine&quot;, &quot;Ten&quot;, &quot;Jack&quot;, &quot;Queen&quot;, &quot;King&quot;) suits &lt;- c(&quot;of Hearts&quot;, &quot;of Diamonds&quot;, &quot;of Spades&quot;, &quot;of Clubs&quot;) # Create card combinations combination &lt;- expand.grid(numero = numbers, palo = suits) # Concatenate vectors to have our final combination paste(combination$numero, combination$palo) #&gt; [1] &quot;Ace of Hearts&quot; &quot;Two of Hearts&quot; &quot;Three of Hearts&quot; #&gt; [4] &quot;Four of Hearts&quot; &quot;Five of Hearts&quot; &quot;Six of Hearts&quot; #&gt; [7] &quot;Seven of Hearts&quot; &quot;Eight of Hearts&quot; &quot;Nine of Hearts&quot; #&gt; [10] &quot;Ten of Hearts&quot; &quot;Jack of Hearts&quot; &quot;Queen of Hearts&quot; #&gt; [13] &quot;King of Hearts&quot; &quot;Ace of Diamonds&quot; &quot;Two of Diamonds&quot; #&gt; [16] &quot;Three of Diamonds&quot; &quot;Four of Diamonds&quot; &quot;Five of Diamonds&quot; #&gt; [19] &quot;Six of Diamonds&quot; &quot;Seven of Diamonds&quot; &quot;Eight of Diamonds&quot; #&gt; [22] &quot;Nine of Diamonds&quot; &quot;Ten of Diamonds&quot; &quot;Jack of Diamonds&quot; #&gt; [25] &quot;Queen of Diamonds&quot; &quot;King of Diamonds&quot; &quot;Ace of Spades&quot; #&gt; [28] &quot;Two of Spades&quot; &quot;Three of Spades&quot; &quot;Four of Spades&quot; #&gt; [31] &quot;Five of Spades&quot; &quot;Six of Spades&quot; &quot;Seven of Spades&quot; #&gt; [34] &quot;Eight of Spades&quot; &quot;Nine of Spades&quot; &quot;Ten of Spades&quot; #&gt; [37] &quot;Jack of Spades&quot; &quot;Queen of Spades&quot; &quot;King of Spades&quot; #&gt; [40] &quot;Ace of Clubs&quot; &quot;Two of Clubs&quot; &quot;Three of Clubs&quot; #&gt; [43] &quot;Four of Clubs&quot; &quot;Five of Clubs&quot; &quot;Six of Clubs&quot; #&gt; [46] &quot;Seven of Clubs&quot; &quot;Eight of Clubs&quot; &quot;Nine of Clubs&quot; #&gt; [49] &quot;Ten of Clubs&quot; &quot;Jack of Clubs&quot; &quot;Queen of Clubs&quot; #&gt; [52] &quot;King of Clubs&quot; Once our deck is created we can inquire some probabilities easily with the created vector. Let’s calculate the probability that when choosing a card it is “King of Diamonds”: # Store the combinationl in the variable deck deck &lt;- paste(combination$numero, combination$palo) mean(deck == &quot;King of Diamonds&quot;) #&gt; [1] 0.01923077 Or we can also calculate the probability that when choosing a card it is some Queen: # First create the vector of &quot;Queen of...&quot; queens &lt;- paste(&quot;Queen&quot;, suits) # Probability calculation mean(deck %in% queens) #&gt; [1] 0.07692308 7.3 Exercises Store in the variable prob the probability that when rolling a die you do not get the number 1. Using the variable prob, now calculate the probability that when rolling 3 times in a row, one after another, in none of those times the number 1 comes out. Solution prob &lt;- 5 / 6 prob * prob * prob Given a container containing 5 blue marbles, 3 yellow, and 4 gray. What is the probability that if you choose a marble at random it is blue? Solution marbles &lt;- rep(c(&quot;blue&quot;, &quot;yellow&quot;, &quot;gray&quot;), times = c(5, 3, 4)) # Solution using monte carlo simulation, repeating the event 10,000 times: results &lt;- replicate(10000, { sample(marbles, 1) }) prop.table(table(results)) # Solution using the `mean()` function: mean(marbles == &quot;blue&quot;) Mathematically it would be: Given the event: \\(X = chosen\\ marble\\ is\\ blue\\): \\(P(X)=\\frac{5}{5+3+4}=\\frac{5}{12}=41.67\\%\\) The probability that the marble is blue is 41.67%. What is the probability that when choosing a marble at random from the previous container it is not blue? Solution marbles &lt;- rep(c(&quot;blue&quot;, &quot;yellow&quot;, &quot;gray&quot;), times = c(5, 3, 4)) mean(marbles != &quot;blue&quot;) The probability is 58.33%. Mathematically it would be: Given the event \\(X = chosen\\ marble\\ is\\ blue\\): \\(P(\\sim~X)=1-P(X)=1-\\frac{5}{12}=1-41.67\\%=58.33\\%\\) Now we are going to take out a marble first, place it outside the box and take out another additional marble from the box. What is the probability that the first one is blue and the second is not blue? This time, instead of creating the marble vector, create the numerical variables: blue, yellow and gray assigning as value the number of marbles. Then calculate mathematics probabilities. Solution # Create variables blue &lt;- 5 yellow &lt;- 3 gray &lt;- 4 # Calculate probability that the first marble is blue: p_1 &lt;- blue / (blue + yellow + gray) # First calculate the probability that the second is blue: p_aux &lt;- (blue - 1) / (blue - 1 + yellow + gray) # Calculate the complement because they ask that the second is NOT blue: p_2 &lt;- 1 - p_aux # Calculate what is asked: p_1 * p_2 This is called sampling without replacement. We have two events, we are taking out two marbles. The second event depends on the first. These two events are not independent of each other. Now we will repeat the previous experiment, but after taking out the first marble we put it back in the box and take out one more marble at random. What is the probability that the first marble is blue and the second is not blue? Modify the R code you created previously to calculate this probability. Solution # Create variables blue &lt;- 5 yellow &lt;- 3 gray &lt;- 4 # Calculate probability that the first marble is blue: p_1 &lt;- blue / (blue + yellow + gray) # First calculate the probability that the second is blue: p_aux &lt;- blue / (blue + yellow + gray) # Calculate the complement because they ask that the second is NOT blue: p_2 &lt;- 1 - p_aux # Calculate what is asked: p_1 * p_2 This is called sampling with replacement. We have two events, we are taking out two marbles again. The second event does not depend on the first. These two events are independent. 7.4 Combinations and Permutations Some probability situations involve multiple events. When one of the events affects others, they are called dependent events. For example, when objects are chosen from a list or group and are not returned, the first choice reduces the options for future choices. There are two ways to order or combine results of dependent events. Permutations are groupings in which the order of objects matters. Combinations are groupings in which the content matters but the order does not. For this we will use the gtools package, which includes libraries like gtools that provide us with intuitive functionalities to work with permutations and combinations. # First install the gtools package install.packages(&quot;gtools&quot;) # To start using it, load the gtools library library(gtools) 7.4.1 Permutations The order matters when we calculate, for example, the winners of a competition. Suppose we have 10 students who are competing on equal terms for who builds the most accurate machine learning model. data_scientists &lt;- c(&quot;Jenny&quot;, &quot;Freddy&quot;, &quot;Yasan&quot;, &quot;Iver&quot;, &quot;Pamela&quot;, &quot;Alexandra&quot;, &quot;Bladimir&quot;, &quot;Enrique&quot;, &quot;Karen&quot;, &quot;Christiam&quot;) Only the top 3 will receive the prize. In this case the order matters, so we will use the function permutations(total, selection, data) where total indicates the size of the vector, selection indicates the size of the result I want, and finally data is my source vector. permutations(10, 3, v = data_scientists) We have already calculated all possible results. We can calculate on this result the probability that Freddy wins the competition and that Pamela is in second place. results &lt;- permutations(10, 3, v = data_scientists) # Total results: nrow(results) total &lt;- nrow(results) # Probability that Freddy wins: mean(results[, 1] == &quot;Freddy&quot; &amp; results[, 2] == &quot;Pamela&quot;) #&gt; [1] 0.01111111 7.4.2 Combinations The order does not matter when, for example, we form groups of 2 to participate in the competition. combinations(10, 2, v = data_scientists) If now only one team is going to win the prize, we could calculate the probability that the team made up of Pamela and Enrique are the ones who win. results &lt;- combinations(10, 2, v = data_scientists) # Total results: nrow(results) #&gt; [1] 45 # Probability: mean((results[, 1] == &quot;Pamela&quot; &amp; results[, 2] == &quot;Enrique&quot;) | (results[, 1] == &quot;Enrique&quot; &amp; results[, 2] == &quot;Pamela&quot;)) #&gt; [1] 0.02222222 Although we can obtain the probability by calculating all combinations, in R it will be very frequent to use Monte Carlo to estimate the probability by simulation. For the previous case we would not have to generate all combinations, but simply take a sample of two people who would be the members of the winning team. Recall that we have assumed that everyone has equal chances of winning. sample(data_scientists, 2) Then, we would have to replicate this experiment over and over again, store the sampling results and calculate the proportion of how many times the winning team was composed of Pamela and Enrique. n &lt;- 10000 resultado &lt;- replicate(n, { equipo &lt;- sample(data_scientists, 2) cumple_condicion &lt;- (equipo[1] == &quot;Pamela&quot; &amp; equipo[2] == &quot;Enrique&quot;) | (equipo[2] == &quot;Pamela&quot; &amp; equipo[1] == &quot;Enrique&quot;) cumple_condicion }) mean(resultado) #&gt; [1] 0.0199 Note that, as we saw previously, the value converges as we increase the number of times we repeat the experiment n. We have simulated repeating the experiment 10 thousand times. However, how many times would it be necessary to replicate the experiment to trust the results of the simulation? 7.5 Sufficient Experiments with Monte Carlo Simulation Intuitively we can indicate that the greater the number of experiments the more precise the estimated probability. We can, thus, do several simulations with different number of experiments for each simulation. In this way we could find a reasonable number of experiments for our simulation. To do this, first we create a numerical vector where the number of times we are going to simulate the experiment is indicated. Our vector will contain the following values: 10, 20, 40, 80, 160,…, etc. This means that the first time we will simulate the experiment 10 times, the second 20 times and so on. n_times &lt;- 10*2^(1:17) n_times #&gt; [1] 20 40 80 160 320 640 1280 2560 5120 #&gt; [10] 10240 20480 40960 81920 163840 327680 655360 1310720 Then, we use the code we created to replicate the experiment to create a function called probabilidad_por_muestra: probabilidad_por_muestra &lt;- function(n) { resultado &lt;- replicate(n, { equipo &lt;- sample(data_scientists, 2) cumple_condicion &lt;- (equipo[1] == &quot;Pamela&quot; &amp; equipo[2] == &quot;Enrique&quot;) | (equipo[2] == &quot;Pamela&quot; &amp; equipo[1] == &quot;Enrique&quot;) cumple_condicion }) mean(resultado) } We already have a function that allows us to replicate the experiment as many times as we want. For example, in the previous section we simulated 10 thousand experiments. Now that we have created the function we would do: # Probability using functions: probabilidad_por_muestra(10000) #&gt; [1] 0.0215 Again, this is a simulation. So every time we execute that function the probability will vary as it is a random sample. To apply a function on each of the values of a vector we use the function sapply(vector, function) where vector is the vector where the data on which I want to apply the function is and function is the function I want to apply. prob &lt;- sapply(n_times, probabilidad_por_muestra) prob #&gt; [1] 0.00000000 0.02500000 0.01250000 0.01875000 0.03125000 0.01562500 #&gt; [7] 0.02421875 0.01562500 0.02382812 0.02236328 0.02294922 0.02370605 #&gt; [13] 0.02252197 0.02181396 0.02189026 0.02223511 0.02223129 This gives us the probabilities depending on the number of times we repeat the experiment. Now let’s place these results in a scatter plot to see how it converges probabilities &lt;- data.frame( n = n_times, probabilidad = prob ) probabilities |&gt; ggplot() + aes(n, probabilidad) + geom_line() + geom_point() + xlab(&quot;# of times of the experiment&quot;) We can also change the scale to zoom in on the probabilities for smaller experiment number values and add a reference line with the theoretical probability value calculated previously: probabilities |&gt; ggplot() + aes(n, probabilidad) + geom_line() + geom_point() + xlab(&quot;# of times of the experiment&quot;) + scale_x_continuous(trans = &quot;log2&quot;) + geom_hline(yintercept = 0.022222, color = &quot;blue&quot;, lty = 2) We observe that, for this experiment, repeating the experiment 10 thousand times (x-axis = 3 because it is \\(10^3\\)) already gives us a good approximation to the real value. 7.6 Case: Birthdays in Classrooms Let’s review the concepts learned with another example. In a Data Science for Managers class there are 50 students. Using Monte Carlo simulation, let’s estimate what is the probability that there are at least two people who have birthdays on the same day. (Let’s ignore those who have birthdays on February 29). First let’s list all the days of the year available for birthdays: days &lt;- 1:365 Let’s generate a random sample of 50 numbers from the days vector, but this time with replacement because a person could have the same day, and store it in the colleagues variable. colleagues &lt;- sample(days, 50, replace = TRUE) To validate if any of the values are repeated we will use the duplicated() function which validates if there are duplicate values within the vector: duplicated(colleagues) #&gt; [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #&gt; [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #&gt; [25] FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #&gt; [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE #&gt; [49] FALSE FALSE Finally, to determine if there was any TRUE value we use the any() function: any(duplicated(colleagues)) #&gt; [1] TRUE The result tells us whether it is true or not that there are at least two people who have birthdays on the same day. To estimate by Monte Carlo simulation what the probability is, we have to repeat the experiment many times and take the proportion of how many times we get TRUE as a result. # Monte Carlo simulation with 10 thousand repetitions n_times &lt;- 10000 results &lt;- replicate(n_times, { colleagues &lt;- sample(days, 50, replace = TRUE) # Returns a logical value of whether there are duplicates any(duplicated(colleagues)) }) # Probability: mean(results) #&gt; [1] 0.9697 We see that the estimated probability is very high, above 95%. What would happen if I have a room of 25 people? To do this, we modify the previous code and create the variable classroom which will indicate the number of students in that class: # Monte Carlo simulation with 10 thousand repetitions n_times &lt;- 10000 classroom &lt;- 25 results &lt;- replicate(n_times, { # Returns a logical vector colleagues &lt;- sample(days, classroom, replace = TRUE) any(duplicated(colleagues)) }) # Probability: mean(results) #&gt; [1] 0.5649 Let’s now create the function estima_probabilidad and estimate using this function the probability of finding at least two people with the same birthday in a room of 25 people. This time we have to specify that the sampling is with “replacement” because by default the sample() function is “without replacement”. # Create the function estima_probabilidad &lt;- function(classroom, n_times = 10000){ results &lt;- replicate(n_times, { # Returns a logical vector colleagues &lt;- sample(days, classroom, replace = TRUE) any(duplicated(colleagues)) }) # Probability: mean(results) } estima_probabilidad(25) #&gt; [1] 0.5687 Finally, if we already have a function that calculates based on the number of people in a room we can create a numerical vector with the total number of people from different rooms and then apply the function we have created. The result can be stored in the variable prob. # Create 80 different classrooms # The first room with 1 person, the last room with 80 people classrooms &lt;- 1:80 # Estimate probability depending on the number of students per room prob &lt;- sapply(classrooms, estima_probabilidad) prob #&gt; [1] 0.0000 0.0025 0.0083 0.0153 0.0279 0.0392 0.0588 0.0741 0.0988 0.1158 #&gt; [11] 0.1443 0.1653 0.1962 0.2230 0.2520 0.2801 0.3223 0.3488 0.3835 0.4014 #&gt; [21] 0.4438 0.4770 0.5116 0.5344 0.5714 0.5942 0.6318 0.6571 0.6848 0.7131 #&gt; [31] 0.7335 0.7501 0.7777 0.7957 0.8101 0.8255 0.8542 0.8572 0.8801 0.8962 #&gt; [41] 0.9055 0.9131 0.9231 0.9354 0.9427 0.9495 0.9556 0.9570 0.9661 0.9660 #&gt; [51] 0.9738 0.9747 0.9831 0.9843 0.9867 0.9892 0.9915 0.9909 0.9935 0.9945 #&gt; [61] 0.9948 0.9960 0.9961 0.9980 0.9973 0.9983 0.9979 0.9992 0.9989 0.9989 #&gt; [71] 0.9989 0.9998 0.9998 1.0000 0.9999 0.9998 0.9999 0.9995 1.0000 0.9998 Thus, if we place it in a scatter plot we can see how the probability increases as there are more students: probabilities &lt;- data.frame( n = classrooms, probabilidad = prob ) probabilities |&gt; ggplot() + aes(n, probabilidad) + geom_point() + xlab(&quot;Number of students in each class&quot;) We can now impress our friends from different groups by telling them that, if they are in a room of 60 people, “you can bet them” that there are two people in that room who have birthdays on the same day. It is not definitive, but the chances are very much in our favor. 7.7 Exercises Two students of the Data Science with R course, Alonso and Georgina, usually play chess in their free time. Given Georgina’s experience, she has a 60% chance of winning every time she plays with Alonso. Without using Monte Carlo simulation What is the probability that when playing 4 times in a row, Alonso has won at least once. Solution Calculating the probability that Alonso has won at least once is the complement of the probability that Georgina has won all 4 times. Thus, we will first calculate the probability that Georgina has always won and then calculate the complement. # Probability that Georgina wins all 4 games prob &lt;- 0.6^4 # Probability that Alonso wins at least once 1 - prob Estimate the previous probability using Monte Carlo simulation. Use the following code to generate a sample of 4 games, where Alonso has a 60% probability of losing to Georgina. resultado_juegos &lt;- sample(c(&quot;loses&quot;,&quot;wins&quot;), 4, replace = TRUE, prob = c(0.6, 0.4)) Solution # Times I run the simulation n_times &lt;- 10000 # Generate result of experiments that Alonso wins alonso_gana &lt;- replicate(n_times, { resultado_juegos &lt;- sample(c(&quot;loses&quot;,&quot;wins&quot;), 4, replace = TRUE, prob = c(0.6, 0.4)) any(resultado_juegos == &quot;wins&quot;) }) # Estimate probability mean(alonso_gana) In the previous exercise we used the probability that Alonso wins as 40%. Now, create the function probabilidad_de_ganar that takes as input the probability p that Alonso wins. Then, assign this sequence seq(0.4, 0.95, 0.025) in a vector p to, finally, apply the created function to each value of vector p and store the result in the variable prob. Report a scatter plot of vector p on the x-axis and vector prob on the y-axis Solution # Create function probabilidad_de_ganar &lt;- function(p){ n_times &lt;- 10000 alonso_gana &lt;- replicate(n_times, { resultado_juegos &lt;- sample(c(&quot;loses&quot;,&quot;wins&quot;), 4, replace = TRUE, prob = c(1-p, p)) any(resultado_juegos == &quot;wins&quot;) }) mean(alonso_gana) } # Create vector with different probabilities p &lt;- seq(0.4, 0.95, 0.025) prob &lt;- sapply(p, probabilidad_de_ganar) plot(p, prob, xlab=&quot;p: probability that Alonso wins in each game&quot;, ylab=&quot;prob: prob. that Alonso wins at least one game&quot;) 7.8 Integrative Exercise Let’s solve together this exercise that integrates everything we have learned in this chapter, called the Monty Hall problem. 7.8.1 Monty Hall Problem Monty Hall was a TV presenter who made famous a contest in his show which we are going to replicate below. We have three doors in front of us: Behind one of these doors is a zero-kilometer car, while in the other two there is a goat. We, as contest participants, have to choose together which door to open. Whatever is behind it will be ours. Suppose we have chosen door number 2. Once you announce our choice, Monty Hall tells us that he is going to help us and opens a door for us right now. He opens one of the other doors and it turns out there is a goat in door 3 that he opened. Monty Hall asks us: I am going to give you a chance to change doors and that will be your final choice, Would you change doors or stick with the door chosen at the beginning? Intuitively we knew that, when all doors were closed, the car is in one of 3 doors. The probability of winning would be \\(\\frac{1}{3} = 0.3333\\) so it didn’t matter which door to choose. But when he opens door number three he gives us information and the first thing we should ask ourselves is whether the probabilities have been affected or not. Although this is an advanced math exercise using change of variable, we can execute a Monte Carlo simulation to estimate the probabilities and solve it without using almost any mathematical formula. Let’s start by simulating the experiment. At the beginning we had three doors, door 1, 2 and 3. We will create the variable doors. doors &lt;- c(&quot;door 1&quot;, &quot;door 2&quot;, &quot;door 3&quot;) Then, we know that behind the doors there is a car and two goats distributed randomly. We will use the function sample to order them randomly. prizes &lt;- sample(c(&quot;car&quot;, &quot;goat&quot;, &quot;goat&quot;)) Since Monty Hall knows where the prize is. We are going to create a variable prize_door where we will store where the car is. prize_door &lt;- doors[prizes == &quot;car&quot;] Now we choose a door randomly and store our result in the variable choice. choice &lt;- sample(doors, 1) Given that we already have the chosen door shown we will simulate Monty Hall choosing the door to open. Since he is the presenter he will choose any door that is not the door where the prize is or your door. door_opened &lt;- sample(doors[!doors %in% c(choice, prize_door)],1) Finally, we are going to put all the code together and in the last line add the comparison of whether the prize door matches our choice. This time we are going to choose not to change doors, so our choice does not vary. doors &lt;- c(&quot;door 1&quot;, &quot;door 2&quot;, &quot;door 3&quot;) prizes &lt;- sample(c(&quot;car&quot;, &quot;goat&quot;, &quot;goat&quot;)) prize_door &lt;- doors[prizes == &quot;car&quot;] choice &lt;- sample(doors, 1) door_opened &lt;- sample(doors[!doors %in% c(choice, prize_door)],1) choice == prize_door With our experiment created we are going to simulate what would happen if we stick with the choice and what would happen if we change it. 7.8.1.1 Stick with the chosen door Let’s replicate about 10 thousand times to see the proportion of times we would win if we stick with our door. n_times &lt;- 10000 results &lt;- replicate(n_times, { doors &lt;- c(&quot;door 1&quot;, &quot;door 2&quot;, &quot;door 3&quot;) prizes &lt;- sample(c(&quot;car&quot;, &quot;goat&quot;, &quot;goat&quot;)) prize_door &lt;- doors[prizes == &quot;car&quot;] choice &lt;- sample(doors, 1) door_opened &lt;- sample(doors[!doors %in% c(choice, prize_door)],1) choice == prize_door }) mean(results) #&gt; [1] 0.326 We see that the probability obtained by Monte Carlo simulation is an estimation very close to the probability that we had intuitively calculated. That is, if we keep our choice of the door we chose we have a 33.33% probability of winning. But, what happens if we change doors? Is the probability of winning the same? 7.8.1.2 Change door We are going to use the code and modify it by creating the variable new_choice to make the door change. n_times &lt;- 10000 results &lt;- replicate(n_times, { doors &lt;- c(&quot;door 1&quot;, &quot;door 2&quot;, &quot;door 3&quot;) prizes &lt;- sample(c(&quot;car&quot;, &quot;goat&quot;, &quot;goat&quot;)) prize_door &lt;- doors[prizes == &quot;car&quot;] choice &lt;- sample(doors, 1) door_opened &lt;- sample(doors[!doors %in% c(choice, prize_door)],1) new_choice &lt;- doors[!doors %in% c(choice, door_opened)] new_choice == prize_door }) mean(results) #&gt; [1] 0.6641 As we see, changing the door in this show gave us a probability of 66.66% of winning, while keeping our choice only 33.33%. It may sound counterintuitive, but statistically speaking it is better to change doors instead of trusting our luck and keeping the initial choice. "],["continuous-probabilities.html", "Chapter 8 Continuous Probabilities 8.1 Empirical Distribution 8.2 Theoretical Distribution 8.3 Exercises 8.4 Monte Carlo Simulation for Continuous Variables 8.5 Exercises", " Chapter 8 Continuous Probabilities Recall that a continuous variable is a variable that takes values along a continuum, that is, over an entire interval of values. An essential attribute of a continuous variable is that, unlike a discrete variable, it can never be measured exactly; the observed value depends largely on the precision of the measuring instruments. With a continuous variable, there is inevitably a measurement error. As an example, the height of a person (1.72m, 1.719m, 1.7186m….). Another example could be the time it takes an athlete to run 100 meters flat, since this time can take values such as 9.623 seconds; 10.456485 seconds; 12.456412 seconds; that is, an interval of values. For example, recall that in the heights data frame we have the heights of a group of university students. heights |&gt; filter(sex == &quot;Male&quot;) |&gt; # Filter only males mutate(height_m = height/39.37) |&gt; # Convert to centimeters ggplot() + aes(sex, height_m, color = height_m) + geom_point(position = position_jitterdodge()) When graphing the data distribution, we intuitively realize that it does not make sense to calculate the proportion of people who measure exactly 1.73m because it would also serve us if a person measures 1.731, 1.729, or any close value that is not exactly 1.73, whether due to how it was measured or any other type of error. It makes more sense to analyze the data by intervals, as can be seen in this histogram that groups by intervals of 0.05 meters = 5 cm. heights |&gt; filter(sex == &quot;Male&quot;) |&gt; # Filter only males mutate(height_m = height/39.37) |&gt; # Convert to centimeters ggplot() + aes(height_m) + geom_histogram(binwidth = 0.05, color = &quot;black&quot;) It is much more practical to define a function that operates on intervals instead of unique values. For this we use the Cumulative Distribution Function (CDF). 8.1 Empirical Distribution When we use data to analyze its distribution, we speak of an empirical distribution. It is the actual distribution of a subject or an option, and measures the real and individual possibilities, regarding the measurement of the subject’s direct score, or of an option of which the frequency of occurrence has been measured. For example, for our case we can create the vector men (men) made up of all the values of the heights of men: men &lt;- heights |&gt; filter(sex == &quot;Male&quot;) |&gt; mutate(height_m = height/39.37) |&gt; pull(height_m) Then, we can create the function FDA (CDF) that takes x as a variable and calculates the proportion of men who measure less than or equal to x within the data found in the men vector. FDA &lt;- function(x){ mean(men &lt;= x) } Thus, if we want to calculate what the proportion of students who measure 1.73m or less would be. FDA(1.73) #&gt; [1] 0.3768473 On the other hand, recall that the median is the value that divides our data into two equal parts. So, if we calculate the median: median(men) #&gt; [1] 1.752604 And then we enter the value 1.7526035 into our function to ask what is the proportion of students who measure 1.7526035 or less, we should get a value very close to 50% by definition of the median. median_val &lt;- median(men) FDA(median_val) #&gt; [1] 0.5073892 So far we have calculated proportions with the cumulative distribution function FDA. However, if we want to know what is the probability that when choosing a man at random he measures 1.9m or less, we could use the same FDA. Since each student has the same chance of being chosen, the answer to the question would be the proportion of students who measure 1.9 or less. \\(F(1.9) = P(x \\le 1.9)\\) FDA(1.9) #&gt; [1] 0.9396552 We observe that the probability is approximately 93.97%. If we now want to calculate the probability that someone chosen at random is taller than 1.80m, we first calculate the FDA for 1.8 and then obtain the complement. \\(P(x &gt; 1.8) = 1 - P(x \\le 1.8)\\) # Probability of measuring 1.80m or less prob &lt;- FDA(1.8) # Probability of measuring more than 1.80m 1 - prob #&gt; [1] 0.3583744 The probability is approximately 35.8%. If we now wanted to know the probability that when choosing someone at random they measure more than 1.6m, but not more than 1.95m, we would have. \\(P(x &gt; 1.6\\ \\cap\\ x \\le 1.95) = P(x \\le 1.95) - P(x \\le 1.6)\\) prob_1 &lt;- FDA(1.95) prob_2 &lt;- FDA(1.6) prob_1 - prob_2 #&gt; [1] 0.9445813 8.2 Theoretical Distribution On the other hand, a theoretical distribution is a distribution that is derived from certain principles or assumptions by logical and mathematical reasoning, as opposed to one derived from real-world data obtained by empirical research. Among them we have the normal distribution, the binomial distribution, and the Poisson distribution. For example, if we draw an approximate line of our data on men’s heights we would have this graph: heights |&gt; filter(sex == &quot;Male&quot;) |&gt; mutate(height_m = height/39.37) |&gt; ggplot() + aes(height_m) + geom_histogram(binwidth = 0.05, color = &quot;black&quot;) + geom_density(aes(y = ..count..*0.05), colour = &quot;blue&quot;, lty = 5) We see that the distribution has an approximately symmetric, bell shape. This distribution could be modeled using a normal distribution (also called Gaussian distribution, Gauss curve, or Gauss bell). To do this, in R we will use the function pnorm(x, average, std_dev) to estimate the probability but using a normal distribution function with an average promedio and a standard deviation desv_est. In this way, we can estimate what is the probability that if we choose a value at random it is less than or equal to x. For example, let’s calculate again the probability that when choosing a man at random he measures 1.65m or less, we could use the same FDA and now the pnorm() function. # Using the empirical distribution (real data): FDA(1.9) #&gt; [1] 0.9396552 # Using the theoretical normal distribution (approx. data): avg &lt;- mean(men) std_dev &lt;- sd(men) probability &lt;- pnorm(1.9, avg, std_dev) probability #&gt; [1] 0.9357267 We obtain approximately the same results. Using a normal distribution facilitates our work when our data has a normal behavior. Mathematically we are calculating the area under the curve which is seen in blue: sec &lt;- seq(-4, 4, length = 100) * std_dev + avg normal &lt;- dnorm(sec, avg, std_dev) data.frame(value = normal) |&gt; ggplot() + aes(sec, value) + geom_line() + theme(axis.text.y = element_blank()) + xlab(&quot;Height (Estatura)&quot;) + ylab(&quot;&quot;) + ggtitle(&quot;Normal Distribution&quot;) + geom_area(aes(x = ifelse(sec &lt; 1.9, sec, 0)), fill = &quot;blue&quot;) + xlim(min(sec), max(sec)) + labs(subtitle = paste(&quot;P(x &lt;= 1.9) =&quot;, probability)) In the same way, we could estimate the probability that a person chosen at random measures more than 1.8m. # Using the empirical distribution (real data): 1- FDA(1.8) #&gt; [1] 0.3583744 # Using the theoretical normal distribution (approx. data): avg &lt;- mean(men) std_dev &lt;- sd(men) probability &lt;- 1- pnorm(1.8, avg, std_dev) probability #&gt; [1] 0.3337484 Mathematically we are calculating the area under the curve which is seen in blue: sec &lt;- seq(-4, 4, length = 100) * std_dev + avg normal &lt;- dnorm(sec, avg, std_dev) data.frame(value = normal) |&gt; ggplot() + aes(sec, value) + geom_line() + theme(axis.text.y = element_blank()) + xlab(&quot;Height (Estatura)&quot;) + ylab(&quot;&quot;) + ggtitle(&quot;Normal Distribution&quot;) + geom_area(aes(x = ifelse(sec &gt; 1.8, sec, 0)), fill = &quot;blue&quot;) + xlim(min(sec), max(sec)) + labs(subtitle = paste(&quot;P(x &gt; 1.8) =&quot;, probability)) Finally, let’s recalculate the probability that when choosing someone at random they measure more than 1.6m, but not more than 1.95m, we would have. # Using the empirical distribution (real data): prob_1 &lt;- FDA(1.95) prob_2 &lt;- FDA(1.6) prob_1 - prob_2 #&gt; [1] 0.9445813 # Using the theoretical normal distribution (approx. data): avg &lt;- mean(men) std_dev &lt;- sd(men) probability &lt;- pnorm(1.95, avg, std_dev) - pnorm(1.6, avg, std_dev) probability #&gt; [1] 0.9405618 Mathematically we are calculating the area under the curve which is seen in blue: sec &lt;- seq(-4, 4, length = 100) * std_dev + avg normal &lt;- dnorm(sec, avg, std_dev) data.frame(value = normal) |&gt; ggplot() + aes(sec, value) + geom_line() + theme(axis.text.y = element_blank()) + xlab(&quot;Height (Estatura)&quot;) + ylab(&quot;&quot;) + ggtitle(&quot;Normal Distribution&quot;) + geom_area(aes(x = ifelse(sec &gt; 1.6 &amp; sec &lt;= 1.95, sec, 0)), fill = &quot;blue&quot;) + xlim(min(sec), max(sec)) + labs(subtitle = paste(&quot;P(1.6 &lt; x &lt;= 1.95) =&quot;, probability)) We can plot a Q-Q plot, which is a scatter plot created by plotting two sets of quantiles against each other. The function stat_qq(x) creates a normal Q-Q plot. This function plots the data in sorted order against the quantiles of a standard Normal distribution. The function stat_qq_line() adds a reference line. Although understanding this requires advanced statistics, we can interpret it that if when using this function the correlation is very close to the line then our data is very likely to follow a normal distribution. heights |&gt; filter(sex == &quot;Male&quot;) |&gt; mutate(height_m = height/39.37) |&gt; ggplot() + aes(sample = height_m) + stat_qq() + stat_qq_line() The points seem to fall on a straight line. This gives us a good indication that assuming our height data comes from a population that is normally distributed is reasonable. Observe that the y-axis plots the empirical quantiles and x-axis plots the theoretical quantiles. The latter are the quantiles of the standard Normal distribution with mean 0 and standard deviation 1. Visual inspection is not always reliable. It is possible to use a significance test that compares the sample distribution with a normal one to determine whether or not the data shows a serious deviation from normality. The most used test for these tests is the Shapiro-Wilk normality test. For this we will use the function shapiro.test(), which performs a normality test and gives us a p-value^(https://www.investopedia.com/terms/p/p-value.asp). It is based on the correlation between the data and the corresponding normal scores. If the p-value &gt; 0.05 then the data distribution is not significantly different from the normal distribution. In other words, we can assume normality. shapiro.test(men) #&gt; #&gt; Shapiro-Wilk normality test #&gt; #&gt; data: men #&gt; W = 0.96374, p-value = 2.623e-13 The p-value is less than 0.05 so, although our distribution resembles a normal one, it does not pass the significance test. 8.3 Exercises For the following exercises assume that the distribution of ages of students in the Data Science with R course approximates a normal distribution with an average of 24 years and a standard deviation of 3. If we select a student at random: What is the probability that they are at most 23 years old? Solution avg_e &lt;- 24 std_dev_e &lt;- 3 pnorm(23, avg_e, std_dev_e) What is the probability that they are older than 28 years? Solution avg_e &lt;- 24 std_dev_e &lt;- 3 1 - pnorm(28, avg_e, std_dev_e ) What is the probability that they are older than 22, but at most 27 years old? Solution avg_e &lt;- 24 std_dev_e &lt;- 3 pnorm(27, avg_e, std_dev_e ) - pnorm(22, avg_e, std_dev_e ) What is the probability that they are at most one standard deviation away from the average? Solution avg_e &lt;- 24 std_dev_e &lt;- 3 max &lt;- avg_e + std_dev_e min &lt;- avg_e - std_dev_e pnorm(max, avg_e, std_dev_e ) - pnorm(min, avg_e, std_dev_e ) 8.4 Monte Carlo Simulation for Continuous Variables Although we have used a normal function to calculate the approximate probability, we can create more than one normal function with that average and that standard deviation. We will use the function rnorm(n, average, std_dev) to create a vector of n random data, such that they are normally distributed with an average avg and a standard deviation std_dev. Recall that our original data has the following characteristics: avg &lt;- mean(men) avg #&gt; [1] 1.760598 std_dev &lt;- sd(men) std_dev #&gt; [1] 0.09172018 length_val &lt;- length(men) length_val #&gt; [1] 812 If we want to generate a random normal distribution we will use rnorm(): # Creation of the normally distributed random vector: random_normal &lt;- rnorm(length_val, avg, std_dev) # We create a histogram to visualize it better hist(random_normal) result &lt;- paste(&quot;Sample:&quot;, length_val, &quot;. Average:&quot;, round(mean(random_normal), 3), &quot;. Std. dev.:&quot;, round(sd(random_normal), 3) ) mtext(result,3) We can execute the code again to verify that it generates another distribution for us: # Creation of the normally distributed random vector: random_normal &lt;- rnorm(length_val, avg, std_dev) # We create a histogram to visualize it better hist(random_normal) result &lt;- paste(&quot;Sample:&quot;, length_val, &quot;. Average:&quot;, round(mean(random_normal), 3), &quot;. Std. dev.:&quot;, round(sd(random_normal), 3) ) mtext(result,3) We can repeat this experiment of obtaining n random data that have approximately the same average and the same std_dev about 10 thousand times to calculate the proportion of times that a man measures more than 1.8m. n_times &lt;- 10000 simulation_results &lt;- replicate(n_times, { random_normal &lt;- rnorm(length_val, avg, std_dev) random_normal &gt; 1.8 }) mean(simulation_results) #&gt; [1] 0.3339383 Thus, we have obtained practically the same value that we achieved in the previous section, but this time estimating using the Monte Carlo simulation. 8.5 Exercises The distribution of the admission exam grades of the Univ. UNISM is distributed approximately normally. The average is 14.5 and the standard deviation is 1. We want to know the distribution of the first place. It is known that 5 thousand people apply once a year per exam and take a single exam. Generate 5 thousand grades about 1,000 times using Monte Carlo simulation and perform a histogram of the result. Solution n_times &lt;- 1000 avg &lt;- 14.5 std_dev &lt;- 1 max_grades &lt;- replicate(n_times, { exam_simulation &lt;- rnorm(5000, avg, std_dev) max(exam_simulation) }) hist(max_grades) shapiro.test(max_grades) Modify the previous simulation to analyze the distribution of the average grades of each year. Solution n_times &lt;- 1000 avg &lt;- 14.5 std_dev &lt;- 1 notas_avg &lt;- replicate(n_times, { exam_simulation &lt;- rnorm(5000, avg, std_dev) mean(exam_simulation) }) hist(notas_avg) shapiro.test(notas_avg) Using the first Monte Carlo simulation created, calculate the probability that the first place in grades is greater than 18.5. Solution n_times &lt;- 1000 avg &lt;- 14.5 std_dev &lt;- 1 max_grades &lt;- replicate(n_times, { exam_simulation &lt;- rnorm(5000, avg, std_dev) max(exam_simulation) }) mean(max_grades &gt; 18.5) "],["statistical-inference.html", "Chapter 9 Statistical Inference 9.1 Expected Value 9.2 Central Limit Theorem 9.3 Exercises 9.4 Parameter Estimation Method 9.5 Spread Estimation 9.6 Estimates Outside Election Polls 9.7 Exercises", " Chapter 9 Statistical Inference To infer means to draw a conclusion from general or particular facts. Statistical inference is a set of methods and techniques that allow deducing characteristics of a population using data from a random sample. The method we are going to use most to infer is the parameter estimation method. We estimate parameters of a population from a sample because very rarely we will be able to have access to all the data of the population. Such is the case of election polls, disease studies, etc. Recall previously some concepts such as expected value, standard error, among others, that will be useful to us to make inferences. 9.1 Expected Value Let’s use the following case to understand this concept intuitively. We have been hired in a casino to analyze if it is reasonable to install a roulette with 37 values ranging from 0 to 36. The house wants to open the game with a special offer if the ball lands on 0 or 21 paying 10 to 1. This means that if a player plays and wins we pay him 10 soles and if he loses he would pay us 1 sol. With what we have learned so far we can simulate our game with the case data. We have 37 values, of which in 2 of them give a player a profit of +10 or a loss -1. Let’s also define prob_win as the probability that a player wins. # Total times played plays &lt;- 1 # Probability that a player wins each time prob_win &lt;- 2/37 prob_lose &lt;- 1 - prob_win # Random sample sample_vec &lt;- sample(c(10, -1), plays, replace = TRUE, prob = c(prob_win, prob_lose)) sample_vec #&gt; [1] -1 The distribution of this variable is simple given that it can only take two values: 10 or -1. When we simulate a very large number of games it can be seen how it is distributed according to the indicated probability of winning and losing. plays &lt;- 100000 prob_win &lt;- 2/37 prob_lose &lt;- 1 - prob_win sample_vec &lt;- sample(c(10, -1), plays, replace = TRUE, prob = c(prob_win, prob_lose)) hist(sample_vec) We have been using Monte Carlo simulation to estimate what the mean of the game results would be in real life. # Estimation of the mean by Monte Carlo simulation mean(sample_vec) #&gt; [1] -0.40875 In addition, we have seen that, the more the sample grows, our mean in the Monte Carlo simulation converges to a value, in this case the probability of winning mainly in the roulette. That value to which it converges we will call expected value, which as its name indicates will be the value we expect to obtain in reality. The more the sample size grows the more our sample mean converges to this expected value. The notation we will use will be \\(E[X]\\). When there are only two possible results \\(a\\) and \\(b\\) with proportions \\(p\\) and \\(1-p\\) respectively, the expected value will be calculated using this formula: \\(E[X] = ap + b(1-p)\\) # Expected Value: (10) * prob_win + (-1) * prob_lose #&gt; [1] -0.4054054 Previously we had calculated the mean using Monte Carlo simulation. If we compare it with the expected value we see how both numbers are approximately the same, as the theory predicts. Returning to the simulation of roulette games, a single person does not play so many times. Each person plays about 40 times a day at roulette. Thus, we can generate 40 games that a random player could play and find how much he would win: plays &lt;- 40 prob_win &lt;- 2/37 prob_lose &lt;- 1 - prob_win sample_vec &lt;- sample(c(10, -1), plays, replace = TRUE, prob = c(prob_win, prob_lose)) sum(sample_vec) #&gt; [1] -7 Finally, not only one person will play. Let’s replicate this sample about 100,000 times to simulate the number of players we would have in a quarter. players &lt;- 100000 jugadas &lt;- 40 prob_win &lt;- 2/37 prob_lose &lt;- 1 - prob_win winnings_simulation &lt;- replicate(players, { sample_vec &lt;- sample(c(10, -1), jugadas, replace = TRUE, prob = c(prob_win, prob_lose)) sum(sample_vec) }) So far we have done the same as we have learned in previous chapters. However, we could also see how the players’ winnings are distributed. And for that it is enough to create a histogram of the result. hist(winnings_simulation) It is not a coincidence that if we create a histogram with all the winnings of all the players the result looks like a normal distribution. In fact, that was the main approach that George Pólya made in 1920 when he presented his Central Limit Theorem. 9.2 Central Limit Theorem The Central Limit Theorem tells us that if we take several samples of the same size \\(n\\) and in each sample we sum the values within each sample we will obtain a value \\(S\\) (the sum) then we will find that its distribution approximates well to a normal curve. If we replicate this language to our example it would be: The central limit theorem tells us that if we take samples of 40 games for each player and then calculate for each player the total they won, then we will find that the distribution of the amount won by many players approximates a normal distribution. Since it is a new distribution, we can calculate its mean and standard deviation. Being samples we will use the learned term expected value of the sum to refer to the sample mean and we will add the term of standard error of the sum to refer to the sample standard deviation This would be the formula to calculate the expected value of the sum: \\(E[S_n] = n (ap+b(1-p))\\) plays &lt;- 40 prob_win &lt;- 2/37 prob_lose &lt;- 1 - prob_win # Expected value of the sum E_sum &lt;- plays * ( (10)*prob_win + (-1)*prob_lose ) E_sum #&gt; [1] -16.21622 And to calculate the standard error of the sum we will use the following formula: \\(SE[S_n]=\\sqrt{n}\\ |a-b|\\ \\sqrt{p(1-p)}\\) plays &lt;- 40 prob_win &lt;- 2/37 prob_lose &lt;- 1 - prob_win # Standard error of the sum SE_sum &lt;- sqrt(plays) * abs(10 - -1) * sqrt(prob_win*prob_lose) SE_sum #&gt; [1] 15.73149 With these two theoretical data, the expected value and the standard error, we can graph the normal curve of the sum of winnings of our game. What does this mean? That if theoretically we can already graph the normal curve then we can also calculate the probability that the sum is greater or less than some value. This is the main advantage of the Central Limit Theorem since we can calculate probabilities of the population using this approximation and the data of a single sample. For example, if we want to know what is the probability that a player wins money after playing 40 times in roulette we would have to calculate the probability that \\(S\\) is greater than zero, represented by the blue shaded area: To perform this calculation in R we would use the pnorm function: # Probability of getting more than 0 soles having played 40 games: 1- pnorm(0, E_sum, SE_sum) #&gt; [1] 0.1513144 Let’s validate that the Monte Carlo simulation approximates this theoretical value we just calculated: # Probability of getting more than 0 soles having played 40 games: mean(winnings_simulation &gt; 0) #&gt; [1] 0.16833 We have used two ways to estimate the probability, the theoretical estimation using the central limit theorem and the Monte Carlo simulation. These two numbers are quite close to the real probability. In both cases, the larger the sample, the more reasonable our estimation will be. On the other hand, the same happens if we wanted to analyze the average and not the sum of the winnings. But for the average case we will use the following formulas: Expected value of the average: \\(E[\\overline{X}]=ap+b(1-p)\\). Standard error of the average: \\(SE[\\overline{X}]=|a-b|\\sqrt{\\frac{p(1-p) }{n}}\\). 9.3 Exercises The admission exam of the National Univ. of San Marcos consists of 100 multiple choice questions (A, B, C, D, E) with a value of 20 points for each correct question and 1.125 for each wrong answer. We want to analyze what would happen if a student answers all 100 questions randomly and if there are chances of getting a vacancy knowing that minimum 900 points are needed to enter some career. What is the expected value of the points received by guessing a question? Solution puntaje_a_favor &lt;- 20 puntaje_en_contra &lt;- -1.125 prob_correcta &lt;- 1/5 prob_incorrecta &lt;- 1 - prob_correcta # Expected value of guessing a question: E &lt;- puntaje_a_favor * prob_correcta + puntaje_en_contra * prob_incorrecta E What is the expected value of guessing the 100 questions? Solution # Total questions: n &lt;- 100 # Expected value of the sum E_suma &lt;- n * E E_suma What is the standard error of guessing the 100 questions? Solution # Standard error of the sum SE_suma &lt;- sqrt(n)*abs(puntaje_a_favor - puntaje_en_contra) * sqrt(prob_correcta*prob_incorrecta) SE_suma Using the central limit theorem, What is the probability that a student obtains more than 900 points by marking all answers randomly? Solution puntaje_minimo &lt;- 900 # Probability of obtaining less than the minimum: prob &lt;- pnorm(puntaje_minimo, E_suma, SE_suma) # Probability of obtaining more than the minimum: 1 - prob This means that the probability that a student obtains the minimum score by guessing all the questions is: 0.0000000000014525. Conclusion: let’s study before taking the exam. It is not reasonable to take the exam randomly and mark randomly. Recall that e-n is the representation of \\(10^{-n}\\). Using Monte Carlo simulation for the 22,000 applicants who apply each time, calculate the probability that a student obtains more than 900 points by marking all answers randomly. Solution total &lt;- 22000 simulacion_admision &lt;- replicate(total, { puntaje_del_examen &lt;- sample(c(puntaje_a_favor, puntaje_en_contra), n, replace = TRUE, prob = c(prob_correcta, prob_incorrecta) ) sum(puntaje_del_examen) }) # Probability of obtaining more than 900 points: mean(simulacion_admision &gt; 900) # Histogram if we want to see the distribution of points obtained: hist(simulacion_admision) We see that the simulation gives us practically the same result. Practically there are no possibilities of entering UNMSM by guessing the answers. 9.4 Parameter Estimation Method So far, using Monte Carlo simulation we have built samples randomly, but knowing the probability of occurrence. However, we will not always know the proportion previously. If we have, for example, a population and we want to know how many have been infected by Covid-19, we cannot test everyone. Or if we have the total voters for an election, we cannot survey everyone to know who would win. Not only is it very expensive, but it would take us a lot of time. The parameter estimation method is the procedure used to know the characteristics of a population parameter, from the knowledge of a sample of \\(n\\) respondents We will analyze this following case. We have two political parties: Blue and Red. We do not know how much the total population is, nor the proportion that will vote for one or the other party. The only thing we can do is conduct voting intention polls. For example, these would be the results of the poll of a random sample of 10 people: Intuitively we know that we cannot deduce which party will win given that the sample is very small. To know which party will win we need to estimate as precisely as possible the parameter \\(p\\) that represents the proportion of voters of the Blue Party in the population and the parameter \\(1-p\\) that represents the proportion of voters of the Red party. Making some mathematical transformations to our theoretical estimates seen previously and defining \\(a=1\\) as value if they vote Blue and \\(b=0\\) if they do not vote for Blue, we can obtain the following theoretical estimates for this case: Expected value of each vote: \\(E[X]=p\\) We see that the value we expect to obtain per vote \\(E[X]\\) matches the parameter \\(p\\) that we are looking for. Expected value of the average of votes: \\(E[\\overline{X}]=p\\) If we do multiple surveys, all of \\(n\\) respondents and calculate the mean \\(\\overline{X}\\) of each sample, these samples will follow a normal distribution with an expected value \\(E[\\overline{X}]\\) which, as we see, matches the parameter \\(p\\) that we are estimating. Standard error of the average of votes: \\(SE[\\overline{X}]=\\sqrt{\\frac{p(1-p) }{n}}\\) In the same way as in the previous point, the multiple surveys generate multiple means on which the standard error \\(SE[\\overline{X}]\\) can be calculated which will depend on the size \\(n\\) of respondents. The expected value of the average \\(E[\\overline{X}]\\), formula 2, is theoretically equal to the parameter \\(p\\) that we are looking to estimate. However, without knowing how much \\(p\\) is we would have to have multiple samples of \\(n\\) respondents, then calculate the mean for each case \\(\\overline{X}\\) and finally calculate the average of these values. This is very expensive, so we will look for another way to estimate \\(E[\\overline{X}]\\). Given that we do not have so far how to estimate \\(E[\\overline{X}]\\), and given that we know that \\(E[\\overline{X}]=p\\) then we could give several values to \\(p\\) and see the impact on the standard error of the average that we know also depends on \\(p\\). Let’s generate, first, a sequence of parameter \\(p\\), from 0% to 100%, 100 different values: p &lt;- seq(0, 1, length=100) p #&gt; [1] 0.00000000 0.01010101 0.02020202 0.03030303 0.04040404 0.05050505 #&gt; [7] 0.06060606 0.07070707 0.08080808 0.09090909 0.10101010 0.11111111 #&gt; [13] 0.12121212 0.13131313 0.14141414 0.15151515 0.16161616 0.17171717 #&gt; [19] 0.18181818 0.19191919 0.20202020 0.21212121 0.22222222 0.23232323 #&gt; [25] 0.24242424 0.25252525 0.26262626 0.27272727 0.28282828 0.29292929 #&gt; [31] 0.30303030 0.31313131 0.32323232 0.33333333 0.34343434 0.35353535 #&gt; [37] 0.36363636 0.37373737 0.38383838 0.39393939 0.40404040 0.41414141 #&gt; [43] 0.42424242 0.43434343 0.44444444 0.45454545 0.46464646 0.47474747 #&gt; [49] 0.48484848 0.49494949 0.50505051 0.51515152 0.52525253 0.53535354 #&gt; [55] 0.54545455 0.55555556 0.56565657 0.57575758 0.58585859 0.59595960 #&gt; [61] 0.60606061 0.61616162 0.62626263 0.63636364 0.64646465 0.65656566 #&gt; [67] 0.66666667 0.67676768 0.68686869 0.69696970 0.70707071 0.71717172 #&gt; [73] 0.72727273 0.73737374 0.74747475 0.75757576 0.76767677 0.77777778 #&gt; [79] 0.78787879 0.79797980 0.80808081 0.81818182 0.82828283 0.83838384 #&gt; [85] 0.84848485 0.85858586 0.86868687 0.87878788 0.88888889 0.89898990 #&gt; [91] 0.90909091 0.91919192 0.92929293 0.93939394 0.94949495 0.95959596 #&gt; [97] 0.96969697 0.97979798 0.98989899 1.00000000 Thinking of 100 different values of \\(p\\) would be like thinking of 100 different elections where the Blue party and the red one have participation, like the election for mayors nationwide. In some districts the candidate of the Blue party loses with 0%, in others ties at 50% and in others wins clearly with 100% of the votes. Intuitively we know that if our real proportion was \\(p=80\\%\\) for the Blue party, that is that 8 out of every 10 will vote Blue, then it is very likely that in each survey we take we will find that in that district the Blue party has the majority of votes. This is predicted with the formula seen before and also includes the size of the survey \\(n\\) as part of the calculation: \\(SE[\\overline{X}]=\\sqrt{\\frac{p(1-p) }{n}}\\) That said, let’s return to our vector p that contains several values of parameter \\(p\\). On those values we can calculate what would happen if we survey groups of 20 people. Knowing the sample size we can calculate the standard error of the average for each of the values of \\(p\\): # Total people in each survey: n &lt;- 20 # Standard error of the average: SE_avg &lt;- sqrt(p*(1-p))/sqrt(n) SE_avg #&gt; [1] 0.00000000 0.02235954 0.03145942 0.03833064 0.04402928 0.04896646 #&gt; [7] 0.05335399 0.05731823 0.06094183 0.06428243 0.06738214 0.07027284 #&gt; [13] 0.07297936 0.07552152 0.07791540 0.08017428 0.08230929 0.08432982 #&gt; [19] 0.08624394 0.08805856 0.08977974 0.09141275 0.09296223 0.09443229 #&gt; [25] 0.09582660 0.09714840 0.09840064 0.09958592 0.10070661 0.10176486 #&gt; [31] 0.10276258 0.10370152 0.10458327 0.10540926 0.10618079 0.10689904 #&gt; [37] 0.10756509 0.10817988 0.10874431 0.10925913 0.10972506 0.11014270 #&gt; [43] 0.11051262 0.11083529 0.11111111 0.11134044 0.11152357 0.11166072 #&gt; [49] 0.11175205 0.11179770 0.11179770 0.11175205 0.11166072 0.11152357 #&gt; [55] 0.11134044 0.11111111 0.11083529 0.11051262 0.11014270 0.10972506 #&gt; [61] 0.10925913 0.10874431 0.10817988 0.10756509 0.10689904 0.10618079 #&gt; [67] 0.10540926 0.10458327 0.10370152 0.10276258 0.10176486 0.10070661 #&gt; [73] 0.09958592 0.09840064 0.09714840 0.09582660 0.09443229 0.09296223 #&gt; [79] 0.09141275 0.08977974 0.08805856 0.08624394 0.08432982 0.08230929 #&gt; [85] 0.08017428 0.07791540 0.07552152 0.07297936 0.07027284 0.06738214 #&gt; [91] 0.06428243 0.06094183 0.05731823 0.05335399 0.04896646 0.04402928 #&gt; [97] 0.03833064 0.03145942 0.02235954 0.00000000 Now let’s generate a scatter plot of both the different values of \\(p\\) and the standard errors for each \\(p\\). plot(p, SE_avg, ylim = c(0, 0.12)) Thus, we see how we can obtain different standard errors of the average for different values of \\(p\\). Intuitively we had the notion of what would happen given a \\(p=80\\%\\). Now in the graph we see it better. If the real intention of vote was 80% in that district then when taking several surveys and seeing the results of each survey we would obtain as expected value 80% and as standard error 8.8% or 0.088 as seen in the graph highlighted in blue: plot(p, SE_avg, ylim = c(0, 0.12)) coord_x &lt;- 0.8 # Value on X axis coord_y &lt;- max(SE_avg[p &gt;= coord_x]) # Value on Y axis for value x # Add vertical and horizontal reference line abline(h = coord_y, v = coord_x, col = &quot;blue&quot;, lty = 2) With these values of \\(E[\\overline{X}]=p=80\\%\\) and \\(SE[\\overline{X}]=8.8\\%\\) of standard error we can calculate a range of one standard error around \\(80\\%\\), which would go from \\(71.2\\%\\) to \\(88.8\\%\\) and then calculate what would be the probability that the mean \\(\\overline{X}\\) found in one of the surveys falls in this range. Visually it would be: In R, calculating the probability that a data point falls in the range of 1 standard error would be: # Calculation of probability that dat is between -1 and 1 standard error: pnorm(1) - pnorm(-1) #&gt; [1] 0.6826895 We can expand to have a greater range of 2 standard errors around \\(80\\%\\) and increase our probability: In R it would be: # Calculation of probability that dat is between -2 and 2 standard errors: pnorm(2) - pnorm(-2) #&gt; [1] 0.9544997 The probability increases to 95%, however how do we interpret this?. We haven’t even calculated the real value of the mean \\(\\overline{X}\\) of some survey. Simple, this means that, theoretically, there is a 95% probability that the mean \\(\\overline{X}\\) that we find in each survey is in the range of 62% to 98%, two standard errors around \\(80\\%\\). 95% of the time in the worst case, in a survey of 20 people, the Blue party would obtain 62% and in the best case 98%, so we could predict that the Blue party will win. Or not? Several things should make noise to us so far. First, the range so large, from 62% to 98%. Second, we have assumed a scenario: that the Blue voting intention was known and was 80%. That is, we have assumed \\(p=80\\%\\) which allowed us to calculate \\(E[\\overline{X}]=80\\%\\) and place that value at the center of the normal. However, \\(p\\) is unknown and is precisely what we are trying to estimate. If, on the contrary, the result was tighter, for example \\(p=55\\%\\), such a wide range would not serve us. Let’s see how it would be: plot(p, SE_avg, ylim = c(0, 0.12)) coord_x &lt;- 0.55 # Value on X axis coord_y &lt;- max(SE_avg[p &gt;= coord_x]) # Value on Y axis for value x # Add vertical and horizontal reference line abline(h = coord_y, v = coord_x, col = &quot;blue&quot;, lty = 2) If the real voting intention was \\(55\\%\\) we would have an expected value of the average \\(E[\\overline{X}]=p=55\\%\\) and a corresponding standard error of the average \\(SE[\\overline{X}]=11\\%\\). Again, by Central Limit Theorem we can calculate a range of two standard errors around \\(55\\%\\): The calculation of the probability of being in that range in R would be the same because we continue in the range of 2 standard errors. Therefore the probability would be the same. # Calculation of probability that dat is between -2 and 2 standard errors: pnorm(2) - pnorm(-2) #&gt; [1] 0.9544997 However, what does change is the range. Now the range goes from 32.8% to 77.2%, two standard errors around the expected value of the average \\(E[\\overline{X}]\\). Although the probability is still 95%, that does not help us at all this time because there is 95% that what we find in our sample is a value between 33% and 77%. Some survey samples will give us 33% of votes for Blue and other samples 77%. And the problem lies in the number of samples taken \\(n\\). If we see again the formula we see how \\(n\\) influences the result. \\(SE[\\overline{X}]=\\sqrt{\\frac{p(1-p) }{n}}\\) Let’s increment then our number of respondents to 500: n &lt;- 500 p &lt;- seq(0, 1, length = 100) SE_avg &lt;- sqrt(p*(1-p))/sqrt(n) plot(p, SE_avg, ylim = c(0, 0.12)) This sample gives us smaller standard errors. For example, if the real proportion of voters of the Blue party was \\(p=55\\%\\) we would have \\(E[\\overline{X}]=p=55\\%\\) and a \\(SE[\\overline{X}]=2.2\\%\\): plot(p, SE_avg, ylim = c(0, 0.12)) coord_x &lt;- 0.55 # Value on X axis coord_y &lt;- max(SE_avg[p &gt;= coord_x]) # Value on Y axis for value x # Add vertical and horizontal reference line abline(h = coord_y, v = coord_x, col = &quot;blue&quot;, lty = 2) If we now calculate a range of two standard errors around \\(55\\%\\) we would have a range that goes from \\(50.6\\%\\) to \\(59.4\\%\\). Again, interpretation is that the mean that we find in our random survey has a 95% probability of being in that range. We see then that this theoretical prediction, the standard error, becomes smaller as the sample size \\(n\\) increases and in turn depends on the probability of the population \\(p\\) that we do not know. Moreover, with a real value of \\(p=0.5\\), (50%), we have the maximum value of the standard error that we can obtain. Thus, if we correct \\(p\\) at 50%, which would be the extreme of cases, a tie, we can calculate how the value of the standard error of the average changes according to the sample size: p &lt;- 0.5 n &lt;- seq(20, 5000, 20) # number of surveys increases by 20. SE_avg &lt;- sqrt(p*(1-p)/n) plot(n, SE_avg) abline(h = 0.015, v = 1000, col = &quot;blue&quot;, lty = 2) # Add vertical and horizontal reference line A sample of 1,000 people, for example, generates us a maximum standard error of 0.015 or 1.5%. 9.4.1 Margin of Error As we have already seen, we could consider a range of 1 standard error or 2 standard errors around \\(E[\\overline{X}]\\) and calculate the probability that our sample mean \\(\\overline{X}\\) is in that range. Or what is mathematically the same, we could say that if we build a range of 1 or 2 standard errors around our sample mean \\(\\overline{X}\\) there is a determined probability that in that range is included the expected value \\(E[\\overline{X}]\\) which is, by formula equal to \\(p\\), the value we want to estimate. It is crucial then to calculate the standard error of the average \\(SE[\\overline{X}]\\), but we see ourselves limited because it depends on \\(p\\). There is another way to calculate \\(SE[\\overline{X}]\\) without using \\(p\\) and is known as the standard error of estimation \\(\\hat{SE}[\\overline{X}]\\). For this we will use the following formula: \\(\\hat{SE}[\\overline{X}]=\\sqrt{\\frac{\\overline{X}(1-\\overline{X})}{n}}\\) Where, as we already know, \\(\\overline{X}\\) is the mean of our sample or sample mean. For our example case, it is the percentage that the Blue party obtained in the survey we conducted. Now that we have the sample mean \\(\\overline{X}\\) and we can already calculate the standard error of estimation \\(\\hat{SE}[\\overline{X}]\\) we can start building ranges around \\(\\overline{X}\\) that increase the probability of finding \\(p\\). To make communication simpler, we will use by convention the notation margin of error to indicate that we are going to take a range of 2 standard errors of estimation. For example, we have a sample of 1100 people and after reviewing the survey results we have a sample mean of \\(\\overline{X}=56\\%\\) for the Blue party. With this we can estimate the standard error of estimation \\(\\hat{SE}[\\overline{X}]\\) with the formula we just described and finally calculate the margin of error. # Total respondents total &lt;- 1100 # Survey results, 56% indicated Blue: X_avg &lt;- 0.56 # Standard error estimation SE_est &lt;- sqrt(X_avg * (1 - X_avg)/total) SE_est #&gt; [1] 0.01496663 # Margin of error, MoE MoE &lt;- 2 * SE_est MoE #&gt; [1] 0.02993326 With this we would have that from a sample of 1100 people, we have estimated 56% voting intention for the Blue party with a margin of error of \\(+- 2.99\\%\\). Finally, let’s see examples of the different surveys conducted in April and early May 2020 to measure voting intentions in the US. We see as columns: Poll: Surveying company that conducted the study (Encuesta) Date: Date of the survey (Fecha) Sample: Number of respondents varying by pollster (Muestra) MoE: Margin of error Candidates: Candidates for the presidency. Their percentages do not sum 100% because blanks and nulls are omitted. Spread: This is the estimation of by how much one candidate beats the other (Propagación). If, on the other hand, we ask ourselves why larger surveys are not done, for example 50,000 people, the reason is that: The cost of surveying 50 thousand people is very expensive. Parameter estimation is a theoretical prediction, if we give a very small margin of error our results could be misinterpreted as absolute truths when we know that: 2.1 Every person can change their opinion and we are only taking a snapshot of the moment. 2.2 No matter how much effort is put in, no survey is totally random or they are not done in rural areas. 2.3 We could survey people who say they will go to vote, but finally they will not do so, etc. 9.4.2 Confidence Intervals Confidence intervals are a very useful concept widely used by Data Scientists. However, it is nothing more than another way of expressing what we have already learned so far. And it is that a confidence interval of 95% tells us that there is a 95% probability that the interval we generate includes the parameter \\(p\\) that we want to estimate. This is nothing more than another way of indicating that we have to build an interval considering the margin of error, that is two standard errors around our sample mean. For the sample of 1100 people we saw in the previous section, we reported an estimate of 56% with a margin of error of \\(+- 2.99\\%\\). If we now want to use confidence intervals in our language we would say: We estimate 56% for the Blue party with a confidence interval of 95%. This confidence interval goes from 53% to 59%. 9.5 Spread Estimation Although we are interested in estimating the proportion that the Blue party would obtain \\(p\\), sometimes it is more useful to know the difference (by how much it wins/loses). For example, when we have two parties in the second round of elections not only do we have votes for Blue and Red, but also blank/spoiled. Also, in regular elections we have more than one pollster doing several surveys. So one could give 45% for Blue, 41% for Red. While another can give 41% for Blue and 38% for Red, etc. If we compare surveys, rather than knowing the exact percentage it is more useful to know by how much the blue party wins, since if we see that in all, for example it wins by 4%, with a tiny standard error, then \\(p\\) would not matter much. Only with the difference data we could take get an idea of who will win. This difference is called spread. We had defined that the voting intention for the Blue party was \\(p\\) and for the red party \\(1-p\\). So what we would expect to obtain for the difference would be \\(p - (1-p)\\), that is \\(2p - 1\\). Standard error of the spread: \\(SE[spread]=2\\sqrt{\\frac{p(1-p) }{n}}\\) We see that the standard error is twice the standard error of the average, which depends on \\(p\\), and we have already found previously an estimation to not depend on \\(p\\) but on the mean of our sample. So we will use: \\(\\hat{SE[spread]}=2\\sqrt{\\frac{\\overline{X}(1-\\overline{X}) }{n}}\\) Let’s see with an example these concepts. Let’s study the 2016 US elections. In this case we have multiple pollsters, conducting multiple surveys months prior to elections, and mainly two parties competing for president. We are going to use the polls_us_election_2016 data frame included in the dslabs library which includes data from multiple surveys conducted for the 2016 US elections between Hillary Clinton and Donald Trump. The first thing we will do is explore the data: library(dslabs) head(polls_us_election_2016) As we see, we do not have the standard error, nor the confidence interval. So we will proceed to make some mutations applying the formulas learned so far focusing on the voting intention for Hillary Clinton. surveys &lt;- polls_us_election_2016 |&gt; filter(state == &quot;U.S.&quot;) |&gt; mutate(X_avg = rawpoll_clinton/100) |&gt; mutate(SE_prom = sqrt((X_avg*(1-X_avg))/samplesize)) |&gt; mutate(inferior = X_avg - 2*SE_prom, superior = X_avg + 2*SE_prom) |&gt; select(pollster, enddate, X_avg, SE_prom, inferior, superior) # First 5 rows surveys |&gt; head(5) #&gt; pollster enddate X_avg SE_prom inferior superior #&gt; 1 ABC News/Washington Post 2016-11-06 0.4700 0.010592790 0.4488144 0.4911856 #&gt; 2 Google Consumer Surveys 2016-11-07 0.3803 0.002978005 0.3743440 0.3862560 #&gt; 3 Ipsos 2016-11-06 0.4200 0.010534681 0.3989306 0.4410694 #&gt; 4 YouGov 2016-11-07 0.4500 0.008204286 0.4335914 0.4664086 #&gt; 5 Gravis Marketing 2016-11-06 0.4700 0.003869218 0.4622616 0.4777384 For example, IPSOS in a survey published on 11/06/16 estimated 42% voting intention for Clinton with a 95% confidence interval in a range going from 39.89% to 44.10%. Does this data mean that they estimated she would lose? No, given that in this case we are using real data the proportion of votes for Clinton with those for Trump will not sum 100%. In fact, on actual election day Clinton obtained 48.2% and Trump 46.1% of total votes cast. That is real \\(p\\) was 48.2%. What we could calculate is how many of these pollsters guessed right in their estimation. That is, if in their confidence intervals is the \\(p=48.2\\%\\) that Clinton finally obtained. To do this, we will add a column guessed_right (guessed_right) with the validation of whether it is in the confidence interval and then use summarize() to calculate the percentage of surveys that guessed right. surveys |&gt; mutate(guessed_right = inferior &lt;= 0.482 &amp; 0.482 &lt;= superior) |&gt; summarize(mean(guessed_right)) #&gt; mean(guessed_right) #&gt; 1 0.2802893 Only 28% of published surveys published confidence intervals that included \\(p\\). This, among many other reasons, because at the beginning there are many more undecided who finally decide in the last weeks. Let’s analyze now how many guessed right in the spread. It could be that even though they did not estimate exact \\(p\\) the difference did remain over time. To do this, let’s add to our surveys the column spread with the difference of votes: surveys_spread &lt;- polls_us_election_2016 |&gt; filter(state == &quot;U.S.&quot;) |&gt; mutate(spread = (rawpoll_clinton - rawpoll_trump)/100) And now we are going to do a trick calculating the spread of our sample: \\(spread=2*\\overline{X}-1\\) We can transform this formula: \\(spread-1=2*\\overline{X}\\) \\(\\frac{spread-1}{2}=\\overline{X}\\) Or what is the same: \\(\\overline{X}=\\frac{spread-1}{2}\\) This formula gives us an approximation of how much \\(\\overline{X}\\) would be transformed to a scale of 0 to 100%. With it, let’s calculate the standard error and the confidence interval: surveys_spread &lt;- surveys_spread |&gt; mutate(X_avg = (spread + 1)/2) |&gt; mutate(SE_spread = 2*sqrt((X_avg*(1-X_avg))/samplesize)) |&gt; mutate(inferior = spread - 2*SE_spread, superior = spread + 2*SE_spread) |&gt; select(pollster, enddate, spread, SE_spread, inferior, superior) # First 5 rows surveys_spread |&gt; head(5) #&gt; pollster enddate spread SE_spread inferior #&gt; 1 ABC News/Washington Post 2016-11-06 0.0400 0.021206832 -0.002413664 #&gt; 2 Google Consumer Surveys 2016-11-07 0.0234 0.006132712 0.011134575 #&gt; 3 Ipsos 2016-11-06 0.0300 0.021334733 -0.012669466 #&gt; 4 YouGov 2016-11-07 0.0400 0.016478037 0.007043926 #&gt; 5 Gravis Marketing 2016-11-06 0.0400 0.007746199 0.024507601 #&gt; superior #&gt; 1 0.08241366 #&gt; 2 0.03566542 #&gt; 3 0.07266947 #&gt; 4 0.07295607 #&gt; 5 0.05549240 Now let’s calculate how many of these pollsters guessed right in their estimation. That is, if in their confidence intervals is the real value of \\(spread=48.2\\%-46.1\\%=2.1\\%\\) that Clinton finally obtained spread. To do this, we will add the column guessed_right and then summarize(). surveys_spread |&gt; mutate(guessed_right = inferior &lt;= 0.021 &amp; 0.021 &lt;= superior) |&gt; summarize(mean(guessed_right)) #&gt; mean(guessed_right) #&gt; 1 0.6735986 In this case we see how 67.3% of the time, surveys correctly estimated the difference in votes favorable to Clinton. As a clarification, final reminder of this case, even though Clinton obtained more votes she did not win the elections because the US system is different and not necessarily if you win in votes you obtain the presidency. 9.6 Estimates Outside Election Polls We have used election polls to understand statistical inference concepts. However, most Data Scientists are not related to voting intention estimation calculations. That does not mean we will not use those concepts. The central limit theorem not only works in election polls. What it means is that we will use some slightly different formulas that apply to more daily life cases. From what we have learned so far the main change is the formula to calculate the standard error. We will use instead the standard deviation \\(\\sigma\\) of the sample to calculate the standard error: \\(SE[\\overline{X}]=\\frac{\\sigma}{\\sqrt{n}}\\) Where \\(\\overline{X}\\) is the average of our random sample and \\(n\\) is the sample size. 9.7 Exercises The most common data a Data Scientist manages comes from people, some attribute/characteristic of them. In these exercises we are going to use the heights data frame that we already used for other purposes in previous chapters. library(dslabs) data(heights) Create the vector x to extract the heights data of each person. Then report the average and standard deviation of our population. Remember to convert height to meters Solution x &lt;- heights |&gt; filter(sex == &quot;Male&quot;) |&gt; mutate(height_m = height/39.37) |&gt; pull(height_m) # Average of the population mean(x) # Standard deviation sd(x) Mathematically we use x in lowercase to refer to our total population and X to refer to a random sample. We will denote the population mean as \\(\\mu\\) and the population standard deviation as \\(\\sigma\\) Most of the time we will not have access to the mean and standard deviation of the population because it is very large and highly expensive. Let’s assume we do not have access to the population. We can only obtain a random sample of 100 people. Create a random sample with replacement and store these values in the vector called X. With the data from your sample, build a 95% confidence interval to estimate the average of the population. Solution N &lt;- 100 X &lt;- sample(x, N, replace = TRUE) # Expected value mean(X) # Standard error se &lt;- sd(X)/sqrt(N) # 95% confidence interval is approx. 2 se ic &lt;- c(mean(X) - 2*se, mean(X) + 2*se) Using Monte Carlo simulation, repeat this sampling about 10 thousand times and validate the percentage of times that the confidence interval you generate from your sample includes the true population value. Solution promedio_real &lt;- mean(x) N &lt;- 100 num_veces &lt;- 10000 simulacion &lt;- replicate(num_veces, { X &lt;- sample(x, N, replace = TRUE) se &lt;- sd(X)/sqrt(N) inferior &lt;- mean(X) - 2*se superior &lt;- mean(X) + 2*se between(promedio_real, inferior, superior) }) mean(simulacion) "],["introduction-1.html", "Introduction", " Introduction A frequent error in Data Science projects is thinking that they start with analysis. In fact, when a data analyst is asked where they spend most of their time, the answer remains the same: 80% in Data Wrangling9. Data in its natural form (Raw Data), usually contain registration errors that make exact analysis impossible. Being recorded by different systems and people, it is normal to end up with a file in which the same value is expressed in different ways (for example, a date can be recorded as June 28, or as 28/06), there may be blank records, and of course, grammatical errors. When analyzing this data, all these records have to be pre-processed. That is, the data must be cleaned, unified, consolidated, and normalized so that it can be used to extract valuable information. Data Wrangling is the process of preparing data to be leveraged. In the following chapters, we will see several common steps of the Data Wrangling process such as Importing data into R from files, converting data to tidy type, string processing, html processing, date and time formatting, and text mining. https://www.infoworld.com/article/3228245/the-80-20-data-science-dilemma.html↩︎ "],["data-import-and-consolidation.html", "Chapter 10 Data import and consolidation 10.1 Importing from files 10.2 Tidy data 10.3 Exercises 10.4 Joining tables 10.5 Web Scraping 10.6 Exercises", " Chapter 10 Data import and consolidation 10.1 Importing from files For importing files, whether they are text types or spreadsheets, we need to know where we are going to import the data from. 10.1.1 Working Directory By default, when we import files, R will search in the working directory. To find out the path of our working directory we will use the getwd() function. getwd() #&gt; [1] &quot;c:/Documents/dparedesi/git-repository/DS&quot; This is the path where we can place our files to load them. If we want to load data from another folder we can change the working directory using setwd(). setwd(&quot;c:/Documents/Proyectos/Archivos para R&quot;) getwd() #&gt; [1] &quot;c:/Documents/Proyectos/Archivos para R&quot; For practical purposes, we are going to use a file already available in one of the previously installed packages, dslabs, when we analyzed the danger level to decide which US state to travel to. To do this, we can use the system.file() function and determine the path where the dslabs package was installed. dslabs_path &lt;- system.file(package=&quot;dslabs&quot;) Likewise, we can list the files and folders within that path using the list.files() function. dslabs_path &lt;- system.file(package=&quot;dslabs&quot;) list.files(dslabs_path) #&gt; [1] &quot;data&quot; &quot;DESCRIPTION&quot; &quot;extdata&quot; &quot;help&quot; &quot;html&quot; #&gt; [6] &quot;INDEX&quot; &quot;Meta&quot; &quot;NAMESPACE&quot; &quot;R&quot; &quot;script&quot; The folder we will use is extdata. We can access the path of this folder if we modify the parameters of the system.file() function. dslabs_path &lt;- system.file(&quot;extdata&quot;, package=&quot;dslabs&quot;) list.files(dslabs_path) #&gt; [1] &quot;2010_bigfive_regents.xls&quot; #&gt; [2] &quot;calificaciones.csv&quot; #&gt; [3] &quot;carbon_emissions.csv&quot; #&gt; [4] &quot;fertility-two-countries-example.csv&quot; #&gt; [5] &quot;HRlist2.txt&quot; #&gt; [6] &quot;life-expectancy-and-fertility-two-countries-example.csv&quot; #&gt; [7] &quot;murders.csv&quot; #&gt; [8] &quot;olive.csv&quot; #&gt; [9] &quot;RD-Mortality-Report_2015-18-180531.pdf&quot; #&gt; [10] &quot;ssa-death-probability.csv&quot; The file we will use is murders.csv. To build the complete path of this file we can concatenate the strings or we can also directly use the file.path(path, file_name) function. csv_example_path &lt;- file.path(dslabs_path, &quot;murders.csv&quot;) Finally, we will copy the file to our working directory with the file.copy(source_path, destination_path) function. file.copy(csv_example_path, getwd()) #&gt; [1] TRUE We can validate the copy with the file.exists(file_name) function. file.exists(&quot;murders.csv&quot;) #&gt; [1] TRUE It is recommended to check the documentation of the file manipulation functions. ?files 10.1.2 readr and readxl packages Now that we have the file in our working directory, we will use functions within the readr and readxl packages to import files into R. Both are included in the tidyverse package that we previously installed. library(tidyverse) # Here readr is included automatically library(readxl) The functions we will use the most will be read_csv() and read_excel(). The latter supports .xls and .xlsx extensions. data_df &lt;- read_csv(&quot;murders.csv&quot;) #&gt; Rows: 51 Columns: 5 #&gt; ── Column specification ───────────────────────────── #&gt; Delimiter: &quot;,&quot; #&gt; chr (3): state, abb, region #&gt; dbl (2): population, total #&gt; #&gt; ℹ Use `spec()` to retrieve the full column specification for this data. #&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Once imported we can remove the file if we wish file.remove(&quot;murders.csv&quot;) #&gt; [1] TRUE We see how by default it detects the headers in the first row and assigns them a default data type. Let’s now explore our data_df object. data_df #&gt; # A tibble: 51 × 5 #&gt; state abb region population total #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Alabama AL South 4779736 135 #&gt; 2 Alaska AK West 710231 19 #&gt; 3 Arizona AZ West 6392017 232 #&gt; 4 Arkansas AR South 2915918 93 #&gt; 5 California CA West 37253956 1257 #&gt; 6 Colorado CO West 5029196 65 #&gt; 7 Connecticut CT Northeast 3574097 97 #&gt; 8 Delaware DE South 897934 38 #&gt; 9 District of Columbia DC South 601723 99 #&gt; 10 Florida FL South 19687653 669 #&gt; # ℹ 41 more rows The first thing it indicates is that the object is of type tibble. This object is very similar to a data frame, but with improved features such as, for example, the number of rows and columns in the console, the data type under the header, the default report of only the first 10 records automatically, among many others that we will discover in this chapter. The same syntax and logic would apply for importing an excel file. In this case we are importing directly from the package path and not from our working directory. excel_example_path &lt;- file.path(dslabs_path, &quot;2010_bigfive_regents.xls&quot;) data_df_de_excel &lt;- read_excel(excel_example_path) readr gives us 7 different types of functions for importing flat files: TABLE 10.1: readr functions for importing files Function Usage read_csv() Comma separated files read_tsv() Tab separated files read_delim() General delimited files read_fwf() Fixed width files read_table() Tabular files where columns are separated by white space read_log() Web log files 10.1.3 Importing files from the internet We have seen how we can enter the full path to load a file directly from another source different from our working directory. In the same way, if we have a file in an internet path we can pass it directly to R since read_csv() and the other readr import functions support URL input as a parameter. Here we see the import of grades from students of the Data Science with R course. ##### Example 1: # Historical grades data url &lt;- &quot;https://dparedesi.github.io/DS-con-R/grades-estudiantes.csv&quot; grades &lt;- read_csv(url) grades &lt;- grades |&gt; mutate(total = (P1 + P2 + P3 + P4 + P5 + P6)/30*20) grades |&gt; select(P1, P2, P3, P4, P5, total) |&gt; summary() From this we could visualize a histogram: hist(grades$total) Or we could compare between genders which one has the highest median: grades |&gt; ggplot() + aes(gender, total) + geom_boxplot() We could also extract updated Covid-19 information. ##### Example 2: # Covid-19 Data url &lt;- &quot;https://covid.ourworldindata.org/data/owid-covid-data.csv&quot; internet_data &lt;- read_csv(url) internet_data |&gt; arrange(desc(date)) |&gt; head(10) 10.2 Tidy data Ordered data or tidy data are those obtained from a process called data tidying. It is one of the important cleaning processes during processing of large data or ‘big data’ and is a very used step in Data Science. The main characteristics are that each different observation of that variable has to be in a different row and that each variable you measure has to be in a column (Leek 2015). As we may have noticed, we have been using tidy data since the first chapters. However, not all our data comes ordered. Most of it comes in what we call wide data or wide data. For example, we have previously used data from Gapminder. Let’s filter the data from Germany and South Korea to remember how we had our data. gapminder |&gt; filter(country %in% c(&quot;South Korea&quot;, &quot;Germany&quot;)) |&gt; head(10) #&gt; country year infant_mortality life_expectancy fertility population #&gt; 1 Germany 1960 34.0 69.26 2.41 73179665 #&gt; 2 South Korea 1960 80.2 53.02 6.16 25074028 #&gt; 3 Germany 1961 NA 69.85 2.44 73686490 #&gt; 4 South Korea 1961 76.1 53.75 5.99 25808542 #&gt; 5 Germany 1962 NA 70.01 2.47 74238494 #&gt; 6 South Korea 1962 72.4 54.51 5.79 26495107 #&gt; 7 Germany 1963 NA 70.10 2.49 74820389 #&gt; 8 South Korea 1963 68.8 55.27 5.57 27143075 #&gt; 9 Germany 1964 NA 70.66 2.49 75410766 #&gt; 10 South Korea 1964 65.3 56.04 5.36 27770874 #&gt; gdp continent region #&gt; 1 NA Europe Western Europe #&gt; 2 28928298962 Asia Eastern Asia #&gt; 3 NA Europe Western Europe #&gt; 4 30356298714 Asia Eastern Asia #&gt; 5 NA Europe Western Europe #&gt; 6 31102566019 Asia Eastern Asia #&gt; 7 NA Europe Western Europe #&gt; 8 34067175844 Asia Eastern Asia #&gt; 9 NA Europe Western Europe #&gt; 10 36643076469 Asia Eastern Asia We notice that each row is an observation and each column represents a variable. It is ordered data, tidy data. On the other hand, we can see what the data was like for these two countries if we access the source in the dslabs package. fertility_path &lt;- file.path(dslabs_path, &quot;fertility-two-countries-example.csv&quot;) wide_data &lt;- read_csv(fertility_path) #&gt; Rows: 2 Columns: 57 #&gt; ── Column specification ───────────────────────────── #&gt; Delimiter: &quot;,&quot; #&gt; chr (1): country #&gt; dbl (56): 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, ... #&gt; #&gt; ℹ Use `spec()` to retrieve the full column specification for this data. #&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. wide_data #&gt; # A tibble: 2 × 57 #&gt; country `1960` `1961` `1962` `1963` `1964` `1965` `1966` `1967` `1968` `1969` #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Germany 2.41 2.44 2.47 2.49 2.49 2.48 2.44 2.37 2.28 2.17 #&gt; 2 South K… 6.16 5.99 5.79 5.57 5.36 5.16 4.99 4.85 4.73 4.62 #&gt; # ℹ 46 more variables: `1970` &lt;dbl&gt;, `1971` &lt;dbl&gt;, `1972` &lt;dbl&gt;, `1973` &lt;dbl&gt;, #&gt; # `1974` &lt;dbl&gt;, `1975` &lt;dbl&gt;, `1976` &lt;dbl&gt;, `1977` &lt;dbl&gt;, `1978` &lt;dbl&gt;, #&gt; # `1979` &lt;dbl&gt;, `1980` &lt;dbl&gt;, `1981` &lt;dbl&gt;, `1982` &lt;dbl&gt;, `1983` &lt;dbl&gt;, #&gt; # `1984` &lt;dbl&gt;, `1985` &lt;dbl&gt;, `1986` &lt;dbl&gt;, `1987` &lt;dbl&gt;, `1988` &lt;dbl&gt;, #&gt; # `1989` &lt;dbl&gt;, `1990` &lt;dbl&gt;, `1991` &lt;dbl&gt;, `1992` &lt;dbl&gt;, `1993` &lt;dbl&gt;, #&gt; # `1994` &lt;dbl&gt;, `1995` &lt;dbl&gt;, `1996` &lt;dbl&gt;, `1997` &lt;dbl&gt;, `1998` &lt;dbl&gt;, #&gt; # `1999` &lt;dbl&gt;, `2000` &lt;dbl&gt;, `2001` &lt;dbl&gt;, `2002` &lt;dbl&gt;, `2003` &lt;dbl&gt;, … We see that the original data had two rows, one per country and then each column represented a year. This is what we call wide data or wide data. Normally we will have wide data that we first have to convert to tidy data to later be able to perform our analyses. 10.2.1 Transforming to tidy data The tidyverse library provides two functions to reshape data between wide and long (tidy) formats. We use pivot_longer() to convert from wide data to tidy data and pivot_wider() to convert from tidy data to wide data. 10.2.1.1 pivot_longer function Let’s see the utility with the wide_data object that we created in the previous section as a result of importing data from the csv. First apply the pivot_longer() function to explore the conversion that is performed by default. tidy_data &lt;- wide_data |&gt; pivot_longer(cols = -country, names_to = &quot;key&quot;, values_to = &quot;value&quot;) tidy_data #&gt; # A tibble: 112 × 3 #&gt; country key value #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Germany 1960 2.41 #&gt; 2 Germany 1961 2.44 #&gt; 3 Germany 1962 2.47 #&gt; 4 Germany 1963 2.49 #&gt; 5 Germany 1964 2.49 #&gt; 6 Germany 1965 2.48 #&gt; 7 Germany 1966 2.44 #&gt; 8 Germany 1967 2.37 #&gt; 9 Germany 1968 2.28 #&gt; 10 Germany 1969 2.17 #&gt; # ℹ 102 more rows We see how the pivot_longer() function has collected the columns into two, the names column (“key”) and the values column (“value”). We can change the title of these new columns, for example “year” and “fertility”. tidy_data &lt;- wide_data |&gt; pivot_longer(cols = -country, names_to = &quot;year&quot;, values_to = &quot;fertility&quot;) tidy_data #&gt; # A tibble: 112 × 3 #&gt; country year fertility #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Germany 1960 2.41 #&gt; 2 Germany 1961 2.44 #&gt; 3 Germany 1962 2.47 #&gt; 4 Germany 1963 2.49 #&gt; 5 Germany 1964 2.49 #&gt; 6 Germany 1965 2.48 #&gt; 7 Germany 1966 2.44 #&gt; 8 Germany 1967 2.37 #&gt; 9 Germany 1968 2.28 #&gt; 10 Germany 1969 2.17 #&gt; # ℹ 102 more rows We use cols = -country to exclude the country column from being pivoted. By default the column names are collected as text. To convert them to numbers we use the names_transform argument. tidy_data &lt;- wide_data |&gt; pivot_longer(cols = -country, names_to = &quot;year&quot;, values_to = &quot;fertility&quot;, names_transform = list(year = as.integer)) tidy_data #&gt; # A tibble: 112 × 3 #&gt; country year fertility #&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Germany 1960 2.41 #&gt; 2 Germany 1961 2.44 #&gt; 3 Germany 1962 2.47 #&gt; 4 Germany 1963 2.49 #&gt; 5 Germany 1964 2.49 #&gt; 6 Germany 1965 2.48 #&gt; 7 Germany 1966 2.44 #&gt; 8 Germany 1967 2.37 #&gt; 9 Germany 1968 2.28 #&gt; 10 Germany 1969 2.17 #&gt; # ℹ 102 more rows This data would now be ready to create graphs using ggplot(). tidy_data |&gt; ggplot() + aes(year, fertility, color = country) + geom_point() 10.2.1.2 pivot_wider function Sometimes, as we will see in the following section, it will be useful to go back from rows to columns. For this we will use the pivot_wider() function, where we specify names_from (the column containing the new column names) and values_from (the column containing the values). Additionally, we can use the : operator to indicate from which column to which column we want to select. tidy_data |&gt; pivot_wider(names_from = year, values_from = fertility) |&gt; select(country, `1965`:`1970`) #&gt; # A tibble: 2 × 7 #&gt; country `1965` `1966` `1967` `1968` `1969` `1970` #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Germany 2.48 2.44 2.37 2.28 2.17 2.04 #&gt; 2 South Korea 5.16 4.99 4.85 4.73 4.62 4.53 10.2.2 separate function In the cases described above we had a situation with relatively ordered data. We only had to do a collection transformation and converted to tidy data. However, the data is not always stored in such an easily interpretable way. Sometimes we have data like this: ruta &lt;- file.path(dslabs_path, &quot;life-expectancy-and-fertility-two-countries-example.csv&quot;) data &lt;- read_csv(ruta) #&gt; Rows: 2 Columns: 113 #&gt; ── Column specification ───────────────────────────── #&gt; Delimiter: &quot;,&quot; #&gt; chr (1): country #&gt; dbl (112): 1960_fertility, 1960_life_expectancy, 1961_fertility, 1961_life_e... #&gt; #&gt; ℹ Use `spec()` to retrieve the full column specification for this data. #&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. data |&gt; select(1:5) #Report first 5 columns #&gt; # A tibble: 2 × 5 #&gt; country `1960_fertility` `1960_life_expectancy` `1961_fertility` #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Germany 2.41 69.3 2.44 #&gt; 2 South Korea 6.16 53.0 5.99 #&gt; # ℹ 1 more variable: `1961_life_expectancy` &lt;dbl&gt; If we apply pivot_longer() directly we would not have our data ordered yet. Let’s see: data |&gt; pivot_longer(cols = -country, names_to = &quot;key_col&quot;, values_to = &quot;value_col&quot;) #&gt; # A tibble: 224 × 3 #&gt; country key_col value_col #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Germany 1960_fertility 2.41 #&gt; 2 Germany 1960_life_expectancy 69.3 #&gt; 3 Germany 1961_fertility 2.44 #&gt; 4 Germany 1961_life_expectancy 69.8 #&gt; 5 Germany 1962_fertility 2.47 #&gt; 6 Germany 1962_life_expectancy 70.0 #&gt; 7 Germany 1963_fertility 2.49 #&gt; 8 Germany 1963_life_expectancy 70.1 #&gt; 9 Germany 1964_fertility 2.49 #&gt; 10 Germany 1964_life_expectancy 70.7 #&gt; # ℹ 214 more rows We will use the separate() function to separate a column into multiple columns using a specific separator. In this case our separator would be the character _. Also, we will add the attribute extra=\"merge\" to indicate that if there is more than one separator character, do not separate them and keep them joined. data |&gt; pivot_longer(cols = -country, names_to = &quot;key_col&quot;, values_to = &quot;value_col&quot;) |&gt; separate(key_col, c(&quot;año&quot;, &quot;other_var&quot;), sep=&quot;_&quot;, extra = &quot;merge&quot;) #&gt; # A tibble: 224 × 4 #&gt; country año other_var value_col #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Germany 1960 fertility 2.41 #&gt; 2 Germany 1960 life_expectancy 69.3 #&gt; 3 Germany 1961 fertility 2.44 #&gt; 4 Germany 1961 life_expectancy 69.8 #&gt; 5 Germany 1962 fertility 2.47 #&gt; 6 Germany 1962 life_expectancy 70.0 #&gt; 7 Germany 1963 fertility 2.49 #&gt; 8 Germany 1963 life_expectancy 70.1 #&gt; 9 Germany 1964 fertility 2.49 #&gt; 10 Germany 1964 life_expectancy 70.7 #&gt; # ℹ 214 more rows We already have the year separated, but this data is still not tidy data since there is a row for fertility and a row for life expectancy for each country. We have to pass these values from row to columns. And for that we already learned that we can use the pivot_wider() function data |&gt; pivot_longer(cols = -country, names_to = &quot;key_col&quot;, values_to = &quot;value_col&quot;) |&gt; separate(key_col, c(&quot;año&quot;, &quot;other_var&quot;), sep=&quot;_&quot;, extra = &quot;merge&quot;) |&gt; pivot_wider(names_from = other_var, values_from = value_col) #&gt; # A tibble: 112 × 4 #&gt; country año fertility life_expectancy #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Germany 1960 2.41 69.3 #&gt; 2 Germany 1961 2.44 69.8 #&gt; 3 Germany 1962 2.47 70.0 #&gt; 4 Germany 1963 2.49 70.1 #&gt; 5 Germany 1964 2.49 70.7 #&gt; 6 Germany 1965 2.48 70.6 #&gt; 7 Germany 1966 2.44 70.8 #&gt; 8 Germany 1967 2.37 71.0 #&gt; 9 Germany 1968 2.28 70.6 #&gt; 10 Germany 1969 2.17 70.5 #&gt; # ℹ 102 more rows In other cases, instead of separating a column we will want to join them. In future cases we will see how the unite(column_1, column2) function can also be useful. 10.3 Exercises Explore the following file: https://dparedesi.github.io/DS-con-R/uber_peru_2010.csv and import it into the uber_peru_2010 object. Solution url &lt;- &quot;https://dparedesi.github.io/DS-con-R/uber_peru_2010.csv&quot; # We will use read_csv since it is separated by commas uber_peru_2010 &lt;- read_csv(url) # Upon importing it we realize it is separated by &quot;;&quot; uber_peru_2010 |&gt; head() # Therefore we import again using read_delim uber_peru_2010 &lt;- read_delim(&quot;external/uber_peru_2010.csv&quot;, delim = &quot;;&quot;, col_types = cols(.default = &quot;c&quot;) ) uber_peru_2010 |&gt; head() Explore the following file: https://www.datosabiertos.gob.pe/sites/default/files/DATASET_SINADEF.csv and import it into the deaths object. Solution url &lt;- &quot;https://www.datosabiertos.gob.pe/sites/default/files/DATASET_SINADEF.csv&quot; url &lt;- &quot;https://dparedesi.github.io/DS-con-R/DATASET_SINADEF.csv&quot; # We will use read_delim because it is delimited by &quot;;&quot; and not by &quot;,&quot; # Also we change the encoding to avoid error in loading deaths &lt;- read_delim(url, &quot;;&quot;, local = locale(encoding = &quot;latin1&quot;)) Download the file “https://dparedesi.github.io/DS-con-R/resources-other-idd.xlsx” and load the “Deflators” tab to the data object. Solution # Store the url url &lt;- &quot;https://dparedesi.github.io/DS-con-R/resources-other-idd.xlsx&quot; # Create a temporary name &amp; path for our file. See: ?tempfile temp_file &lt;- tempfile() # Download the file to our temp download.file(url, temp_file) # Import the excel dat &lt;- read_excel(temp_file, sheet = &quot;Deflators&quot;) # Remove the temporary file file.remove(temp_file) For the following files run the following code so that you have access to the objects referred to in the problems: # GDP by countries url &lt;- &quot;https://dparedesi.github.io/DS-con-R/pbi.csv&quot; pbi &lt;- read_csv(url) # Diseases by years by countries url &lt;- &quot;https://dparedesi.github.io/DS-con-R/enfermedades-evolutivo.csv&quot; enfermedades_wide &lt;- read_csv(url) # Number of female mayors url &lt;- &quot;https://dparedesi.github.io/DS-con-R/alcaldes-mujeres.csv&quot; alcaldes_mujeres &lt;- read_csv(url) # Evolution of a university url &lt;- &quot;https://dparedesi.github.io/DS-con-R/universidad.csv&quot; universidad &lt;- read_csv(url) Explore the pbi (GDP) object and convert it to tidy data. Then show a line graph differentiating the evolution of each country’s GDP. Solution # To tidy data pbi &lt;- pbi |&gt; pivot_longer(cols = -pais, names_to = &quot;año&quot;, values_to = &quot;pbi&quot;, names_transform = list(año = as.integer)) pbi # Visualization pbi |&gt; ggplot() + aes(año, pbi, color=pais) + geom_line() Explore the enfermedades_wide object and convert it to tidy data. Solution # Solution enfermedades_1 &lt;- enfermedades_wide |&gt; pivot_longer(cols = c(-pais, -año, -poblacion), names_to = &quot;enfermedad&quot;, values_to = &quot;cantidad&quot;) enfermedades_1 # Alternative solution. Instead of indicating what to omit, we indicate what to take into account enfermedades_2 &lt;- enfermedades_wide |&gt; pivot_longer(cols = HepatitisA:Rubeola, names_to = &quot;enfermedad&quot;, values_to = &quot;cantidad&quot;) enfermedades_2 Explore the alcaldes_mujeres object and then convert it to tidy data Solution alcaldes_mujeres &lt;- alcaldes_mujeres |&gt; pivot_wider(names_from = variable, values_from = total) Explore the universidad object and then convert it to tidy data Solution universidad &lt;- universidad |&gt; pivot_longer(cols = -semestre, names_to = &quot;variable&quot;, values_to = &quot;valor&quot;) |&gt; separate(variable, c(&quot;nombre&quot;, &quot;variable2&quot;), sep=&quot;_&quot;) |&gt; pivot_wider(names_from = variable2, values_from = valor) universidad 10.4 Joining tables Regularly we will have data from different sources that we then have to combine to be able to perform our analyses. For this we will learn different groups of functions that will allow us to combine multiple objects. 10.4.1 Join functions Join functions are the most used in table crossing. To use them we have to make sure we have the dplyr library installed. library(dplyr) This library includes a variety of functions to combine tables. TABLE 10.1: List of join functions Function Usage left_join() Only keeps rows that have information in the first table. right_join() Only keeps rows that have information in the second table. inner_join() Only keeps rows that have information in both tables. full_join() Keeps all rows from both tables. semi_join() Keeps the part of the first table for which we have information in the second. anti_join() Keeps the elements of the first table for which there is no information in the second. To see the join functions with examples we will use the following files: url_1 &lt;- &quot;https://dparedesi.github.io/DS-con-R/j_tarjeta.csv&quot; url_2 &lt;- &quot;https://dparedesi.github.io/DS-con-R/j_cliente.csv&quot; data_1_tarjetas &lt;- read_csv(url_1, col_types = cols(dni = col_character())) data_2_clientes &lt;- read_csv(url_2, col_types = cols(dni = col_character())) data_1_tarjetas #&gt; # A tibble: 6 × 3 #&gt; dni tipo_cliente tarjeta #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 45860518 premium VISA oro #&gt; 2 46534312 bronce Mastercard Black #&gt; 3 47564535 plata VISA plantinum #&gt; 4 48987654 bronce American Express #&gt; 5 78765434 oro VISA Signature #&gt; 6 41346556 premium Diners Club data_2_clientes #&gt; # A tibble: 8 × 4 #&gt; dni nombres apellido_paterno apellido_materno #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 49321442 Iver Castro Rivera #&gt; 2 47564535 Enrique Gutierrez Rivasplata #&gt; 3 48987654 Alexandra Cupe Gaspar #&gt; 4 47542345 Christiam Olortegui Roca #&gt; 5 41346556 Karen Jara Mory #&gt; 6 45860518 Hebert Lopez Chavez #&gt; 7 71234321 Jesus Valle Mariños #&gt; 8 73231243 Jenny Sosa Sosa 10.4.1.1 Left join Given two tables with the same identifier (in our case our identifier consists only of a single column: DNI), the left join function maintains the information of the first table and completes it with the data that crosses in the second table left_join(data_1_tarjetas, data_2_clientes, by = c(&quot;dni&quot;)) #&gt; # A tibble: 6 × 6 #&gt; dni tipo_cliente tarjeta nombres apellido_paterno apellido_materno #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 45860518 premium VISA oro Hebert Lopez Chavez #&gt; 2 46534312 bronce Mastercard Bl… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 3 47564535 plata VISA plantinum Enrique Gutierrez Rivasplata #&gt; 4 48987654 bronce American Expr… Alexan… Cupe Gaspar #&gt; 5 78765434 oro VISA Signature &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 6 41346556 premium Diners Club Karen Jara Mory As we can see, the first three columns are exactly the same as we initially had and to the right of those columns we see the columns of the other table for the values ​​that did cross the data. In this case we are facing a data inconsistency since all customers of data_1_tarjetas should be in data_2_clientes. This inconsistency could lead us to have to map the data loss process, etc. 10.4.1.2 Right join Given two tables with the same identifier, the right join function maintains the information of the second table and completes it with the data that crosses in the first table right_join(data_1_tarjetas, data_2_clientes, by = &quot;dni&quot;) #&gt; # A tibble: 8 × 6 #&gt; dni tipo_cliente tarjeta nombres apellido_paterno apellido_materno #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 45860518 premium VISA oro Hebert Lopez Chavez #&gt; 2 47564535 plata VISA plantinum Enrique Gutierrez Rivasplata #&gt; 3 48987654 bronce American Expr… Alexan… Cupe Gaspar #&gt; 4 41346556 premium Diners Club Karen Jara Mory #&gt; 5 49321442 &lt;NA&gt; &lt;NA&gt; Iver Castro Rivera #&gt; 6 47542345 &lt;NA&gt; &lt;NA&gt; Christ… Olortegui Roca #&gt; 7 71234321 &lt;NA&gt; &lt;NA&gt; Jesus Valle Mariños #&gt; 8 73231243 &lt;NA&gt; &lt;NA&gt; Jenny Sosa Sosa The idea is the same as in left_join, only this time the NA are in the first two columns. 10.4.1.3 Inner join In this case we will only have the intersection of the tables. Only the result of the data that are in both tables will be shown. inner_join(data_1_tarjetas, data_2_clientes, by = &quot;dni&quot;) #&gt; # A tibble: 4 × 6 #&gt; dni tipo_cliente tarjeta nombres apellido_paterno apellido_materno #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 45860518 premium VISA oro Hebert Lopez Chavez #&gt; 2 47564535 plata VISA plantinum Enrique Gutierrez Rivasplata #&gt; 3 48987654 bronce American Expr… Alexan… Cupe Gaspar #&gt; 4 41346556 premium Diners Club Karen Jara Mory 10.4.1.4 Full join Full join is a total crossing of both. It shows us all the data that are in both the first and the second table. full_join(data_1_tarjetas, data_2_clientes, by = &quot;dni&quot;) #&gt; # A tibble: 10 × 6 #&gt; dni tipo_cliente tarjeta nombres apellido_paterno apellido_materno #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 45860518 premium VISA oro Hebert Lopez Chavez #&gt; 2 46534312 bronce Mastercard B… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 3 47564535 plata VISA plantin… Enrique Gutierrez Rivasplata #&gt; 4 48987654 bronce American Exp… Alexan… Cupe Gaspar #&gt; 5 78765434 oro VISA Signatu… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 6 41346556 premium Diners Club Karen Jara Mory #&gt; 7 49321442 &lt;NA&gt; &lt;NA&gt; Iver Castro Rivera #&gt; 8 47542345 &lt;NA&gt; &lt;NA&gt; Christ… Olortegui Roca #&gt; 9 71234321 &lt;NA&gt; &lt;NA&gt; Jesus Valle Mariños #&gt; 10 73231243 &lt;NA&gt; &lt;NA&gt; Jenny Sosa Sosa 10.4.1.5 Semi join The case of the semi join is very similar to left_join with the difference that it only shows us the columns of the first table and eliminates the data that did not manage to cross (what in left_join comes out as NA). Also, none of the columns of table 2 appear. This is like doing a filter requesting the following: show me only the data from table 1 that is also in table 2. semi_join(data_1_tarjetas, data_2_clientes, by = &quot;dni&quot;) #&gt; # A tibble: 4 × 3 #&gt; dni tipo_cliente tarjeta #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 45860518 premium VISA oro #&gt; 2 47564535 plata VISA plantinum #&gt; 3 48987654 bronce American Express #&gt; 4 41346556 premium Diners Club 10.4.1.6 Anti join In the case of anti_join we have the opposite of semi_join since it shows the data from table 1 that are not in table 2. anti_join(data_1_tarjetas, data_2_clientes, by = &quot;dni&quot;) #&gt; # A tibble: 2 × 3 #&gt; dni tipo_cliente tarjeta #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 46534312 bronce Mastercard Black #&gt; 2 78765434 oro VISA Signature 10.4.2 Joining without a common identifier Likewise, we will have some moments when we need to combine only two objects, without using any type of intersection. For this we will use the bind functions. These functions allow us to put together two vectors or tables either in rows or columns. 10.4.2.1 Union of vectors If we have two or more vectors of the same size we can create the union of the columns to create a table using the bind_cols() function. Let’s see with an example: vector_1 &lt;- c(&quot;hola&quot;, &quot;Has visto a&quot;, &quot;el&quot;) vector_2 &lt;- c(&quot;Julian&quot;, &quot;Carla&quot;, &quot;Miércoles&quot;) resultado &lt;- bind_cols(saludo = vector_1, sustantivos = vector_2) resultado #&gt; # A tibble: 3 × 2 #&gt; saludo sustantivos #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 hola Julian #&gt; 2 Has visto a Carla #&gt; 3 el Miércoles 10.4.2.2 Union of tables In the case of tables the use is the same. Likewise, we can also join the rows of two or more tables. To see its application let’s first create some example tables: tabla_1 &lt;- data.frame( nombre = c(&quot;Jhasury&quot;, &quot;Thomas&quot;, &quot;Andres&quot;, &quot;Josep&quot;), apellido = c(&quot;Campos&quot;, &quot;Gonzales&quot;, &quot;Santiago&quot;, &quot;Villaverde&quot;), direccion = c(&quot;Jr. los campos 471&quot;, &quot;Av. Casuarinas 142&quot;, NA, &quot;Av. Tupac Amaru 164&quot;), telefono = c(&quot;976567325&quot;, &quot;956732587&quot;, &quot;961445664&quot;, &quot;987786453&quot;) ) tabla_2 &lt;- data.frame( edad = c(21, 24, 19, 12), signo = c(&quot;Aries&quot;, &quot;Capricornio&quot;, &quot;Sagitario&quot;, &quot;Libra&quot;) ) # Create a table from row 2 to 3 of table 1 tabla_3 &lt;- tabla_1[2:3, ] Once we have our tables let’s proceed to join them. We see that they do not have a common identifier. resultado &lt;- bind_cols(tabla_1, tabla_2) resultado #&gt; nombre apellido direccion telefono edad signo #&gt; 1 Jhasury Campos Jr. los campos 471 976567325 21 Aries #&gt; 2 Thomas Gonzales Av. Casuarinas 142 956732587 24 Capricornio #&gt; 3 Andres Santiago &lt;NA&gt; 961445664 19 Sagitario #&gt; 4 Josep Villaverde Av. Tupac Amaru 164 987786453 12 Libra Likewise, we can join by rows like this: resultado &lt;- bind_rows(tabla_1, tabla_3) resultado #&gt; nombre apellido direccion telefono #&gt; 1 Jhasury Campos Jr. los campos 471 976567325 #&gt; 2 Thomas Gonzales Av. Casuarinas 142 956732587 #&gt; 3 Andres Santiago &lt;NA&gt; 961445664 #&gt; 4 Josep Villaverde Av. Tupac Amaru 164 987786453 #&gt; 5 Thomas Gonzales Av. Casuarinas 142 956732587 #&gt; 6 Andres Santiago &lt;NA&gt; 961445664 10.5 Web Scraping Web Scraping is the process of extracting data from a website. We will use it when we need to extract data directly from tables that are presented on websites. For this we will use the rvest library, included in the tidyverse library. library(tidyverse) library(rvest) The function we will use the most will be read_html() and as an argument we will place the url of the web from where we want to extract the data. We are not talking about a url that downloads a text file but a web page like this: Thus, we will use read_html() to store all the web html and then little by little access the table data in R. data_en_html &lt;- read_html(&quot;https://es.wikipedia.org/wiki/Anexo:Pa%C3%ADses_hispanos_por_poblaci%C3%B3n&quot;) data_en_html #&gt; {html_document} #&gt; &lt;html class=&quot;client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-pinned-clientpref-1 vector-feature-night-mode-enabled skin-theme-clientpref-day vector-sticky-header-enabled vector-toc-available&quot; lang=&quot;es&quot; dir=&quot;ltr&quot;&gt; #&gt; [1] &lt;head&gt;\\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8 ... #&gt; [2] &lt;body class=&quot;skin--responsive skin-vector skin-vector-search-vue mediawik ... Now that we have the data stored in the object we have to go looking for the data, doing scraping. For this we will use the html_nodes(\"table\") function to access the “table” node. tablas_web &lt;- data_en_html |&gt; html_nodes(&quot;table&quot;) Finally, we have to go index by index looking for the table that interests us. To give it table format we will use html_table. In this case we will use double brackets because it is a list of lists and the setNames() function to change the name of the columns. # We format as table and store in wide_tabla tabla_en_bruto &lt;- tablas_web[[1]] |&gt; html_table() # Change header names tabla_en_bruto &lt;- tabla_en_bruto |&gt; setNames( c(&quot;N&quot;, &quot;pais&quot;, &quot;poblacion&quot;, &quot;prop_poblacion&quot;, &quot;cambio_medio&quot;, &quot;link&quot;) ) # Convert to tibble tabla_en_bruto &lt;- tabla_en_bruto |&gt; as_tibble() # Report first rows tabla_en_bruto |&gt; head(5) #&gt; # A tibble: 1 × 2 #&gt; N pais #&gt; &lt;lgl&gt; &lt;chr&gt; #&gt; 1 NA Este artículo o sección se encuentra desactualizado.La información sumi… We already have our data imported and we could already start exploring its content in detail. 10.6 Exercises For the following exercises we will use objects from the Lahman library, which contains US baseball player data. Run the following Script before starting to solve the exercises. install.packages(&quot;Lahman&quot;) library(Lahman) # Top 10 players of the year 2016 top_jugadores &lt;- Batting |&gt; filter(yearID == 2016) |&gt; arrange(desc(HR)) |&gt; # sorted by number of &quot;Home run&quot; slice(1:10) # Take from row 1 to 10 top_jugadores &lt;- top_jugadores |&gt; as_tibble() # List of all baseball players from recent years maestra &lt;- Master |&gt; as_tibble() # Awards won by players premios &lt;- AwardsPlayers |&gt; filter(yearID == 2016) |&gt; as_tibble() Using the top_jugadores object and the Maestra object, report the following fields playerID, nameFirst, nameLast, HR of the top 10 players of 2016. Solution top_10 &lt;- left_join(top_jugadores, maestra, by = &quot;playerID&quot;) |&gt; select(playerID, nameFirst, nameLast, HR) top_10 Report the ID and names of the top 10 players who have won at least one prize, premios object, in 2016. Solution semi_join(top_10, premios, by = &quot;playerID&quot;) Report the ID and names of the players who won at least one prize in 2016, but are not part of the top 10. Solution # First we calculate all prizes of those who are not top 10: ID_de_premiados_no_top &lt;- anti_join(premios, top_10, by = &quot;playerID&quot;) |&gt; select(playerID) # As a player could have obtained several prizes we obtain unique values ID_de_premiados_no_top &lt;- unique(ID_de_premiados_no_top) # Then we cross with the master to obtain the names nombres_de_otros &lt;- left_join(ID_de_premiados_no_top, maestra, by = &quot;playerID&quot;) |&gt; select(playerID, nameFirst, nameLast) nombres_de_otros Store the tables from the following web page: http://www.stevetheump.com/Payrolls.htm which contains the payroll that each US baseball team pays as of February 2020 in the html object. Then, access the nodes using html_nodes(\"table\") and store it in the nodos object. Finally, report node 4 in html_table format. Solution url &lt;- &quot;http://www.stevetheump.com/Payrolls.htm&quot; html &lt;- read_html(url) nodos &lt;- html |&gt; html_nodes(&quot;table&quot;) nodos[[4]] |&gt; html_table() From the nodos object created in the previous exercise, store node 4 in the planilla_2019 object and node 5 in the planilla_2018 object. Now format both tables so that the headers are: equipo (team), planilla_2019 or planilla_2018 according to the table. Finally, do a crossing of both tables by team name using full_join(). Solution planilla_2019 &lt;- nodos[[4]] |&gt; html_table() planilla_2018 &lt;- nodos[[5]] |&gt; html_table() ####### Payroll 2019: ################ #We eliminate row 15 which is the league average: planilla_2019 &lt;- planilla_2019[-15, ] #We filter the requested columns: planilla_2019 &lt;- planilla_2019 |&gt; select(X2, X4) |&gt; rename(equipo = X2, planilla_2019 = X4) # We eliminate row 1 since it is the source header planilla_2019 &lt;- planilla_2019[-1,] ####### Payroll 2018: ################ # We select the two columns that interest us and #change name to headers planilla_2018 &lt;- planilla_2018 |&gt; select(Team, Payroll) |&gt; rename(equipo = Team, planilla_2018 = Payroll) ####### Full join: ################ full_join(planilla_2018, planilla_2019, by = &quot;equipo&quot;) References "],["string-processing-and-text-mining.html", "Chapter 11 String processing and text mining 11.1 Basic functions 11.2 Regular expressions 11.3 From strings to dates 11.4 Exercises 11.5 Text Mining: Word Cloud 11.6 Text Mining: Sentiment Analysis 11.7 Exercises", " Chapter 11 String processing and text mining 11.1 Basic functions We have already learned how to import data and consolidate it. However, we cannot yet work with this data. We have to validate through string processing and ensure a minimum quality to be able to perform our analyses. For example, in the previous chapter we imported data from Wikipedia, however we did not focus on whether we could already perform operations or visualizations with our data. library(rvest) url &lt;- &quot;https://es.wikipedia.org/wiki/Anexo:Pa%C3%ADses_hispanos_por_poblaci%C3%B3n&quot; #url &lt;- &quot;https://es.wikipedia.org/wiki/Distribuci%C3%B3n_geogr%C3%A1fica_del_idioma_espa%C3%B1ol&quot; #as a back up URL html_data &lt;- read_html(url) web_tables &lt;- html_data |&gt; html_nodes(&quot;body&quot;) |&gt; html_nodes(&quot;table&quot;) raw_table &lt;- web_tables[[2]] |&gt; html_table() raw_table &lt;- raw_table |&gt; setNames(c(&quot;N&quot;, &quot;pais&quot;, &quot;poblacion&quot;, &quot;prop_poblacion&quot;, &quot;cambio_medio&quot;, &quot;link&quot;)) raw_table &lt;- raw_table |&gt; as_tibble() raw_table |&gt; head(5) We may not have noticed, but we can observe columns with spaces or commas where there should be numbers. We can validate this not only by analyzing the class of the column, but also if we try to calculate the average of that variable. class(raw_table$poblacion) mean(raw_table$poblacion) We cannot do a direct conversion to number either because white spaces and commas are characters. as.numeric(raw_table$poblacion) There are so frequent and so many possible use cases that there are already multiple functions for processing strings included in the tidyverse library. Likewise, there is more than one way to process strings. It will always depend on how the raw data is found. 11.1.1 Replacing characters One of the basic functions that we will use the most will be replacing characters. We apply this function when we are sure that this change will not compromise the rest of the data. We have spaces and we have commas. So we could start by replacing one of the two to normalize them using the str_replace_all(string, pattern, replacement) function. In the pattern attribute we will use \\\\s, which comes from space. We are going to learn first to modify the data stored in a vector and then we will replicate it to our entire table. library(tidyverse) library(stringr) population_vector &lt;- tabla_en_bruto$poblacion population_vector &lt;- str_replace_all(population_vector, &quot;\\\\s&quot;, &quot;,&quot;) population_vector We have purposely taken all the values to be separated by commas because now we can easily use the parse_number(vector) function which not only replaces the commas with empty strings, but also removes any non-numeric value before the first number, which facilitates us if we had monetary values, and also converts the value from character type to numeric type. population_vector &lt;- parse_number(population_vector) # Additional example in case we had a monetary value: parse_number(&quot;$345,153&quot;) This vector now allows us to perform mathematical operations or visualization of the distribution. # Convert to millions population_vector &lt;- population_vector/10^6 # We remove the last value which is the world population: length_val &lt;- length(population_vector) population_vector &lt;- population_vector[-length_val] # Visualization boxplot(population_vector) We already know which functions to use to transform the fields of our case. However, we have applied them to vectors. To mutate the columns of our table in raw form we will use the function mutate(across(columns, function)) using the pipeline operator |&gt;. Let’s apply the first change of spaces by commas and not only to column 3, population, but also to column 5, medium change. raw_table |&gt; mutate(across(c(3,5), ~str_replace_all(., &quot;\\\\s&quot;, &quot;,&quot;))) We have removed from the str_replace_all function the string attribute and replaced it with a dot .. And that dot . indicates that it will evaluate for each column c(3,5) of our table. Now, let’s apply the parse_number function that we applied previously. raw_table |&gt; mutate(across(c(3,5), ~str_replace_all(., &quot;\\\\s&quot;, &quot;,&quot;))) |&gt; mutate(across(c(3,5), ~parse_number(.))) 11.2 Regular expressions A regular expression10 (or regex as it is known in English) is a pattern that describes a set of strings. We have already used regex in the previous section using only the pattern \\\\s. However, usually we will have many more use cases that will require a pattern that can convert a wider range of cases. Although we could analyze all possible use cases available in the documentation, we learn faster by use cases. Let’s analyze a case that will allow us to learn some patterns little by little. In the dslabs library we found and used previously the height data, heights, of students from a university expressed in inches. library(dslabs) data(heights) heights |&gt; head(10) #&gt; sex height #&gt; 1 Male 75 #&gt; 2 Male 70 #&gt; 3 Male 68 #&gt; 4 Male 74 #&gt; 5 Male 61 #&gt; 6 Female 65 #&gt; 7 Female 66 #&gt; 8 Female 62 #&gt; 9 Female 66 #&gt; 10 Male 67 These data were ready to be analyzed. However, that was not how it came from the source. The students had to fill out a survey and even when they were asked for their height in inches, they completed their height in inches, feet, centimeters, writing numbers, letters, etc. We can see the initial data from the form in the reported_heights data frame. reported_heights |&gt; head(10) #&gt; time_stamp sex height #&gt; 1 2014-09-02 13:40:36 Male 75 #&gt; 2 2014-09-02 13:46:59 Male 70 #&gt; 3 2014-09-02 13:59:20 Male 68 #&gt; 4 2014-09-02 14:51:53 Male 74 #&gt; 5 2014-09-02 15:16:15 Male 61 #&gt; 6 2014-09-02 15:16:16 Female 65 #&gt; 7 2014-09-02 15:16:19 Female 66 #&gt; 8 2014-09-02 15:16:21 Female 62 #&gt; 9 2014-09-02 15:16:21 Female 66 #&gt; 10 2014-09-02 15:16:22 Male 67 Although we might think that they entered the data correctly, we do not have to trust and it is always better to validate the quality of our data. There are multiple ways to validate, as we can see below: heights &lt;- reported_heights$height # Validation option 1: Random sample sample(heights, 100) #&gt; [1] &quot;72&quot; &quot;69&quot; &quot;69&quot; &quot;68&quot; &quot;70&quot; #&gt; [6] &quot;71&quot; &quot;69&quot; &quot;69&quot; &quot;5&#39;6&quot; &quot;72.83&quot; #&gt; [11] &quot;74&quot; &quot;183&quot; &quot;70&quot; &quot;5&#39;10&#39;&#39;&quot; &quot;72.44&quot; #&gt; [16] &quot;87&quot; &quot;72&quot; &quot;63&quot; &quot;72.44&quot; &quot;72&quot; #&gt; [21] &quot;73&quot; &quot;74&quot; &quot;70&quot; &quot;68&quot; &quot;71&quot; #&gt; [26] &quot;87&quot; &quot;68.8976&quot; &quot;64.2&quot; &quot;73.22&quot; &quot;70&quot; #&gt; [31] &quot;183&quot; &quot;60&quot; &quot;63&quot; &quot;65&quot; &quot;63&quot; #&gt; [36] &quot;5.5&quot; &quot;5&#39; 11\\&quot;&quot; &quot;66&quot; &quot;72&quot; &quot;66&quot; #&gt; [41] &quot;70&quot; &quot;71.5&quot; &quot;71&quot; &quot;67.7165&quot; &quot;183&quot; #&gt; [46] &quot;67&quot; &quot;68&quot; &quot;67&quot; &quot;68&quot; &quot;62&quot; #&gt; [51] &quot;72&quot; &quot;25&quot; &quot;5&#39;5&quot; &quot;5.5&quot; &quot;70&quot; #&gt; [56] &quot;63&quot; &quot;65&quot; &quot;5&#39;7\\&quot;&quot; &quot;63&quot; &quot;73.2&quot; #&gt; [61] &quot;70&quot; &quot;68.8976378&quot; &quot;5&#39;2\\&quot;&quot; &quot;68&quot; &quot;68.89&quot; #&gt; [66] &quot;5.9&quot; &quot;70&quot; &quot;67&quot; &quot;68&quot; &quot;63&quot; #&gt; [71] &quot;170&quot; &quot;73&quot; &quot;72&quot; &quot;67&quot; &quot;72.5&quot; #&gt; [76] &quot;63&quot; &quot;70&quot; &quot;74&quot; &quot;74&quot; &quot;5&#39;6&quot; #&gt; [81] &quot;71&quot; &quot;70&quot; &quot;62&quot; &quot;67&quot; &quot;64.5&quot; #&gt; [86] &quot;5&#39;7&quot; &quot;65&quot; &quot;66&quot; &quot;68.5&quot; &quot;64.961&quot; #&gt; [91] &quot;180&quot; &quot;5,3&quot; &quot;63&quot; &quot;172&quot; &quot;77&quot; #&gt; [96] &quot;6.1&quot; &quot;5&#39;7.5&#39;&#39;&quot; &quot;72&quot; &quot;22&quot; &quot;5&#39;12&quot; # Validation option 2: convert to numbers and count if there are NAs x &lt;- as.numeric(heights) #&gt; Warning: NAs introduced by coercion sum(is.na(x)) #&gt; [1] 81 # Validation option 3: add column of those that cannot be converted to number: reported_heights |&gt; mutate(estatuta_numero = as.numeric(height)) |&gt; filter(is.na(estatuta_numero)) |&gt; head(10) #&gt; Warning: There was 1 warning in `mutate()`. #&gt; ℹ In argument: `estatuta_numero = #&gt; as.numeric(height)`. #&gt; Caused by warning: #&gt; ! NAs introduced by coercion #&gt; time_stamp sex height estatuta_numero #&gt; 1 2014-09-02 15:16:28 Male 5&#39; 4&quot; NA #&gt; 2 2014-09-02 15:16:37 Female 165cm NA #&gt; 3 2014-09-02 15:16:52 Male 5&#39;7 NA #&gt; 4 2014-09-02 15:16:56 Male &gt;9000 NA #&gt; 5 2014-09-02 15:16:56 Male 5&#39;7&quot; NA #&gt; 6 2014-09-02 15:17:09 Female 5&#39;3&quot; NA #&gt; 7 2014-09-02 15:18:00 Male 5 feet and 8.11 inches NA #&gt; 8 2014-09-02 15:19:48 Male 5&#39;11 NA #&gt; 9 2014-09-04 00:46:45 Male 5&#39;9&#39;&#39; NA #&gt; 10 2014-09-04 10:29:44 Male 5&#39;10&#39;&#39; NA We might want to choose to eliminate these NA data as they are not significant with respect to the total of 1,095 data points. However, there are several of these data points that follow a determined pattern and instead of being discarded could be converted to the scale we have in the rest of the data. For example, there are people who entered their height as 5’7”, which, for those who remember the conversion, can be converted because 1 foot is 12 inches. So \\(5*12+7=67\\). And so, like that case, we can detect patterns, but we have, again, to be careful in detecting the exact pattern and not a very generic one that can change other use cases. If everyone followed the same pattern \\(x&#39;y&#39;&#39;\\) or \\(x&#39;y\\) it would be much easier to convert it to inches by calculating \\(x*12+y\\). Let’s start by extracting our column to a single character vector with all the values that do not convert automatically to number or were entered in inches. We detect this if they measure more than 5 and up to 7 feet (from 1.5m to 2.1 meters). After that we will create the transformations little by little. heights_error &lt;- reported_heights |&gt; filter(is.na(as.numeric(height)) | # Does not convert to number (!is.na(as.numeric(height)) &amp; as.numeric(height) &gt;= 5 &amp; as.numeric(height) &lt;= 7 ) # or entered in feet and not inches ) |&gt; pull(height) length(heights_error) #&gt; [1] 168 Adding the condition of having entered in feet we have 168 errors. We cannot ignore 15.3% of errors. We will use the str_detect(string, pattern) function that will allow us to detect if a string matches a certain pattern. The result will be a logical value: TRUE or FALSE that we can use as an index to obtain the values that match in our vector. indice &lt;- str_detect(heights_error, &quot;feet&quot;) heights_error[indice] # Match the pattern #&gt; [1] &quot;5 feet and 8.11 inches&quot; &quot;5 feet 7inches&quot; &quot;5 feet 6 inches&quot; heights_error[!indice] |&gt; # Do not match the pattern head(40) #&gt; [1] &quot;6&quot; &quot;5&#39; 4\\&quot;&quot; &quot;5.3&quot; #&gt; [4] &quot;165cm&quot; &quot;6&quot; &quot;5&#39;7&quot; #&gt; [7] &quot;&gt;9000&quot; &quot;5&#39;7\\&quot;&quot; &quot;5&#39;3\\&quot;&quot; #&gt; [10] &quot;5.25&quot; &quot;5&#39;11&quot; &quot;5.5&quot; #&gt; [13] &quot;5&#39;9&#39;&#39;&quot; &quot;6&quot; &quot;6.5&quot; #&gt; [16] &quot;5&#39;10&#39;&#39;&quot; &quot;5.8&quot; &quot;5&quot; #&gt; [19] &quot;5.6&quot; &quot;5,3&quot; &quot;6&#39;&quot; #&gt; [22] &quot;6&quot; &quot;5.9&quot; &quot;6,8&quot; #&gt; [25] &quot;5&#39; 10&quot; &quot;5.5&quot; &quot;6.2&quot; #&gt; [28] &quot;Five foot eight inches&quot; &quot;6.2&quot; &quot;5.8&quot; #&gt; [31] &quot;5.1&quot; &quot;5.11&quot; &quot;5&#39;5\\&quot;&quot; #&gt; [34] &quot;5&#39;2\\&quot;&quot; &quot;5.75&quot; &quot;5,4&quot; #&gt; [37] &quot;7&quot; &quot;5.4&quot; &quot;6.1&quot; #&gt; [40] &quot;5&#39;3&quot; 11.2.1 Alternation | is the alternation operator that will choose between one or more possible values. In our case, we have indicated to detect if there is the word “feet”, but we also have “ft” and “foot” to refer to the same thing in our data. Thus, we can create the pattern “feet” or “ft” or “foot”. indice &lt;- str_detect(heights_error, &quot;feet|ft|foot&quot;) heights_error[indice] # Match #&gt; [1] &quot;5 feet and 8.11 inches&quot; &quot;Five foot eight inches&quot; &quot;5 feet 7inches&quot; #&gt; [4] &quot;5ft 9 inches&quot; &quot;5 ft 9 inches&quot; &quot;5 feet 6 inches&quot; In the same way we can find the variations for inches and other symbols that we can remove: indice &lt;- str_detect(heights_error, &quot;inches|in|&#39;&#39;|\\&quot;|cm|and&quot;) heights_error[indice] # Match #&gt; [1] &quot;5&#39; 4\\&quot;&quot; &quot;165cm&quot; &quot;5&#39;7\\&quot;&quot; #&gt; [4] &quot;5&#39;3\\&quot;&quot; &quot;5 feet and 8.11 inches&quot; &quot;5&#39;9&#39;&#39;&quot; #&gt; [7] &quot;5&#39;10&#39;&#39;&quot; &quot;Five foot eight inches&quot; &quot;5&#39;5\\&quot;&quot; #&gt; [10] &quot;5&#39;2\\&quot;&quot; &quot;5&#39;10&#39;&#39;&quot; &quot;5&#39;3&#39;&#39;&quot; #&gt; [13] &quot;5&#39;7&#39;&#39;&quot; &quot;5&#39;3\\&quot;&quot; &quot;5&#39;6&#39;&#39;&quot; #&gt; [16] &quot;5&#39;7.5&#39;&#39;&quot; &quot;5&#39;7.5&#39;&#39;&quot; &quot;5&#39;2\\&quot;&quot; #&gt; [19] &quot;5&#39; 7.78\\&quot;&quot; &quot;5 feet 7inches&quot; &quot;5&#39;8\\&quot;&quot; #&gt; [22] &quot;5&#39;11\\&quot;&quot; &quot;5&#39;7\\&quot;&quot; &quot;5&#39; 11\\&quot;&quot; #&gt; [25] &quot;6&#39;1\\&quot;&quot; &quot;69\\&quot;&quot; &quot;5&#39; 7\\&quot;&quot; #&gt; [28] &quot;5&#39;10&#39;&#39;&quot; &quot;5ft 9 inches&quot; &quot;5 ft 9 inches&quot; #&gt; [31] &quot;5&#39;11&#39;&#39;&quot; &quot;5&#39;8\\&quot;&quot; &quot;5 feet 6 inches&quot; #&gt; [34] &quot;5&#39;10&#39;&#39;&quot; &quot;6&#39;3\\&quot;&quot; &quot;5&#39;5&#39;&#39;&quot; #&gt; [37] &quot;5&#39;7\\&quot;&quot; &quot;6&#39;4\\&quot;&quot; &quot;170 cm&quot; In this case we have entered '' to detect those who entered that symbol to denote inches and \\\" in case they used double quotes. In this latter case we have used \\ so that it does not generate an error when interpreting as closing the string. We could already start replacing based on the detected patterns: heights_error &lt;- str_replace_all(heights_error, &quot;feet|ft|foot&quot;, &quot;&#39;&quot;) heights_error &lt;- str_replace_all(heights_error, &quot;inches|in|&#39;&#39;|\\&quot;|cm|and&quot;, &quot;&quot;) heights_error |&gt; head(30) #&gt; [1] &quot;6&quot; &quot;5&#39; 4&quot; &quot;5.3&quot; &quot;165&quot; #&gt; [5] &quot;6&quot; &quot;5&#39;7&quot; &quot;&gt;9000&quot; &quot;5&#39;7&quot; #&gt; [9] &quot;5&#39;3&quot; &quot;5 &#39; 8.11 &quot; &quot;5.25&quot; &quot;5&#39;11&quot; #&gt; [13] &quot;5.5&quot; &quot;5&#39;9&quot; &quot;6&quot; &quot;6.5&quot; #&gt; [17] &quot;5&#39;10&quot; &quot;5.8&quot; &quot;5&quot; &quot;5.6&quot; #&gt; [21] &quot;5,3&quot; &quot;6&#39;&quot; &quot;6&quot; &quot;5.9&quot; #&gt; [25] &quot;6,8&quot; &quot;5&#39; 10&quot; &quot;5.5&quot; &quot;6.2&quot; #&gt; [29] &quot;Five &#39; eight &quot; &quot;6.2&quot; As an additional effort, we could also look to solve that some people have written words instead of numbers. For this we create a function that replaces each word with a number and apply it to the vector: words_to_number &lt;- function(s){ str_to_lower(s) |&gt; str_replace_all(&quot;zero&quot;, &quot;0&quot;) |&gt; str_replace_all(&quot;one&quot;, &quot;1&quot;) |&gt; str_replace_all(&quot;two&quot;, &quot;2&quot;) |&gt; str_replace_all(&quot;three&quot;, &quot;3&quot;) |&gt; str_replace_all(&quot;four&quot;, &quot;4&quot;) |&gt; str_replace_all(&quot;five&quot;, &quot;5&quot;) |&gt; str_replace_all(&quot;six&quot;, &quot;6&quot;) |&gt; str_replace_all(&quot;seven&quot;, &quot;7&quot;) |&gt; str_replace_all(&quot;eight&quot;, &quot;8&quot;) |&gt; str_replace_all(&quot;nine&quot;, &quot;9&quot;) |&gt; str_replace_all(&quot;ten&quot;, &quot;10&quot;) |&gt; str_replace_all(&quot;eleven&quot;, &quot;11&quot;) } heights_error &lt;- words_to_number(heights_error) heights_error |&gt; head(30) #&gt; [1] &quot;6&quot; &quot;5&#39; 4&quot; &quot;5.3&quot; &quot;165&quot; &quot;6&quot; #&gt; [6] &quot;5&#39;7&quot; &quot;&gt;9000&quot; &quot;5&#39;7&quot; &quot;5&#39;3&quot; &quot;5 &#39; 8.11 &quot; #&gt; [11] &quot;5.25&quot; &quot;5&#39;11&quot; &quot;5.5&quot; &quot;5&#39;9&quot; &quot;6&quot; #&gt; [16] &quot;6.5&quot; &quot;5&#39;10&quot; &quot;5.8&quot; &quot;5&quot; &quot;5.6&quot; #&gt; [21] &quot;5,3&quot; &quot;6&#39;&quot; &quot;6&quot; &quot;5.9&quot; &quot;6,8&quot; #&gt; [26] &quot;5&#39; 10&quot; &quot;5.5&quot; &quot;6.2&quot; &quot;5 &#39; 8 &quot; &quot;6.2&quot; 11.2.2 Anchoring Now that it is more standardized we can start with regex with more generic characteristics. For example, there is a person who has entered 6'. It would be convenient to have everything in the form feet plus inches. With which we should have 6'0. To achieve this we have to create a regex according to this generic situation. We will use the symbol ^ to anchor our validation to “start with” and the symbol $ to match with the end of the string. Before replacing, let’s first see who matches. indice &lt;- str_detect(heights_error, &quot;^6&#39;$&quot;) heights_error[indice] # Match #&gt; [1] &quot;6&#39;&quot; This regex indicates that it starts with 6' and that the expression ends there. We could still make it more generic to address those who, in the future, write 5 inches (1.52m) or 6 inches (1.82m). For this we will use brackets and inside them we will put all the values that we will accept. indice &lt;- str_detect(heights_error, &quot;^[56]&#39;$&quot;) heights_error[indice] # Match #&gt; [1] &quot;6&#39;&quot; There is still only one result, but our regex is more generic now and we can already use it to replace. Before replacing in our vector we are going to do a test to learn how to create what we need from a pattern. prueba &lt;- c(&quot;5&#39;&quot;, &quot;6&#39;&quot;) str_replace_all(prueba, &quot;^([56])&#39;$&quot;, &quot;\\\\1&#39;0&quot;) #&gt; [1] &quot;5&#39;0&quot; &quot;6&#39;0&quot; We have placed between parentheses to indicate that what is inside is our first value and we use \\\\1 to refer to that first value. So we are indicating to write the first value, then a quote ', and then a zero 0. Now we are ready to apply to our entire vector. We are going to make the change to consider not only 5 and 6, but up to the value of 7 inches (2.1m). Likewise, we are going to take the cases in which there is only a number without the foot symbol '. heights_error &lt;- str_replace_all(heights_error, &quot;^([5-7])&#39;$&quot;, &quot;\\\\1&#39;0&quot;) heights_error &lt;- str_replace_all(heights_error, &quot;^([5-7])$&quot;, &quot;\\\\1&#39;0&quot;) heights_error |&gt; head(30) #&gt; [1] &quot;6&#39;0&quot; &quot;5&#39; 4&quot; &quot;5.3&quot; &quot;165&quot; &quot;6&#39;0&quot; #&gt; [6] &quot;5&#39;7&quot; &quot;&gt;9000&quot; &quot;5&#39;7&quot; &quot;5&#39;3&quot; &quot;5 &#39; 8.11 &quot; #&gt; [11] &quot;5.25&quot; &quot;5&#39;11&quot; &quot;5.5&quot; &quot;5&#39;9&quot; &quot;6&#39;0&quot; #&gt; [16] &quot;6.5&quot; &quot;5&#39;10&quot; &quot;5.8&quot; &quot;5&#39;0&quot; &quot;5.6&quot; #&gt; [21] &quot;5,3&quot; &quot;6&#39;0&quot; &quot;6&#39;0&quot; &quot;5.9&quot; &quot;6,8&quot; #&gt; [26] &quot;5&#39; 10&quot; &quot;5.5&quot; &quot;6.2&quot; &quot;5 &#39; 8 &quot; &quot;6.2&quot; 11.2.3 Repetitions We can control how many times a pattern matches using repetition operators: TABLE 10.1: Repetition operators: Operator Number of times ? 0 or 1 time + 1 or more times * 0 or more times For example, to find all cases where instead of using the foot symbol ' they entered a comma, a period, or a space we will use the following pattern: pattern &lt;- &quot;^([4-7])\\\\s*[,\\\\.]\\\\s*(\\\\d*)$&quot; Let’s read the pattern: The string starts with a digit ranging from 4 to 7. \\\\s means that it is followed by a white space, but we use * to indicate that this character appears 0 or more times. After that space we will look for any of the following characters: ,, a period \\\\. (to which we put double backslash because the period alone in a pattern means “any value”). We use \\\\s* again to look for zero or more white spaces. Finally we indicate that the string ends there with a digit, to denote that look for any digit we use \\\\d, d for digit. And we add asterisk so that it keeps one or more digits that it finds. In summary: it starts with a number, then symbols and then a digit. Between the symbols there could be white spaces. That is our pattern. index &lt;- str_detect(heights_error, &quot;^([4-7])\\\\s*[,\\\\.]\\\\s*(\\\\d*)$&quot;) heights_error[index] # Match #&gt; [1] &quot;5.3&quot; &quot;5.25&quot; &quot;5.5&quot; &quot;6.5&quot; &quot;5.8&quot; &quot;5.6&quot; &quot;5,3&quot; &quot;5.9&quot; &quot;6,8&quot; #&gt; [10] &quot;5.5&quot; &quot;6.2&quot; &quot;6.2&quot; &quot;5.8&quot; &quot;5.1&quot; &quot;5.11&quot; &quot;5.75&quot; &quot;5,4&quot; &quot;5.4&quot; #&gt; [19] &quot;6.1&quot; &quot;5.6&quot; &quot;5.6&quot; &quot;5.4&quot; &quot;5.9&quot; &quot;5.6&quot; &quot;5.6&quot; &quot;5.5&quot; &quot;5.2&quot; #&gt; [28] &quot;5.5&quot; &quot;5.5&quot; &quot;6.5&quot; &quot;5,8&quot; &quot;5.11&quot; &quot;5.5&quot; &quot;6.7&quot; &quot;5.1&quot; &quot;5.6&quot; #&gt; [37] &quot;5.5&quot; &quot;5.2&quot; &quot;5.6&quot; &quot;5.7&quot; &quot;5.9&quot; &quot;6.5&quot; &quot;5.11&quot; &quot;5 .11&quot; &quot;5.7&quot; #&gt; [46] &quot;5.5&quot; &quot;5.8&quot; &quot;5.8&quot; &quot;5.1&quot; &quot;5.11&quot; &quot;5.7&quot; &quot;5.9&quot; &quot;5.2&quot; &quot;5.5&quot; #&gt; [55] &quot;5.51&quot; &quot;5.8&quot; &quot;5.7&quot; &quot;6.1&quot; &quot;5.69&quot; &quot;5.7&quot; &quot;5.25&quot; &quot;5.5&quot; &quot;5.1&quot; #&gt; [64] &quot;6.3&quot; &quot;5.5&quot; &quot;5.7&quot; &quot;5.57&quot; &quot;5.7&quot; We already found the values that match the pattern, so we are ready to replace. heights_error &lt;- str_replace_all( heights_error, &quot;^([4-7])\\\\s*[,\\\\.]\\\\s*(\\\\d*)$&quot;, &quot;\\\\1.\\\\2&#39;0&quot; ) heights_error |&gt; head(30) #&gt; [1] &quot;6&#39;0&quot; &quot;5&#39; 4&quot; &quot;5.3&#39;0&quot; &quot;165&quot; &quot;6&#39;0&quot; #&gt; [6] &quot;5&#39;7&quot; &quot;&gt;9000&quot; &quot;5&#39;7&quot; &quot;5&#39;3&quot; &quot;5 &#39; 8.11 &quot; #&gt; [11] &quot;5.25&#39;0&quot; &quot;5&#39;11&quot; &quot;5.5&#39;0&quot; &quot;5&#39;9&quot; &quot;6&#39;0&quot; #&gt; [16] &quot;6.5&#39;0&quot; &quot;5&#39;10&quot; &quot;5.8&#39;0&quot; &quot;5&#39;0&quot; &quot;5.6&#39;0&quot; #&gt; [21] &quot;5.3&#39;0&quot; &quot;6&#39;0&quot; &quot;6&#39;0&quot; &quot;5.9&#39;0&quot; &quot;6.8&#39;0&quot; #&gt; [26] &quot;5&#39; 10&quot; &quot;5.5&#39;0&quot; &quot;6.2&#39;0&quot; &quot;5 &#39; 8 &quot; &quot;6.2&#39;0&quot; Another pattern we see now is when before or after the foot symbol ' there is a white space. Let’s make the change with what we learned and include cases where there are decimals: index &lt;- str_detect(heights_error, &quot;^([4-7]\\\\.?\\\\d*)\\\\s*&#39;\\\\s*(\\\\d+\\\\.?\\\\d*)\\\\s*$&quot;) heights_error[index] |&gt; # Match head(30) #&gt; [1] &quot;6&#39;0&quot; &quot;5&#39; 4&quot; &quot;5.3&#39;0&quot; &quot;6&#39;0&quot; &quot;5&#39;7&quot; #&gt; [6] &quot;5&#39;7&quot; &quot;5&#39;3&quot; &quot;5 &#39; 8.11 &quot; &quot;5.25&#39;0&quot; &quot;5&#39;11&quot; #&gt; [11] &quot;5.5&#39;0&quot; &quot;5&#39;9&quot; &quot;6&#39;0&quot; &quot;6.5&#39;0&quot; &quot;5&#39;10&quot; #&gt; [16] &quot;5.8&#39;0&quot; &quot;5&#39;0&quot; &quot;5.6&#39;0&quot; &quot;5.3&#39;0&quot; &quot;6&#39;0&quot; #&gt; [21] &quot;6&#39;0&quot; &quot;5.9&#39;0&quot; &quot;6.8&#39;0&quot; &quot;5&#39; 10&quot; &quot;5.5&#39;0&quot; #&gt; [26] &quot;6.2&#39;0&quot; &quot;5 &#39; 8 &quot; &quot;6.2&#39;0&quot; &quot;5.8&#39;0&quot; &quot;5.1&#39;0&quot; heights_error &lt;- str_replace_all( heights_error, &quot;^([4-7]\\\\.?\\\\d*)\\\\s*&#39;\\\\s*(\\\\d+\\\\.?\\\\d*)\\\\s*$&quot;, &quot;\\\\1&#39;\\\\2&quot; ) heights_error |&gt; head(30) #&gt; [1] &quot;6&#39;0&quot; &quot;5&#39;4&quot; &quot;5.3&#39;0&quot; &quot;165&quot; &quot;6&#39;0&quot; &quot;5&#39;7&quot; &quot;&gt;9000&quot; &quot;5&#39;7&quot; #&gt; [9] &quot;5&#39;3&quot; &quot;5&#39;8.11&quot; &quot;5.25&#39;0&quot; &quot;5&#39;11&quot; &quot;5.5&#39;0&quot; &quot;5&#39;9&quot; &quot;6&#39;0&quot; &quot;6.5&#39;0&quot; #&gt; [17] &quot;5&#39;10&quot; &quot;5.8&#39;0&quot; &quot;5&#39;0&quot; &quot;5.6&#39;0&quot; &quot;5.3&#39;0&quot; &quot;6&#39;0&quot; &quot;6&#39;0&quot; &quot;5.9&#39;0&quot; #&gt; [25] &quot;6.8&#39;0&quot; &quot;5&#39;10&quot; &quot;5.5&#39;0&quot; &quot;6.2&#39;0&quot; &quot;5&#39;8&quot; &quot;6.2&#39;0&quot; Likewise, we have the pattern in which they entered: feet + space + inches without any symbol. Let’s make the change with what we learned. index &lt;- str_detect(heights_error, &quot;^([4-7])\\\\s+(\\\\d*)\\\\s*$&quot;) heights_error[index] # Match #&gt; [1] &quot;5 11&quot; &quot;6 04&quot; heights_error &lt;- str_replace_all( heights_error, &quot;^([4-7])\\\\s+(\\\\d*)\\\\s*$&quot;, &quot;\\\\1&#39;\\\\2&quot; ) heights_error |&gt; head(30) #&gt; [1] &quot;6&#39;0&quot; &quot;5&#39;4&quot; &quot;5.3&#39;0&quot; &quot;165&quot; &quot;6&#39;0&quot; &quot;5&#39;7&quot; &quot;&gt;9000&quot; &quot;5&#39;7&quot; #&gt; [9] &quot;5&#39;3&quot; &quot;5&#39;8.11&quot; &quot;5.25&#39;0&quot; &quot;5&#39;11&quot; &quot;5.5&#39;0&quot; &quot;5&#39;9&quot; &quot;6&#39;0&quot; &quot;6.5&#39;0&quot; #&gt; [17] &quot;5&#39;10&quot; &quot;5.8&#39;0&quot; &quot;5&#39;0&quot; &quot;5.6&#39;0&quot; &quot;5.3&#39;0&quot; &quot;6&#39;0&quot; &quot;6&#39;0&quot; &quot;5.9&#39;0&quot; #&gt; [25] &quot;6.8&#39;0&quot; &quot;5&#39;10&quot; &quot;5.5&#39;0&quot; &quot;6.2&#39;0&quot; &quot;5&#39;8&quot; &quot;6.2&#39;0&quot; We are ready to put all the patterns together and the power of patterns is that they can serve us for future exercises. Thus, we will create a function where we will place each change that we can verify to a string. format_errors &lt;- function(cadena){ cadena |&gt; str_replace_all(&quot;feet|ft|foot&quot;, &quot;&#39;&quot;) |&gt; # Change feet for &#39; str_replace_all(&quot;inches|in|&#39;&#39;|\\&quot;|cm|and&quot;, &quot;&quot;) |&gt; # Remove symbols str_replace_all(&quot;^([5-7])&#39;$&quot;, &quot;\\\\1&#39;0&quot;) |&gt; # Adds 0 to 5&#39;, 6&#39; or 7&#39; str_replace_all(&quot;^([5-7])$&quot;, &quot;\\\\1&#39;0&quot;) |&gt; # Adds 0 to 5, 6 or 7 str_replace_all(&quot;^([4-7])\\\\s*[,\\\\.]\\\\s*(\\\\d*)$&quot;, &quot;\\\\1.\\\\2&#39;0&quot;) |&gt; # Change 5.3&#39; to 5.3&#39;0 str_replace_all(&quot;^([4-7]\\\\.?\\\\d*)\\\\s*&#39;\\\\s*(\\\\d+\\\\.?\\\\d*)\\\\s*$&quot;, &quot;\\\\1&#39;\\\\2&quot;) |&gt; #Removes spaces in middle str_replace_all(&quot;^([4-7])\\\\s+(\\\\d*)\\\\s*$&quot;, &quot;\\\\1&#39;\\\\2&quot;) |&gt; # Adds &#39; str_replace(&quot;^([12])\\\\s*,\\\\s*(\\\\d*)$&quot;, &quot;\\\\1.\\\\2&quot;) |&gt; # Changes decimals from commas to dots str_trim() #Removes spaces at start and end } Thus, we have created two functions that could be useful to us if we were to work with surveys of the same type again. Before applying it to our entire table let’s extract the values to a vector again to apply the created functions. heights_error &lt;- reported_heights |&gt; filter(is.na(as.numeric(height)) | # Does not convert to number (!is.na(as.numeric(height)) &amp; as.numeric(height) &gt;= 5 &amp; as.numeric(height) &lt;= 7 ) # or entered in feet and not inches ) |&gt; pull(height) Now let’s apply the created functions: formatted_heights &lt;- heights_error |&gt; words_to_number() |&gt; format_errors() pattern &lt;- &quot;^([4-7]\\\\.?\\\\d*)\\\\s*&#39;\\\\s*(\\\\d+\\\\.?\\\\d*)\\\\s*$&quot; indice &lt;- str_detect(formatted_heights, pattern) formatted_heights[!indice] # Do not match the pattern #&gt; [1] &quot;165&quot; &quot;&gt;9000&quot; &quot;2&#39;33&quot; &quot;1.70&quot; &quot;yyy&quot; &quot;6*12&quot; #&gt; [7] &quot;69&quot; &quot;708,661&quot; &quot;649,606&quot; &quot;728,346&quot; &quot;170&quot; &quot;7,283,465&quot; We have managed to reduce from 168 errors of 1095 records, 15.3% of errors, to 12 errors of 1095, 1% of errors. We can now apply to our initial table. # Apply created formulas heights &lt;- reported_heights |&gt; mutate(height) |&gt; mutate(height = words_to_number(height) |&gt; format_errors()) # Get random samples to validate quality random_indices &lt;- sample(1:nrow(heights)) heights[random_indices, ] |&gt; head(15) #&gt; time_stamp sex height #&gt; 473 2015-02-05 14:49:08 Male 67 #&gt; 234 2014-09-03 23:47:35 Male 65 #&gt; 769 2016-01-14 14:33:13 Male 72 #&gt; 795 2016-01-25 08:15:50 Female 68.8976 #&gt; 92 2014-09-02 15:16:48 Female 63 #&gt; 951 2016-03-01 02:38:42 Male 5.7&#39;0 #&gt; 329 2014-10-16 03:51:23 Male 71 #&gt; 175 2014-09-02 15:17:23 Male 72 #&gt; 987 2016-04-25 06:11:53 Male 70.86 #&gt; 789 2016-01-25 08:15:45 Female 5&#39;5 #&gt; 6 2014-09-02 15:16:16 Female 65 #&gt; 398 2014-12-16 04:20:38 Female 66.1416 #&gt; 560 2015-05-09 12:58:19 Male 69 #&gt; 91 2014-09-02 15:16:48 Male 71 #&gt; 85 2014-09-02 15:16:47 Female 62 We still have to do some conversions. However, since they follow a determined pattern we can use the extract(source_column, new_columns, pattern, remove_source) function to confirm creating new columns for each value of our pattern. pattern &lt;- &quot;^([4-7]\\\\.?\\\\d*)\\\\s*&#39;\\\\s*(\\\\d+\\\\.?\\\\d*)\\\\s*$&quot; heights |&gt; extract(height, c(&quot;feet&quot;, &quot;inches&quot;), regex = pattern, remove = FALSE) |&gt; head(15) #&gt; time_stamp sex height feet inches #&gt; 1 2014-09-02 13:40:36 Male 75 &lt;NA&gt; &lt;NA&gt; #&gt; 2 2014-09-02 13:46:59 Male 70 &lt;NA&gt; &lt;NA&gt; #&gt; 3 2014-09-02 13:59:20 Male 68 &lt;NA&gt; &lt;NA&gt; #&gt; 4 2014-09-02 14:51:53 Male 74 &lt;NA&gt; &lt;NA&gt; #&gt; 5 2014-09-02 15:16:15 Male 61 &lt;NA&gt; &lt;NA&gt; #&gt; 6 2014-09-02 15:16:16 Female 65 &lt;NA&gt; &lt;NA&gt; #&gt; 7 2014-09-02 15:16:19 Female 66 &lt;NA&gt; &lt;NA&gt; #&gt; 8 2014-09-02 15:16:21 Female 62 &lt;NA&gt; &lt;NA&gt; #&gt; 9 2014-09-02 15:16:21 Female 66 &lt;NA&gt; &lt;NA&gt; #&gt; 10 2014-09-02 15:16:22 Male 67 &lt;NA&gt; &lt;NA&gt; #&gt; 11 2014-09-02 15:16:22 Male 72 &lt;NA&gt; &lt;NA&gt; #&gt; 12 2014-09-02 15:16:23 Male 6&#39;0 6 0 #&gt; 13 2014-09-02 15:16:23 Male 69 &lt;NA&gt; &lt;NA&gt; #&gt; 14 2014-09-02 15:16:26 Male 68 &lt;NA&gt; &lt;NA&gt; #&gt; 15 2014-09-02 15:16:26 Male 69 &lt;NA&gt; &lt;NA&gt; Now that we have the data that matches the pattern in two other columns, and we know they are numbers, we can convert everything to number. heights |&gt; extract(height, c(&quot;feet&quot;, &quot;inches&quot;), regex = pattern, remove = FALSE) |&gt; mutate(across(c(&quot;height&quot;, &quot;feet&quot;, &quot;inches&quot;), ~as.numeric(.))) |&gt; head(15) #&gt; Warning: There was 1 warning in `mutate()`. #&gt; ℹ In argument: `across(c(&quot;height&quot;, &quot;feet&quot;, #&gt; &quot;inches&quot;), ~as.numeric(.))`. #&gt; Caused by warning: #&gt; ! NAs introduced by coercion #&gt; time_stamp sex height feet inches #&gt; 1 2014-09-02 13:40:36 Male 75 NA NA #&gt; 2 2014-09-02 13:46:59 Male 70 NA NA #&gt; 3 2014-09-02 13:59:20 Male 68 NA NA #&gt; 4 2014-09-02 14:51:53 Male 74 NA NA #&gt; 5 2014-09-02 15:16:15 Male 61 NA NA #&gt; 6 2014-09-02 15:16:16 Female 65 NA NA #&gt; 7 2014-09-02 15:16:19 Female 66 NA NA #&gt; 8 2014-09-02 15:16:21 Female 62 NA NA #&gt; 9 2014-09-02 15:16:21 Female 66 NA NA #&gt; 10 2014-09-02 15:16:22 Male 67 NA NA #&gt; 11 2014-09-02 15:16:22 Male 72 NA NA #&gt; 12 2014-09-02 15:16:23 Male NA 6 0 #&gt; 13 2014-09-02 15:16:23 Male 69 NA NA #&gt; 14 2014-09-02 15:16:26 Male 68 NA NA #&gt; 15 2014-09-02 15:16:26 Male 69 NA NA Now that our columns are numeric we can perform operations to calculate height. heights |&gt; extract(height, c(&quot;feet&quot;, &quot;inches&quot;), regex = pattern, remove = FALSE) |&gt; mutate(across(c(&quot;height&quot;, &quot;feet&quot;, &quot;inches&quot;), ~as.numeric(.))) |&gt; mutate(fixed_heights = feet*12 + inches) |&gt; head(15) #&gt; Warning: There was 1 warning in `mutate()`. #&gt; ℹ In argument: `across(c(&quot;height&quot;, &quot;feet&quot;, #&gt; &quot;inches&quot;), ~as.numeric(.))`. #&gt; Caused by warning: #&gt; ! NAs introduced by coercion #&gt; time_stamp sex height feet inches fixed_heights #&gt; 1 2014-09-02 13:40:36 Male 75 NA NA NA #&gt; 2 2014-09-02 13:46:59 Male 70 NA NA NA #&gt; 3 2014-09-02 13:59:20 Male 68 NA NA NA #&gt; 4 2014-09-02 14:51:53 Male 74 NA NA NA #&gt; 5 2014-09-02 15:16:15 Male 61 NA NA NA #&gt; 6 2014-09-02 15:16:16 Female 65 NA NA NA #&gt; 7 2014-09-02 15:16:19 Female 66 NA NA NA #&gt; 8 2014-09-02 15:16:21 Female 62 NA NA NA #&gt; 9 2014-09-02 15:16:21 Female 66 NA NA NA #&gt; 10 2014-09-02 15:16:22 Male 67 NA NA NA #&gt; 11 2014-09-02 15:16:22 Male 72 NA NA NA #&gt; 12 2014-09-02 15:16:23 Male NA 6 0 72 #&gt; 13 2014-09-02 15:16:23 Male 69 NA NA NA #&gt; 14 2014-09-02 15:16:26 Male 68 NA NA NA #&gt; 15 2014-09-02 15:16:26 Male 69 NA NA NA Finally, we will do a validation of whether the height is in an interval and/or if it was expressed in centimeters or meters. # We assume for a person a minimum 50&quot; (1.2m) and max 84&quot; (2.1m) min &lt;- 50 max &lt;- 84 heights &lt;- heights |&gt; extract(height, c(&quot;feet&quot;, &quot;inches&quot;), regex = pattern, remove = FALSE) |&gt; mutate(across(c(&quot;height&quot;, &quot;feet&quot;, &quot;inches&quot;), ~as.numeric(.))) |&gt; mutate(fixed_heights = feet*12 + inches) |&gt; mutate(final_height = case_when( !is.na(height) &amp; between(height, min, max) ~ height, #inches !is.na(height) &amp; between(height/2.54, min, max) ~ height/2.54, #cm !is.na(height) &amp; between(height*100/2.54, min, max) ~ height*100/2.54, #meters !is.na(fixed_heights) &amp; inches &lt; 12 &amp; between(fixed_heights, min, max) ~ fixed_heights, #feet&#39;inches TRUE ~ as.numeric(NA))) #&gt; Warning: There was 1 warning in `mutate()`. #&gt; ℹ In argument: `across(c(&quot;height&quot;, &quot;feet&quot;, #&gt; &quot;inches&quot;), ~as.numeric(.))`. #&gt; Caused by warning: #&gt; ! NAs introduced by coercion # Random Sample: random_indices &lt;- sample(1:nrow(heights)) heights[random_indices, ] |&gt; select(-time_stamp) |&gt; # Shows all columns except time_stamp head(10) #&gt; sex height feet inches fixed_heights final_height #&gt; 118 Male 76.0 NA NA NA 76.00000 #&gt; 1038 Male 76.0 NA NA NA 76.00000 #&gt; 184 Female 70.0 NA NA NA 70.00000 #&gt; 775 Male 75.6 NA NA NA 75.60000 #&gt; 389 Male 178.0 NA NA NA 70.07874 #&gt; 1066 Male NA 6 0 72 72.00000 #&gt; 745 Male 167.0 NA NA NA 65.74803 #&gt; 898 Male 66.0 NA NA NA 66.00000 #&gt; 1029 Male 67.0 NA NA NA 67.00000 #&gt; 902 Female 66.0 NA NA NA 66.00000 We already have our sample validated, we would only have to take the columns we need and start using the object for the analyses we need. final_heights &lt;- heights |&gt; select(gender = sex, heights = final_height) final_heights |&gt; head(10) #&gt; gender heights #&gt; 1 Male 75 #&gt; 2 Male 70 #&gt; 3 Male 68 #&gt; 4 Male 74 #&gt; 5 Male 61 #&gt; 6 Female 65 #&gt; 7 Female 66 #&gt; 8 Female 62 #&gt; 9 Female 66 #&gt; 10 Male 67 11.3 From strings to dates Regularly when we import data, we are not only going to want to transform numeric data. We will also have multiple cases where we need to transform our string to a date in some particular format. For this, we will use the lubridate library, included in tidyverse, which provides us with diverse functions to make date treatment more accessible. library(lubridate) When the text string is in the ISO 8601 date format (YYYY-MM-DD), we can directly use the month(), day(), year() function. dates_char &lt;- c(&quot;2010-05-19&quot;, &quot;2020-05-06&quot;, &quot;2010-02-03&quot;) str(dates_char) #&gt; chr [1:3] &quot;2010-05-19&quot; &quot;2020-05-06&quot; &quot;2010-02-03&quot; month(dates_char) #&gt; [1] 5 5 2 However, we do not always have the date in that format and lubridate() gives other functions that are more flexible when coercing data. Look at this example: dates &lt;- c(20090101, &quot;2009-01-02&quot;, &quot;2009 01 03&quot;, &quot;2009-1-4&quot;, &quot;2009-1, 5&quot;, &quot;Created on 2009 1 6&quot;, &quot;200901 !!! 07&quot;) str(dates) #&gt; chr [1:7] &quot;20090101&quot; &quot;2009-01-02&quot; &quot;2009 01 03&quot; &quot;2009-1-4&quot; &quot;2009-1, 5&quot; ... ymd(dates) #&gt; [1] &quot;2009-01-01&quot; &quot;2009-01-02&quot; &quot;2009-01-03&quot; &quot;2009-01-04&quot; &quot;2009-01-05&quot; #&gt; [6] &quot;2009-01-06&quot; &quot;2009-01-07&quot; The first data entered was a number, but we already know that it coerces it to text. Then, we have different values entered, but all follow the same pattern. First is the year, then the month and then the day. When we know that first is the year, then month and then day we will use the ymd() function to convert all dates to ISO 8601 format. In the same way, we will have the following functions that we can use depending on the form in which we have the date from our source. In all cases it will be convenient for us to convert to ISO 8601 format. For example here we can see when it correctly recognizes the format and when the formatting fails. x &lt;- &quot;28/03/89&quot; ymd(x) #&gt; [1] NA mdy(x) #&gt; [1] NA ydm(x) #&gt; [1] NA myd(x) #&gt; [1] NA dmy(x) #&gt; [1] &quot;1989-03-28&quot; dym(x) #&gt; [1] NA Finally, in the same way that we can use these functions of days, months and years, we can also use to refer to hours, minutes and seconds. # Format with hours, minutes and seconds date_val &lt;- &quot;Feb/2/2012 12:34:56&quot; mdy_hms(date_val) #&gt; [1] &quot;2012-02-02 12:34:56 UTC&quot; # Additional data: Showing system date: now() #&gt; [1] &quot;2025-12-23 22:18:49 GMT&quot; 11.4 Exercises Before solving the following exercise run this Script: ventas &lt;- tibble( mes = c(&quot;Abril&quot;, &quot;Mayo&quot;, &quot;Junio&quot;), ventas = c(&quot;s/32,124&quot;, &quot;s/35,465&quot;, &quot;S/38,332&quot;), ganancias = c(&quot;s/8,120&quot;, &quot;s/9,432&quot;, &quot;s/10,543&quot;) ) From the ventas object convert the sales and profits columns to numeric values. Solution # Solution 1 ventas |&gt; mutate(across(c(2,3), ~parse_number(.))) # Alternative solution, longer ventas |&gt; mutate(across(c(2,3), ~str_replace_all(., &quot;\\\\S/|,&quot;, &quot;&quot;))) |&gt; mutate(across(c(2,3), ~as.numeric(.))) Given the vector universities: universidades &lt;- c(&quot;U. Católica de Chile&quot;, &quot;Univ Nacional Autónoma de México&quot;, &quot;Univ. Nacional de Ingeniería&quot;, &quot;Universidad de los Andes&quot;, &quot;U de Barcelona&quot;, &quot;California State University&quot;) Clean the data to obtain the full name as shown below: #&gt; [1] &quot;Universidad Católica de Chile&quot; #&gt; [2] &quot;Universidad Nacional Autónoma de México&quot; #&gt; [3] &quot;Universidad Nacional de Ingeniería&quot; #&gt; [4] &quot;Universidad de los Andes&quot; #&gt; [5] &quot;Universidad de Barcelona&quot; #&gt; [6] &quot;California State University&quot; Solution universidades |&gt; str_replace(&quot;^Univ\\\\.?\\\\s|^U\\\\.?\\\\s&quot;, &quot;Universidad &quot;) For the following exercises, we are going to work on the survey data conducted prior to Brexit in the UK. Run the Script first: library(rvest) library(tidyverse) url &lt;- &quot;https://en.wikipedia.org/w/index.php?title=Opinion_polling_for_the_United_Kingdom_European_Union_membership_referendum&amp;oldid=896735054&quot; tabla &lt;- read_html(url) |&gt; html_nodes(&quot;table&quot;) encuestas &lt;- tabla[[5]] |&gt; html_table(fill = TRUE) Update the encuestas object with the following names c(\"fecha\", \"permanecer\", \"salir\", \"no_decide\", \"spread\", \"muestra\", \"encuestadora\", \"tipo\", \"notas\"). Not all polls have a percentage value in the permanecer (remain) column. Filter the columns so that only values containing the % symbol are shown. Solution names(encuestas) &lt;- c(&quot;fecha&quot;, &quot;permanecer&quot;, &quot;salir&quot;, &quot;no_decide&quot;, &quot;spread&quot;, &quot;muestra&quot;, &quot;encuestadora&quot;, &quot;tipo&quot;, &quot;notas&quot;) encuestas &lt;- encuestas[str_detect(encuestas$permanecer, &quot;%&quot;), ] encuestas # If we want to validate the number of surveys: nrow(encuestas) Store the values of the permanecer column to the permanecer vector and convert the values to the numerical value of the percentage. That is, values from 0 to 1 (0.5 instead of 50%). Solution permanecer &lt;- encuestas$permanecer # Solution 1: porcentajes &lt;- parse_number(permanecer)/100 # Solution 1: temp &lt;- str_replace(permanecer, &quot;%&quot;, &quot;&quot;) porcentajes &lt;- as.numeric(temp)/100 # Solution 2: temp &lt;- str_remove(permanecer, &quot;%&quot;) porcentajes &lt;- as.numeric(temp)/100 We find in the no_decide column the value of “N/A” when by percentages of permanecer plus salir sum 100%. Therefore, no_decide should be zero in those cases and not “N/A”. Store the values of no_decide in the no_decide vector and transform the “N/A” values to 0% Solution no_decide &lt;- encuestas$no_decide str_replace(no_decide, &quot;N/A&quot;, &quot;0%&quot;) Create the function formato_porcentaje(cadena) where you consolidate the transformations performed in the previous exercises. Then test the function with the vector: c(\"13.5%\", \"N/A\", \"10%\") Solution formato_porcentaje &lt;- function(cadena){ cadena |&gt; str_replace(&quot;N/A&quot;, &quot;0%&quot;) |&gt; parse_number()/100 } # Function test: prueba &lt;- c(&quot;13.5%&quot;, &quot;N/A&quot;, &quot;10%&quot;) formato_porcentaje(prueba) Modify the columns of the encuestas table to change the necessary values from text to numbers. Solution encuestas &lt;- encuestas |&gt; mutate(across(c(&quot;permanecer&quot;, &quot;salir&quot;, &quot;no_decide&quot;, &quot;spread&quot;), ~formato_porcentaje(.))) |&gt; mutate(across(c(&quot;muestra&quot;), ~parse_number(.))) Import the following file containing covid cases reported by the Ministry of Health of Peru from the following route: “https://www.datosabiertos.gob.pe/sites/default/files/DATOSABIERTOS_SISCOVID.csv” into the covidPeru object. Convert the birth date column to date type and create a histogram with the ages of the infected. Solution url &lt;- &quot;https://www.datosabiertos.gob.pe/sites/default/files/DATOSABIERTOS_SISCOVID.csv&quot; covidPeru &lt;- read_csv(url) # We look for those that do not follow the ISO 8601 standard: index &lt;- str_detect(covidPeru$FECHA_NACIMIENTO, &quot;\\\\d{4}-\\\\d{2}-\\\\d{2}&quot;) covidPeru$FECHA_NACIMIENTO[!index] # We see dates in DD/MM/YYYY format # Reemplazamos a formato ISO 8601: covidPeru &lt;- covidPeru |&gt; mutate(across(&quot;FECHA_NACIMIENTO&quot;, ~str_replace(., &quot;(\\\\d{2})/(\\\\d{2})/(\\\\d{4})&quot;, &quot;\\\\3-\\\\2-\\\\1&quot;) )) # We search again for those that do not follow ISO 8601 standard: index &lt;- str_detect(covidPeru$FECHA_NACIMIENTO, &quot;\\\\d{4}-\\\\d{2}-\\\\d{2}&quot;) covidPeru$FECHA_NACIMIENTO[!index] # Convert column to date: covidPeru &lt;- covidPeru |&gt; mutate(across(&quot;FECHA_NACIMIENTO&quot;, ~ymd(.))) # Now that it is date format we create histogram: covidPeru |&gt; mutate(edad = year(now()) - year(FECHA_NACIMIENTO)) |&gt; pull(edad) |&gt; hist() 11.5 Text Mining: Word Cloud Text mining is the discovery by computer of new information, previously unknown, by automatically extracting information from different written resources. Written resources can be websites, books, chats, comments, emails, reviews, articles, etc. Thus, text mining, also known as text data mining, approximately equivalent to text analysis, is the process of deriving high-quality information from text. The first text mining technique we will learn will be the construction of word clouds. For this, we will need to install packages developed exclusively for text mining and some libraries for text treatment that we had already used like readr or stringr: install.packages(&quot;syuzhet&quot;) install.packages(&quot;tm&quot;) install.packages(&quot;wordcloud&quot;) library(syuzhet) # Functions get_ library(stringr) # Functions str_ library(tm) # text mining functions library(wordcloud) # Create cloud map 11.5.1 Importing data Word maps or word clouds allow us to quickly identify which are the words that are repeated most in a text. They are very useful when we have fields that come from a form, for example, filled out by customers and we want to know what is being talked about more. It also helps us to analyze content in some book, magazine, etc. We are going to analyze the work “Niebla” written by the author Miguel de Unamuno. We will obtain the text from the Project Gutenberg11 website, which gives us access to a library of around 60 thousand free books. We will import the text using the get_text_as_string() function from the syuzhet library to import all the text as a string. This function is very useful if we wish to import large files. Then, we will use the get_sentences() function, to create a vector of sentences from the initial text. url &lt;- &quot;http://www.gutenberg.org/files/49836/49836-0.txt&quot; work &lt;- get_text_as_string(url) sentences &lt;- get_sentences(work) 11.5.2 Text cleaning As we have learned previously, we do not have to go straight to analyze. Instead, we have to clean our data. The first thing we will do is eliminate the first rows that do not correspond to the work. # We eliminate first rows of notes, prologue, post-prologue total_lines &lt;- length(sentences) start_line &lt;- 115 end_line &lt;- total_lines - start_line clean_text &lt;- sentences[start_line:end_line] Next we will use a regex to detect special encoding characters, such as line breaks and tabulations. For this we will use the regex [[:cntrl:]]. Likewise, we will convert all words to lower case to facilitate comparisons between words. Finally, as we want to analyze the words, we eliminate all punctuation marks. clean_text &lt;- clean_text |&gt; str_replace_all(&quot;[[:cntrl:]]&quot;, &quot; &quot;) |&gt; str_to_lower() |&gt; removePunctuation() |&gt; str_replace_all(&quot;—&quot;, &quot; &quot;) On the other hand, the “tm” library, from text mining, provides us with functions and vectors to clean our data. We already used the removePunctuation() function. However, we also have the stopwords(\"spanish\") function that calls a vector with empty words, that is, those with little value for analysis, such as some prepositions and fillers. In addition, we will use the removeWords() function to remove all words found in our empty words vector. clean_text &lt;- removeWords(clean_text, words = stopwords(&quot;spanish&quot;)) Finally, we eliminate excessive empty spaces, some of them created by the previous transformations. clean_text &lt;- stripWhitespace(clean_text) 11.5.3 Creating the Corpus To be able to create a word map we need to apply the VectorSource() function to convert each row to a document and the Corpus() function that will allow us to create these documents as a data collection. collection &lt;- clean_text |&gt; VectorSource() |&gt; Corpus() We are ready to create our word map. For this we will use the wordcloud() library and the function of the same name. wordcloud(collection, min.freq = 5, max.words = 80, random.order = FALSE, colors = brewer.pal(name = &quot;Dark2&quot;, n = 8) ) 11.5.4 2nd Data Cleaning In text mining we will frequently obtain a result that still requires cleaning more data. For example, we still see words like pronouns of little interest for analysis. We will use the removeWords() function again, but this time with a custom vector of the words we wish to remove. to_remove &lt;- c(&quot;usted&quot;, &quot;pues&quot;, &quot;tal&quot;, &quot;tan&quot;, &quot;así&quot;, &quot;dijo&quot;, &quot;cómo&quot;, &quot;sino&quot;, &quot;entonces&quot;, &quot;aunque&quot;, &quot;don&quot;, &quot;doña&quot;) clean_text &lt;- removeWords(clean_text, words = to_remove) collection &lt;- clean_text |&gt; VectorSource() |&gt; Corpus() wordcloud(collection, min.freq = 5, max.words = 80, random.order = FALSE, colors = brewer.pal(name = &quot;Dark2&quot;, n = 8) ) Augusto and Eugenia, as we can assume, are the protagonists of Niebla and much of the action in this book occurs in the “house” of one or another protagonist, discussing relationships between “man” and “woman”. 11.5.5 Word frequency We already have a visual idea of the most used words. However, we could also know exactly how many times a certain word appeared. For this we have to convert our collection to a matrix. For this we use the functions together TermDocumentMatrix(), as.matrix() and rowSums() which will leave us with a vector with the word frequency. words_vec &lt;- collection |&gt; TermDocumentMatrix() |&gt; as.matrix() |&gt; rowSums() |&gt; sort(decreasing = TRUE) words_vec |&gt; head(20) #&gt; augusto eugenia mujer casa ahora bien hombre ser mismo vez #&gt; 361 200 183 126 120 119 119 98 98 90 #&gt; ojos vida sé cosas fué madre después pobre luego dos #&gt; 86 84 82 71 71 71 71 69 69 69 With this vector it is easy to convert it to data frame, given that we have the names and values, and visualize it. frequencies &lt;- data.frame( palabra = names(words_vec), frecuencia = words_vec ) # Visualization of top 10 words: frequencies[1:10,] |&gt; ggplot() + aes(frecuencia, y = reorder(palabra, frecuencia)) + geom_bar(stat = &quot;identity&quot;, color = &quot;white&quot;, fill = &quot;blue&quot;) + geom_text(aes(label = frecuencia, hjust = 1.5), color = &quot;white&quot;) + labs( x = NULL, y = &quot;Most used words in the work&quot; ) 11.6 Text Mining: Sentiment Analysis When we analyze texts we are not only going to want to know which are the words that are most utilized in texts, whether these are comments left by our customers, complaint requests, etc. It is also very useful to know the tone of the messages. This technique is known as sentiment analysis, which can be done very easily with the library that we have already used syuzhet. And what better place to analyze message tones than on Twitter. For this, we are going to download a history of Tweets from some Spanish-speaking character from the page vicinitas.io. This page allows us to download an excel given a public account: https://www.vicinitas.io/free-tools/download-user-tweets. For our example, we will use tweets from the account of the lawyer Rosa María Palacios12. For this example the excel has already been uploaded to Github. We will download that excel directly from there to our computer to a temporary file and then we will read it using read_excel(). url &lt;- &quot;https://dparedesi.github.io/DS-con-R/rmapalacios_user_tweets.xlsx&quot; # Create a temporary name &amp; path for our file. temp_file &lt;- tempfile() # Download the file to our temp download.file(url, temp_file) # Import the excel posts &lt;- read_excel(temp_file) # Remove the temporary file file.remove(temp_file) #&gt; [1] TRUE We have created our object posts, which has in the column Text the different tweets, retweets and replies, performed. Although we could do a data analysis using the other columns, we are going to focus on the content and tone of the Tweets. For this, we are going to eliminate Retweets and replies, keeping only Tweets. tweets &lt;- posts |&gt; filter(`Tweet Type` == &quot;Tweet&quot;) |&gt; pull(Text) With what was learned doing word maps, let’s create a map with the content of the publications. tweets_limpio &lt;- tweets |&gt; removePunctuation() |&gt; str_to_lower() |&gt; str_replace_all(&quot;[[:cntrl:]]&quot;, &quot; &quot;) |&gt; removeWords(words = stopwords(&quot;spanish&quot;)) |&gt; removeWords(words = c(&quot;usted&quot;, &quot;pues&quot;, &quot;tal&quot;, &quot;tan&quot;, &quot;así&quot;, &quot;dijo&quot;, &quot;cómo&quot;, &quot;sino&quot;, &quot;entonces&quot;, &quot;aunque&quot;, &quot;que&quot;)) collection &lt;- tweets_limpio |&gt; VectorSource() |&gt; Corpus() wordcloud(collection, min.freq = 5, max.words = 80, random.order = FALSE, colors = brewer.pal(name = &quot;Dark2&quot;, n = 8) ) She is a lawyer, which makes a lot of sense that she posts content of what can or cannot be done. We could be more rigorous and seek to achieve this combination by adding underscores if the pattern is detected, but for the moment we are going to focus on the tone. With our object tweets_limpio we can obtain what the tone is using the function get_nrc_sentiment(), which gives us a score for each row of the vector according to the NRC Emotion Lexicon13. The NRC Emotion Lexicon is a list of words and their associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive). result &lt;- get_nrc_sentiment(tweets_limpio, language = &quot;spanish&quot;) result |&gt; head(10) #&gt; anger anticipation disgust fear joy sadness surprise trust negative positive #&gt; 1 0 1 0 2 1 3 0 3 5 3 #&gt; 2 4 0 3 5 0 3 0 0 7 0 #&gt; 3 0 0 0 0 0 0 0 0 0 0 #&gt; 4 0 1 1 1 1 1 1 1 1 2 #&gt; 5 0 2 0 1 3 1 0 4 0 4 #&gt; 6 0 0 0 0 0 0 0 0 0 0 #&gt; 7 1 1 0 3 0 1 0 1 3 0 #&gt; 8 0 0 0 0 0 1 0 0 2 3 #&gt; 9 1 0 1 1 0 1 0 0 1 0 #&gt; 10 0 0 0 0 0 0 0 0 0 0 We can perform some transformations to this data frame, but first we are going to create a translation function. Given that, we still have to translate the headers. translate_emotions &lt;- function(cadena){ case_when( cadena == &quot;anger&quot; ~ &quot;Ira&quot;, cadena == &quot;anticipation&quot; ~ &quot;Anticipación&quot;, cadena == &quot;disgust&quot; ~ &quot;Aversión&quot;, cadena == &quot;fear&quot; ~ &quot;Miedo&quot;, cadena == &quot;joy&quot; ~ &quot;Alegría&quot;, cadena == &quot;sadness&quot; ~ &quot;Tristeza&quot;, cadena == &quot;surprise&quot; ~ &quot;Asombro&quot;, cadena == &quot;trust&quot; ~ &quot;Confianza&quot;, cadena == &quot;negative&quot; ~ &quot;Negativo&quot;, cadena == &quot;positive&quot; ~ &quot;Positivo&quot;, TRUE ~ cadena ) } Now, with our function ready, we can transform our object result to obtain the frequencies of each emotion and sentiment. # Summary of emotions/sentiments sentiments &lt;- result |&gt; pivot_longer(cols = everything(), names_to = &quot;sentimiento&quot;, values_to = &quot;count&quot;) |&gt; mutate(sentimiento = translate_emotions(sentimiento)) |&gt; group_by(sentimiento) |&gt; summarise(total = sum(count)) sentiments #&gt; # A tibble: 10 × 2 #&gt; sentimiento total #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Alegría 547 #&gt; 2 Anticipación 903 #&gt; 3 Asombro 421 #&gt; 4 Aversión 810 #&gt; 5 Confianza 1369 #&gt; 6 Ira 803 #&gt; 7 Miedo 1342 #&gt; 8 Negativo 2334 #&gt; 9 Positivo 2117 #&gt; 10 Tristeza 1177 We see that we have the 8 emotions plus the 2 sentiments. Let’s get the indices of the positive and negative sentiments: index &lt;- sentiments$sentimiento %in% c(&quot;Positivo&quot;, &quot;Negativo&quot;) This vector will serve us to be able to visualize separately the emotions and sentiments. # Visualization of emotions sentiments[!index,] |&gt; ggplot() + aes(sentimiento, total) + geom_bar(aes(fill = sentimiento), stat = &quot;identity&quot;) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + xlab(NULL) + ylab(&quot;Total&quot;) + ggtitle(&quot;Emotions in the Tweets of Rosa María Palacios&quot;) # Visualization of whether they are positive or negative sentiments: sentiments[index,] |&gt; ggplot() + aes(sentimiento, total) + geom_bar(aes(fill = sentimiento), stat = &quot;identity&quot;) + xlab(NULL) + ylab(&quot;Total&quot;) + ggtitle(&quot;Sentiments of the Tweets of Rosa María Palacios&quot;) This technique is very useful to us as a starting point for future analyses on the tonality of some determined text. 11.7 Exercises For these exercises we will use more books from Project Gutenberg. However, we will extract the text with an R library, the gutenbergr library. install.packages(&quot;gutenbergr&quot;) library(gutenbergr) # Tibble: list of books in Gutenberg.org gutenberg_metadata # List of books in Spanish gutenberg_works(languages = &quot;es&quot;) The gutenberg_download(id) function downloads the text in a tibble type object with one row for each line. Download the book “El ingenioso hidalgo don Quijote de la Mancha” to the descarga object. Store the text column in the obra object and report the first 50 lines of the object. Solution descarga &lt;- gutenberg_download(2000) obra &lt;- descarga$text obra[1:50] Due to the number of lines in this work, take a random sample of 1,000 lines and store it in the muestra object. Remove words, punctuation marks, line breaks and other elements learned during this chapter. Store this transformation in the muestra_limpia object. Solution muestra &lt;- sample(obra, 1000) muestra_limpia &lt;- muestra |&gt; str_replace(&quot;\\xe1&quot;, &quot;a&quot;) |&gt; # We remove accents str_replace(&quot;\\xe9&quot;, &quot;e&quot;) |&gt; # We remove accents str_replace(&quot;\\xed&quot;, &quot;i&quot;) |&gt; # We remove accents str_replace(&quot;\\xf3&quot;, &quot;o&quot;) |&gt; # We remove accents removePunctuation() |&gt; str_to_lower() |&gt; str_replace_all(&quot;[[:cntrl:]]&quot;, &quot; &quot;) |&gt; removeWords(words = stopwords(&quot;spanish&quot;)) |&gt; removeWords(words = c(&quot;usted&quot;, &quot;pues&quot;, &quot;tal&quot;, &quot;tan&quot;, &quot;así&quot;, &quot;dijo&quot;, &quot;cómo&quot;, &quot;sino&quot;, &quot;entonces&quot;, &quot;aunque&quot;, &quot;que&quot;)) Create a word map from the muestra_limpia object. Solution coleccion &lt;- muestra_limpia |&gt; VectorSource() |&gt; Corpus() wordcloud(coleccion, min.freq = 5, max.words = 80, random.order = FALSE, colors = brewer.pal(name = &quot;Dark2&quot;, n = 8) ) Perform a sentiment analysis of the sample obtained. Solution resultado &lt;- get_nrc_sentiment(muestra_limpia, language = &quot;spanish&quot;) sentimientos &lt;- resultado |&gt; pivot_longer(cols = everything(), names_to = &quot;sentimiento&quot;, values_to = &quot;cantidad&quot;) |&gt; mutate(sentimiento = trad_emociones(sentimiento)) |&gt; group_by(sentimiento) |&gt; summarise(total = sum(cantidad)) index &lt;- sentimientos$sentimiento %in% c(&quot;Positivo&quot;, &quot;Negativo&quot;) # Visualization of emotions sentimientos[!index,] |&gt; ggplot() + aes(sentimiento, total) + geom_bar(aes(fill = sentimiento), stat = &quot;identity&quot;) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + xlab(NULL) + ylab(&quot;Total&quot;) + ggtitle(&quot;Emociones en El Quijote&quot;) # Visualization of whether they are positive or negative sentiments: sentimientos[index,] |&gt; ggplot() + aes(sentimiento, total) + geom_bar(aes(fill = sentimiento), stat = &quot;identity&quot;) + xlab(NULL) + ylab(&quot;Total&quot;) + ggtitle(&quot;Sentimientos en El Quijote&quot;) https://stringr.tidyverse.org/articles/regular-expressions.html↩︎ www.gutenberg.org↩︎ https://twitter.com/rmapalacios↩︎ https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm↩︎ "],["introduction-2.html", "Introduction", " Introduction We have seen so far how to work with data. Performing analysis of what has happened allows us to take a determined action to change the course of a business. However, we can also use this data to predict. Predictive analysis is a technique that every Data Scientist must master, and Machine learning provides us with robust algorithms to be able to make predictions. Machine learning is the study of computer algorithms that improve automatically through experience. Applications range from data mining programs that discover general rules in large datasets, to information filtering systems that automatically learn user interests (Michell 1997). It is seen as a subset of artificial intelligence. Machine learning algorithms create a mathematical model based on sample data, known as “training data”, to make predictions or decisions without being explicitly programmed to do so. A good data scientist knows how to build prediction algorithms using machine learning. In the following chapters, we will see techniques both for discrete variables and for when we work with continuous variables from the two main machine learning approaches: supervised learning and unsupervised learning. Keep in mind that there are also other approaches, such as semi-supervised learning or reinforcement learning where the algorithm learns from a real or synthetic environment. These approaches will not be covered in this book, which focuses on the most commonly used approaches for starting out as a data scientist. References "],["supervised-learning.html", "Chapter 12 Supervised Learning 12.1 Classification and Regression 12.2 kNN: k-Nearest Neighbors 12.3 tidymodels Framework 12.4 Confusion Matrix 12.5 Exercises 12.6 Simple Linear Regression 12.7 Multiple Linear Regression 12.8 Standard Method for Evaluating Accuracy 12.9 Selection of the Most Optimal Model 12.10 Exercises 12.11 Ethics: Bias in Algorithmic Decision Making", " Chapter 12 Supervised Learning To understand supervised learning intuitively, we will use a daily example. We have all been to the doctor and at some point told them that we have a sore throat, headache, and fever. They will ask us a few more questions and then tell us what illness we might have and what treatment to follow. Intuitively, we know that the doctor had to train initially from classes and books showing past cases, and study which symptoms are signs of each disease. Then, they started to test what they learned on a group of patients during their internship. Finally, when they were already trained, they had the license to be able to apply this learning to patients in their office or hospital. This is an example of supervised learning because the training was performed from known data or inputs which are labeled (sore throat, headache, fever) with the purpose of obtaining a result or output that was also known and labeled (do they have the flu or not?). When a doctor tests what they learned, the patient inputs are known and also the output given by a more experienced doctor who can say how effective their training is. When the doctor goes out to see patients, they will only have labeled inputs with the purpose of predicting a labeled output. This is the logic that has been taken to computational algorithms. We can see it on Facebook, which collects a series of inputs, such as our likes, shares, etc., to predict what we might want to consume and shows it to us as a recommendation. And we will also see it in our work environment when we have inputs from our clients, such as consumption, purchasing power, place where they live, etc., to predict which of our products we predict they are more prone to buy and thus call them to offer that product. 12.1 Classification and Regression There are multiple supervised learning algorithms, but we will differentiate them into two according to the type of variable we handle. When the variable is discrete, we will call them classification. The examples above are proof of this. We have classified into two classes (flu or not) or several classes (product “x” to recommend). When the variable is continuous, we will call them regression. Predicting house prices given the characteristics of the house such as size, price, etc. is one of the common examples of regression. In the following sections, we will learn some algorithms indicating whether they are classification or regression. 12.2 kNN: k-Nearest Neighbors Let’s start with a simple but very useful classification algorithm, the k-Nearest Neighbors algorithm (kNN). 12.2.1 Two variables as input Let’s start by understanding it visually. Imagine that we have two variables as input and as output it gives us whether it is Red Class or Blue Class. This data is our training data. Now that we have our training data, we will start using the test data. As we want to predict the class, the output, we will see how one of these data points would look visually and paint it yellow. Next, we calculate the distance between this point and the other data points. We have traced only some distances, but we could do it with all of them. For this example, we will take the k = 3 nearest neighbors. We notice that if we focus only on the 3 nearest neighbors, there are more reds than blues, so our prediction will be that this point must be Class R (red). Calculating the distance on a Cartesian plane is relatively simple, we only have variables as input: on the x-axis and y-axis. However, the same logic can be taken to more variables. 12.2.2 Multiple variables as input Let’s see how it would be with 4 variables as input. We are going to work again with the iris data frame, which, as we will recall, has 4 attributes of a plant and the last column is the species to which it belongs. data(iris) iris |&gt; head(10) #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; 1 5.1 3.5 1.4 0.2 setosa #&gt; 2 4.9 3.0 1.4 0.2 setosa #&gt; 3 4.7 3.2 1.3 0.2 setosa #&gt; 4 4.6 3.1 1.5 0.2 setosa #&gt; 5 5.0 3.6 1.4 0.2 setosa #&gt; 6 5.4 3.9 1.7 0.4 setosa #&gt; 7 4.6 3.4 1.4 0.3 setosa #&gt; 8 5.0 3.4 1.5 0.2 setosa #&gt; 9 4.4 2.9 1.4 0.2 setosa #&gt; 10 4.9 3.1 1.5 0.1 setosa The idea is as follows, we will take training data, 50 data points. From this data, we have the 4 input attributes and the last column is the output, the species. We will use the kNN algorithm taking this training data as input to create our model. Then, with testing data, another 50 data points, we will test our model. Let’s start by taking a random sample of 100 records and separate half for training and half for testing. Since we have 150 data points in our data frame, let’s take a sample of the indices. In this case, we are going to use the set.seed(n) function to force the random sample values to be the same always. Thus, we can all obtain the same results and the explanation in the book in these chapters is consistent with the results that each reader obtains. For a real exercise, we should not include that line. It is recommended to read the documentation ?set.seed(). # 28 is the author&#39;s birthday set.seed(28) sample_idx &lt;- sample(150, 100) train_idx &lt;- sample(sample_idx, 50) test_idx &lt;- sample_idx[!sample_idx %in% train_idx] Now that we have the indices we can build our training data and our test. iris_train &lt;- iris[train_idx, ] iris_test &lt;- iris[test_idx, ] iris_train_input &lt;- iris_train[, -5] iris_train_output &lt;- iris_train[, 5] iris_test_input &lt;- iris_test[, -5] iris_test_output &lt;- iris_test[, 5] Although we could build the algorithms to calculate the minimum distances for each point, R provides us with libraries that facilitate the creation of these models. To do this, we will load the class library, which will allow us to execute kNN quickly. install.packages(&quot;class&quot;) library(class) This library provides us with the knn() function, which will take the training data to create the model and once the model is created it will take the test data to predict the output for our test data. iris_test_output_kNN &lt;- knn(train = iris_train_input, cl = iris_train_output, test = iris_test_input, k = 3) iris_test_output_kNN #&gt; [1] versicolor versicolor versicolor versicolor setosa versicolor #&gt; [7] virginica virginica virginica virginica versicolor versicolor #&gt; [13] virginica versicolor versicolor versicolor setosa versicolor #&gt; [19] versicolor virginica virginica setosa versicolor versicolor #&gt; [25] versicolor virginica setosa setosa versicolor versicolor #&gt; [31] virginica setosa setosa virginica virginica setosa #&gt; [37] setosa virginica setosa versicolor setosa virginica #&gt; [43] setosa setosa setosa virginica virginica versicolor #&gt; [49] virginica versicolor #&gt; Levels: setosa versicolor virginica Thus, the knn function throws us the prediction just by entering the training data as attributes, the test inputs, and how many nearest neighbors it will look for (k). And not only that, we can compare our prediction with the test output to see how accurate (accuracy) our model is. To do this, we calculate the percentage of correct predictions regarding the test output. mean(iris_test_output_kNN == iris_test_output) #&gt; [1] 0.94 In addition, we can place a summary in a table, also known as a confusion matrix, to see how many predicted values were equal to the real ones using the table() function. table(iris_test_output_kNN, iris_test_output) #&gt; iris_test_output #&gt; iris_test_output_kNN setosa versicolor virginica #&gt; setosa 14 0 0 #&gt; versicolor 0 18 2 #&gt; virginica 0 1 15 Let’s interpret this result cell by cell: Our kNN model predicted 14 values as species “setosa” and it turns out that in our test the real value, output, was also setosa. Our model predicted 20 as species versicolor. However, in the real-test data, of those 20, only 18 are versicolor and 2 are virginica. Our model predicted 16 as species virginica. However, in the real-test data, of those 16, only 15 are virginica. 12.2.3 Diverse values of k So far we have only used a single value for k, 3 nearest neighbors. However, we could see the accuracy for different values of k. Since we have 50 values in our training data, we will see the hits taking a maximum of 50 nearest neighbors. k &lt;- 1:50 result_df &lt;- data.frame(k, precision = 0) for(n in k){ iris_test_output_kNN &lt;- knn(train = iris_train_input, cl = iris_train_output, test = iris_test_input, k = n) result_df$precision[n] &lt;- mean(iris_test_output_kNN == iris_test_output) } result_df |&gt; ggplot() + aes(k, precision) + geom_line() As we can see, for this case, starting from a certain number of nearest neighbors, the success rate of our algorithm begins to reduce. It will depend on each case to choose the best “k” for our model. We have thus built our first machine learning model. 12.3 tidymodels Framework Now that we have created our first machine learning model, we have seen ourselves with many lines of code. For example, to split the sample into training and test, to calculate the optimal “k”, etc. To make the work easier, we will use the tidymodels framework. tidymodels14 is a collection of packages for modeling and machine learning using tidyverse principles. It provides a unified, modern interface for: rsample: Data splitting and resampling recipes: Feature engineering and preprocessing parsnip: Unified model specification tune: Hyperparameter tuning yardstick: Model evaluation metrics workflows: Bundling recipes and models together install.packages(&#39;tidymodels&#39;) library(tidymodels) #&gt; ── Attaching packages ─────────── tidymodels 1.4.1 ── #&gt; ✔ broom 1.0.11 ✔ tailor 0.1.0 #&gt; ✔ dials 1.4.2 ✔ tune 2.0.1 #&gt; ✔ infer 1.1.0 ✔ workflows 1.3.0 #&gt; ✔ parsnip 1.4.0 ✔ workflowsets 1.1.1 #&gt; ✔ recipes 1.3.1 ✔ yardstick 1.3.2 #&gt; ✔ rsample 1.3.1 #&gt; ── Conflicts ────────────── tidymodels_conflicts() ── #&gt; ✖ NLP::annotate() masks ggplot2::annotate() #&gt; ✖ rsample::calibration() masks caret::calibration() #&gt; ✖ scales::discard() masks purrr::discard() #&gt; ✖ Matrix::expand() masks tidyr::expand() #&gt; ✖ dplyr::filter() masks stats::filter() #&gt; ✖ recipes::fixed() masks stringr::fixed() #&gt; ✖ dplyr::lag() masks stats::lag() #&gt; ✖ caret::lift() masks purrr::lift() #&gt; ✖ Matrix::pack() masks tidyr::pack() #&gt; ✖ rsample::permutations() masks gtools::permutations() #&gt; ✖ yardstick::precision() masks caret::precision() #&gt; ✖ dials::prune() masks dendextend::prune() #&gt; ✖ yardstick::recall() masks caret::recall() #&gt; ✖ yardstick::sensitivity() masks caret::sensitivity() #&gt; ✖ yardstick::spec() masks readr::spec() #&gt; ✖ yardstick::specificity() masks caret::specificity() #&gt; ✖ recipes::step() masks stats::step() #&gt; ✖ Matrix::unpack() masks tidyr::unpack() #&gt; ✖ recipes::update() masks Matrix::update(), stats::update() We are going to do another example with k-nearest neighbors, but this time using the functions of the Caret library. The data for this example will be obtained from the ISLR library, which contains the daily percentage returns for the S&amp;P 500 stock index between 2001 and 2005. This data frame has 8 columns that we will use as input and the last column that has two classes (whether the index goes up or down) that we will use as output (See ?Smarket). install.packages(&quot;ISLR&quot;) library(ISLR) data(Smarket) # Data frame that we will use Smarket |&gt; head(10) #&gt; Year Lag1 Lag2 Lag3 Lag4 Lag5 Volume Today Direction #&gt; 1 2001 0.381 -0.192 -2.624 -1.055 5.010 1.1913 0.959 Up #&gt; 2 2001 0.959 0.381 -0.192 -2.624 -1.055 1.2965 1.032 Up #&gt; 3 2001 1.032 0.959 0.381 -0.192 -2.624 1.4112 -0.623 Down #&gt; 4 2001 -0.623 1.032 0.959 0.381 -0.192 1.2760 0.614 Up #&gt; 5 2001 0.614 -0.623 1.032 0.959 0.381 1.2057 0.213 Up #&gt; 6 2001 0.213 0.614 -0.623 1.032 0.959 1.3491 1.392 Up #&gt; 7 2001 1.392 0.213 0.614 -0.623 1.032 1.4450 -0.403 Down #&gt; 8 2001 -0.403 1.392 0.213 0.614 -0.623 1.4078 0.027 Up #&gt; 9 2001 0.027 -0.403 1.392 0.213 0.614 1.1640 1.303 Up #&gt; 10 2001 1.303 0.027 -0.403 1.392 0.213 1.2326 0.287 Up # We make some translations for ease of analysis Smarket &lt;- Smarket |&gt; rename(Direction = Direction) |&gt; mutate(Direction = ifelse(Direction == &quot;Up&quot;, &quot;Sube&quot;, &quot;Baja&quot;)) |&gt; mutate(across(c(&quot;Direction&quot;), ~as.factor(.))) 12.3.1 Creation of training and test data From the total of our data frame, we will split a part of the data for training and the other to do the tests. tidymodels provides the initial_split() function from the rsample package which creates a clean split object. We allocate 75% of the data for training using the prop argument, and we can use strata to ensure balanced class distribution. set.seed(28) data_split &lt;- initial_split(Smarket, prop = 0.75, strata = Direction) SP_train &lt;- training(data_split) SP_test &lt;- testing(data_split) # Check the split nrow(SP_train) #&gt; [1] 937 nrow(SP_test) #&gt; [1] 313 This function makes sampling data much simpler and returns a split object that we can use with training() and testing() accessor functions. 12.3.2 Training our prediction algorithm In tidymodels, we build models in a structured way using three key components: 1. Model specification (parsnip): Define the type of model and its engine 2. Recipe (recipes): Define preprocessing steps 3. Workflow (workflows): Bundle recipe and model together Let’s start by specifying our k-nearest neighbors model. We use tune() as a placeholder for the neighbors parameter to indicate we want to find the optimal value. # Model specification knn_spec &lt;- nearest_neighbor(neighbors = tune()) |&gt; set_engine(&quot;kknn&quot;) |&gt; set_mode(&quot;classification&quot;) knn_spec #&gt; K-Nearest Neighbor Model Specification (classification) #&gt; #&gt; Main Arguments: #&gt; neighbors = tune() #&gt; #&gt; Computational engine: kknn 12.3.3 Data Pre-processing with Recipes tidymodels uses recipes for preprocessing. The scale method (division by standard deviation) and centering (subtraction of the mean) are implemented with step_normalize(). # Define preprocessing recipe knn_recipe &lt;- recipe(Direction ~ ., data = SP_train) |&gt; step_normalize(all_numeric_predictors()) knn_recipe #&gt; #&gt; ── Recipe ─────────────────────────────────────────── #&gt; #&gt; ── Inputs #&gt; Number of variables by role #&gt; outcome: 1 #&gt; predictor: 8 #&gt; #&gt; ── Operations #&gt; • Centering and scaling for: #&gt; all_numeric_predictors() 12.3.4 Creating a Workflow A workflow bundles the recipe and model specification together for easy training and prediction. # Bundle into workflow knn_workflow &lt;- workflow() |&gt; add_recipe(knn_recipe) |&gt; add_model(knn_spec) knn_workflow #&gt; ══ Workflow ═════════════════════════════════════════ #&gt; Preprocessor: Recipe #&gt; Model: nearest_neighbor() #&gt; #&gt; ── Preprocessor ───────────────────────────────────── #&gt; 1 Recipe Step #&gt; #&gt; • step_normalize() #&gt; #&gt; ── Model ──────────────────────────────────────────── #&gt; K-Nearest Neighbor Model Specification (classification) #&gt; #&gt; Main Arguments: #&gt; neighbors = tune() #&gt; #&gt; Computational engine: kknn 12.3.5 Parameter Tuning with Cross-Validation One of the most important parts of training machine learning models is tuning the parameters. We use vfold_cv() to create cross-validation folds and tune_grid() to search for the best hyperparameters. set.seed(28) # Create 5-fold cross-validation folds &lt;- vfold_cv(SP_train, v = 5, strata = Direction) # Create a grid of k values to try k_grid &lt;- grid_regular(neighbors(range = c(1, 50)), levels = 20) # Tune the model knn_tune_results &lt;- tune_grid( knn_workflow, resamples = folds, grid = k_grid ) #&gt; #&gt; Attaching package: &#39;kknn&#39; #&gt; The following object is masked from &#39;package:caret&#39;: #&gt; #&gt; contr.dummy knn_tune_results #&gt; # Tuning results #&gt; # 5-fold cross-validation using stratification #&gt; # A tibble: 5 × 4 #&gt; splits id .metrics .notes #&gt; &lt;list&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 &lt;split [748/189]&gt; Fold1 &lt;tibble [60 × 5]&gt; &lt;tibble [0 × 4]&gt; #&gt; 2 &lt;split [750/187]&gt; Fold2 &lt;tibble [60 × 5]&gt; &lt;tibble [0 × 4]&gt; #&gt; 3 &lt;split [750/187]&gt; Fold3 &lt;tibble [60 × 5]&gt; &lt;tibble [0 × 4]&gt; #&gt; 4 &lt;split [750/187]&gt; Fold4 &lt;tibble [60 × 5]&gt; &lt;tibble [0 × 4]&gt; #&gt; 5 &lt;split [750/187]&gt; Fold5 &lt;tibble [60 × 5]&gt; &lt;tibble [0 × 4]&gt; We can visualize the tuning results using autoplot(): autoplot(knn_tune_results) We can see the accuracy for each value of “k”. The show_best() function shows us the top performing values: show_best(knn_tune_results, metric = &quot;accuracy&quot;) #&gt; # A tibble: 5 × 7 #&gt; neighbors .metric .estimator mean n std_err .config #&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 44 accuracy binary 0.905 5 0.0114 pre0_mod18_post0 #&gt; 2 42 accuracy binary 0.904 5 0.0116 pre0_mod17_post0 #&gt; 3 37 accuracy binary 0.902 5 0.0115 pre0_mod15_post0 #&gt; 4 31 accuracy binary 0.902 5 0.0112 pre0_mod13_post0 #&gt; 5 47 accuracy binary 0.901 5 0.0119 pre0_mod19_post0 12.3.6 Finalizing the Model Once we’ve found the best hyperparameters, we finalize our workflow with those values: # Select the best k value best_k &lt;- select_best(knn_tune_results, metric = &quot;accuracy&quot;) best_k #&gt; # A tibble: 1 × 2 #&gt; neighbors .config #&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 44 pre0_mod18_post0 # Finalize the workflow with the best parameters final_knn_workflow &lt;- finalize_workflow(knn_workflow, best_k) # Fit the final model on the entire training set SP_knn_trained &lt;- fit(final_knn_workflow, data = SP_train) SP_knn_trained #&gt; ══ Workflow [trained] ═══════════════════════════════ #&gt; Preprocessor: Recipe #&gt; Model: nearest_neighbor() #&gt; #&gt; ── Preprocessor ───────────────────────────────────── #&gt; 1 Recipe Step #&gt; #&gt; • step_normalize() #&gt; #&gt; ── Model ──────────────────────────────────────────── #&gt; #&gt; Call: #&gt; kknn::train.kknn(formula = ..y ~ ., data = data, ks = min_rows(44L, data, 5)) #&gt; #&gt; Type of response variable: nominal #&gt; Minimal misclassification: 0.0864461 #&gt; Best kernel: optimal #&gt; Best k: 44 We see the substantial improvement now that we have adjusted some parameters and made it reprocess first. Note that each time we adjust parameters, the value of “k” can change until the most optimal one is found. In this case, it changed to k = 29. This does not mean that the lower the “k”, the better the algorithm, only that it is the most optimal for this particular case with these adjustments made. 12.3.7 Testing the prediction model We already have our model trained and ready to test it. tidymodels makes it easy to make predictions using the augment() function which adds predictions directly to our test data. # Make predictions on test data SP_predictions &lt;- augment(SP_knn_trained, new_data = SP_test) # View predictions SP_predictions |&gt; select(Direction, .pred_class, .pred_Baja, .pred_Sube) |&gt; head(10) #&gt; # A tibble: 10 × 4 #&gt; Direction .pred_class .pred_Baja .pred_Sube #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Sube Sube 0.243 0.757 #&gt; 2 Sube Sube 0.404 0.596 #&gt; 3 Baja Baja 0.611 0.389 #&gt; 4 Baja Baja 0.977 0.0231 #&gt; 5 Sube Sube 0.230 0.770 #&gt; 6 Baja Sube 0.472 0.528 #&gt; 7 Baja Baja 0.955 0.0447 #&gt; 8 Sube Sube 0.0361 0.964 #&gt; 9 Baja Baja 0.522 0.478 #&gt; 10 Baja Baja 1 0 The augment() function adds three columns: .pred_class (the predicted class), and probability columns for each class (.pred_Baja and .pred_Sube). This makes it very easy to compare predictions with actual values. As we can see, for each test value the model calculates the estimated probability for each class. The algorithm assigns the class with the highest probability. 12.3.8 Model Evaluation with yardstick To evaluate our model, we use the yardstick package. The conf_mat() function creates a confusion matrix, and we can calculate various metrics like accuracy, sensitivity, and specificity. # Confusion matrix SP_predictions |&gt; conf_mat(truth = Direction, estimate = .pred_class) #&gt; Truth #&gt; Prediction Baja Sube #&gt; Baja 132 4 #&gt; Sube 19 158 # Calculate accuracy SP_predictions |&gt; accuracy(truth = Direction, estimate = .pred_class) #&gt; # A tibble: 1 × 3 #&gt; .metric .estimator .estimate #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 accuracy binary 0.927 # Calculate multiple metrics at once SP_predictions |&gt; metrics(truth = Direction, estimate = .pred_class) #&gt; # A tibble: 2 × 3 #&gt; .metric .estimator .estimate #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 accuracy binary 0.927 #&gt; 2 kap binary 0.852 We obtain the accuracy as well as other metrics. The yardstick package provides many evaluation functions including sens() (sensitivity), spec() (specificity), precision(), recall(), and more. 12.4 Confusion Matrix We have already used confusion matrices in our two previous examples. Now it is our turn to properly understand its definition as well as some of the evaluation metrics of this matrix. A confusion matrix, also known as an error matrix, allows us to visualize the performance of an algorithm, generally a supervised learning one (in unsupervised learning it is generally called a matching matrix). Each row of the matrix represents the instances in a predicted class, while each column represents the instances in a real class (or vice versa). The name derives from the fact that it makes it easy to see if the system confuses two classes (i.e., commonly mislabeling one as another). Binary classifications, when the outcome can take only two classes, yield this following confusion matrix. 12.4.1 Accuracy We have already been using this term in our examples. The accuracy of the model can be calculated from the confusion matrix: \\(Accuracy=\\frac{VP+VN}{VP+VN+FP+FN}\\) The accuracy of the model is the proportion of times the algorithm predicted correctly, regarding the total data evaluated. 12.4.2 Sensitivity Sensitivity (also called true positive rate, recall, or probability of detection in some fields) measures the proportion of real positives that are correctly identified as such (for example, the percentage of sick people who are correctly identified as having the condition). \\(Sensitivity=\\frac{VP}{VP+FN}\\) 12.4.3 Specificity Specificity (also called true negative rate) measures the proportion of real negatives that are correctly identified as such (for example, the percentage of healthy people who are correctly identified as not having the condition). \\(Specificity=\\frac{VN}{VN+FP}\\) 12.5 Exercises Using the tidymodels library, partition the iris data frame in such a way as to have 70% training data and 30% test data. Solution iris_split &lt;- initial_split(iris, prop = 0.7, strata = Species) iris_train &lt;- training(iris_split) iris_test &lt;- testing(iris_split) Using tidymodels and the training data obtained in the previous exercise, create a k-nearest neighbor model with tuning. Plot the result. Solution # Model specification iris_knn_spec &lt;- nearest_neighbor(neighbors = tune()) |&gt; set_engine(&quot;kknn&quot;) |&gt; set_mode(&quot;classification&quot;) # Recipe with preprocessing iris_recipe &lt;- recipe(Species ~ ., data = iris_train) |&gt; step_normalize(all_numeric_predictors()) # Workflow iris_workflow &lt;- workflow() |&gt; add_recipe(iris_recipe) |&gt; add_model(iris_knn_spec) # Cross-validation and tuning iris_folds &lt;- vfold_cv(iris_train, v = 5) iris_tune &lt;- tune_grid(iris_workflow, resamples = iris_folds, grid = 20) autoplot(iris_tune) Use the model created in the previous exercise to predict the outputs of the test object. Report the confusion matrix. Solution # Finalize model with best k best_k &lt;- select_best(iris_tune, metric = &quot;accuracy&quot;) final_iris_wf &lt;- finalize_workflow(iris_workflow, best_k) iris_fit &lt;- fit(final_iris_wf, data = iris_train) # Predict and evaluate iris_predictions &lt;- augment(iris_fit, new_data = iris_test) iris_predictions |&gt; conf_mat(truth = Species, estimate = .pred_class) 12.6 Simple Linear Regression Now we have to predict on continuous variables, the supervision algorithms for these cases are called regression. To understand linear regression we are going to start with an example with a single variable as input, this is known as Simple Linear Regression. To do this we are going to use data from the HistData library where we will find a dataset that enumerates the individual observations of 934 children in 205 families stored in the object GaltonFamilies. install.packages(&quot;HistData&quot;) library(HistData) data(GaltonFamilies) # We make some filters to have one dad and one son per family heights_df &lt;- GaltonFamilies |&gt; filter(gender == &quot;male&quot;) |&gt; group_by(family) |&gt; slice_sample(n = 1) |&gt; # random sample of 1 son per family ungroup() |&gt; select(father, childHeight) |&gt; rename(son = childHeight, father = father) |&gt; mutate(father = father/39.37) |&gt; # From inches to meters mutate(son = son/39.37) # From inches to meters Visually we could see if there is a relationship between the heights of dad and son: heights_df |&gt; ggplot() + aes(father, son) + geom_point() + geom_abline(lty = 2) As we can see, there is a positive correlation, such that the taller the father, the son grows to be taller as an adult. This line, however, is nothing more than a default line. The challenge lies in finding which line minimizes the distance of the points to this line, known as error minimization. We could try to predict the height the son will have from the father’s height using the equation of this line: \\(Y = \\beta_0+\\beta_1X\\) Where \\(X\\) is an independent, explanatory variable, in this case the dad’s height. \\(\\beta_1\\) is a parameter that measures the influence that the explanatory variable has on the dependent variable \\(Y\\) and \\(\\beta_0\\) is the intercept or constant term. In our case, the son’s height. In statistics, linear regression or linear adjustment is a mathematical model used to approximate the dependency relationship between a dependent variable \\(Y\\) and the independent variables \\(X_i\\). Thus, our problem boils down to training our model to find the values of the intercept, \\(\\beta_0\\), and the value of the parameter accompanying \\(X_1\\), \\(\\beta_1\\), to then use these data as prediction in our test data. heights_split &lt;- initial_split(heights_df, prop = 0.5) heights_train &lt;- training(heights_split) heights_test &lt;- testing(heights_split) Now that we have our data we can train our model using tidymodels. We specify a linear regression model with linear_reg(). # Model specification lm_spec &lt;- linear_reg() |&gt; set_engine(&quot;lm&quot;) |&gt; set_mode(&quot;regression&quot;) # Recipe lm_recipe &lt;- recipe(son ~ father, data = heights_train) # Workflow lm_workflow &lt;- workflow() |&gt; add_recipe(lm_recipe) |&gt; add_model(lm_spec) # Cross-validation heights_folds &lt;- vfold_cv(heights_train, v = 10) lm_results &lt;- fit_resamples(lm_workflow, resamples = heights_folds) # View results collect_metrics(lm_results) #&gt; # A tibble: 2 × 6 #&gt; .metric .estimator mean n std_err .config #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 rmse standard 0.0584 10 0.00362 pre0_mod0_post0 #&gt; 2 rsq standard 0.362 10 0.0821 pre0_mod0_post0 We see as main results the RMSE, which stands for root mean square error, and is the value that linear regression seeks to minimize. In addition, we have the R squared or \\(R^2\\), which is the coefficient of determination which determines the quality of the model to replicate the results. The higher and closer to 1, the better the quality of the model. Now let’s fit the final model and make predictions: # Fit final model heights_fit &lt;- fit(lm_workflow, data = heights_train) # Make predictions heights_predictions &lt;- augment(heights_fit, new_data = heights_test) # Calculate RMSE heights_predictions |&gt; rmse(truth = son, estimate = .pred) #&gt; # A tibble: 1 × 3 #&gt; .metric .estimator .estimate #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 rmse standard 0.0642 If we wish we can also report the coefficients of the equation and visualize them: \\(Y = \\beta_0+\\beta_1X\\) # Extract model coefficients heights_fit |&gt; extract_fit_parsnip() |&gt; tidy() #&gt; # A tibble: 2 × 5 #&gt; term estimate std.error statistic p.value #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 (Intercept) 0.644 0.183 3.51 0.000708 #&gt; 2 father 0.631 0.104 6.05 0.0000000361 model_coefs &lt;- heights_fit |&gt; extract_fit_parsnip() |&gt; tidy() intercept_val &lt;- model_coefs$estimate[1] slope_val &lt;- model_coefs$estimate[2] #Visualization heights_df |&gt; ggplot() + aes(father, son) + geom_point() + geom_abline(lty = 2, intercept = intercept_val, slope = slope_val, color = &quot;red&quot;) 12.7 Multiple Linear Regression Now that we know linear regression we can execute a multiple linear regression model, which involves more than 1 variable as input. To do this, we will use the diamonds dataset containing the prices and other attributes of almost 54,000 diamonds. library(ggplot2) data(&quot;diamonds&quot;) diamonds &lt;- diamonds |&gt; rename(price = price) diamonds |&gt; head(10) #&gt; # A tibble: 10 × 10 #&gt; carat cut color clarity depth table price x y z #&gt; &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 #&gt; 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 #&gt; 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 #&gt; 4 0.29 Premium I VS2 62.4 58 334 4.2 4.23 2.63 #&gt; 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 #&gt; 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 #&gt; 7 0.24 Very Good I VVS1 62.3 57 336 3.95 3.98 2.47 #&gt; 8 0.26 Very Good H SI1 61.9 55 337 4.07 4.11 2.53 #&gt; 9 0.22 Fair E VS2 65.1 61 337 3.87 3.78 2.49 #&gt; 10 0.23 Very Good H VS1 59.4 61 338 4 4.05 2.39 We split the data in two taking 70% of data for training: set.seed(28) diamonds_split &lt;- initial_split(diamonds, prop = 0.7, strata = price) diamonds_train &lt;- training(diamonds_split) diamonds_test &lt;- testing(diamonds_split) We now create our multiple linear regression model and report both the error results and the coefficients of the linear equation using a tidymodels workflow. # Model specification diamonds_spec &lt;- linear_reg() |&gt; set_engine(&quot;lm&quot;) |&gt; set_mode(&quot;regression&quot;) # Recipe diamonds_recipe &lt;- recipe(price ~ ., data = diamonds_train) # Workflow diamonds_workflow &lt;- workflow() |&gt; add_recipe(diamonds_recipe) |&gt; add_model(diamonds_spec) # Cross-validation diamonds_folds &lt;- vfold_cv(diamonds_train, v = 10) diamonds_results &lt;- fit_resamples(diamonds_workflow, resamples = diamonds_folds) collect_metrics(diamonds_results) #&gt; # A tibble: 2 × 6 #&gt; .metric .estimator mean n std_err .config #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 rmse standard 1136. 10 19.7 pre0_mod0_post0 #&gt; 2 rsq standard 0.919 10 0.00256 pre0_mod0_post0 We see that it gives us the RMSE and an R squared quite closer to 1, which denotes a high quality of the model to replicate the results. Let’s use our model to predict the prices of the test data. # Fit final model library(discrim) #&gt; #&gt; Attaching package: &#39;discrim&#39; #&gt; The following object is masked from &#39;package:dials&#39;: #&gt; #&gt; smoothness diamonds_fit &lt;- fit(diamonds_workflow, data = diamonds_train) # Extract coefficients diamonds_fit |&gt; extract_fit_parsnip() |&gt; tidy() #&gt; # A tibble: 24 × 5 #&gt; term estimate std.error statistic p.value #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 (Intercept) 6975. 473. 14.7 5.45e- 49 #&gt; 2 carat 11437. 60.7 188. 0 #&gt; 3 cut.L 571. 27.1 21.1 7.51e- 98 #&gt; 4 cut.Q -305. 21.7 -14.0 1.07e- 44 #&gt; 5 cut.C 139. 18.6 7.48 7.72e- 14 #&gt; 6 cut^4 -23.2 14.8 -1.56 1.18e- 1 #&gt; 7 color.L -1980. 20.8 -95.1 0 #&gt; 8 color.Q -685. 19.0 -36.1 6.69e-281 #&gt; 9 color.C -186. 17.7 -10.5 6.23e- 26 #&gt; 10 color^4 36.8 16.2 2.27 2.33e- 2 #&gt; # ℹ 14 more rows # Prediction and Error calculation diamonds_predictions &lt;- augment(diamonds_fit, new_data = diamonds_test) # Mean Squared Error Calculation RMSE: diamonds_predictions |&gt; rmse(truth = price, estimate = .pred) #&gt; # A tibble: 1 × 3 #&gt; .metric .estimator .estimate #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 rmse standard 1119. Thus, we have learned to perform one more machine learning model: linear regression, both simple and multiple. 12.8 Standard Method for Evaluating Accuracy Now that we know how to build models we will apply metrics that allow us better accuracy in classification models for two classes. To do this let’s recall the results of the model we created using the k-nearest neighbors algorithm to predict if the S&amp;P index goes up or down. SP_knn_trained #&gt; ══ Workflow [trained] ═══════════════════════════════ #&gt; Preprocessor: Recipe #&gt; Model: nearest_neighbor() #&gt; #&gt; ── Preprocessor ───────────────────────────────────── #&gt; 1 Recipe Step #&gt; #&gt; • step_normalize() #&gt; #&gt; ── Model ──────────────────────────────────────────── #&gt; #&gt; Call: #&gt; kknn::train.kknn(formula = ..y ~ ., data = data, ks = min_rows(44L, data, 5)) #&gt; #&gt; Type of response variable: nominal #&gt; Minimal misclassification: 0.0864461 #&gt; Best kernel: optimal #&gt; Best k: 44 In the penultimate line it can be read that accuracy (accuracy) was used to select the most optimal model using the largest value. However, this is not the only way to determine which is the most optimal model. Let’s remember how accuracy (accuracy) is calculated by default, we have used the simple rule that if the probability of it being of a certain class is more than 50% then that class is assigned and then we calculate the proportion of hits among the total cases. However, it doesn’t have to be 50%, we could be more demanding and indicate that if the probability is greater than 60% or 80% then a certain class is assigned. We see that there are different probabilities and that would give us different accuracy. This is how the area under the Receiver Operating Characteristic curve indicator arises, ROC (Fawcett 2005). This indicator measures how well a model can distinguish between two classes and is considered the standard method for evaluating the accuracy of predictive distribution models (Jorge M. Lobo 2007) and calculates accuracies not only for when we discriminate starting from 50%, but for more probability values. To use this metric we will modify our control parameters adding three attributes that will allow calculating the ROC. SP2_ctrl &lt;- metric_set(roc_auc, accuracy) # We define folds set.seed(28) SP2_folds &lt;- vfold_cv(SP_train, v = 5, strata = Direction) With these modified parameters we will proceed to re-train our model selecting by ROC AUC. set.seed(28) # Tune grid specifying ROC as the metric to optimize SP2_knn_res &lt;- tune_grid( knn_workflow, resamples = SP2_folds, grid = 20, metrics = SP2_ctrl ) show_best(SP2_knn_res, metric = &quot;roc_auc&quot;) #&gt; # A tibble: 5 × 7 #&gt; neighbors .metric .estimator mean n std_err .config #&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 15 roc_auc binary 0.965 5 0.00586 pre0_mod13_post0 #&gt; 2 13 roc_auc binary 0.962 5 0.00632 pre0_mod12_post0 #&gt; 3 12 roc_auc binary 0.960 5 0.00667 pre0_mod11_post0 #&gt; 4 11 roc_auc binary 0.957 5 0.00717 pre0_mod10_post0 #&gt; 5 10 roc_auc binary 0.955 5 0.00739 pre0_mod09_post0 We see that now ROC was used to select the most optimal model. The closer the ROC value is to 1 the better our model will be. With this model we can predict values from the test data. ``` r # Select best k based on ROC best_k_roc &lt;- select_best(SP2_knn_res, metric = &quot;roc_auc&quot;) # Finalize workflow final_knn_roc &lt;- finalize_workflow(knn_workflow, best_k_roc) # Fit and predict SP2_knn_fit &lt;- fit(final_knn_roc, data = SP_train) SP2_predictions &lt;- augment(SP2_knn_fit, new_data = SP_test) # Evaluate SP2_predictions |&gt; conf_mat(truth = Direction, estimate = .pred_class) #&gt; Truth #&gt; Prediction Baja Sube #&gt; Baja 135 15 #&gt; Sube 16 147 SP2_predictions |&gt; accuracy(truth = Direction, estimate = .pred_class) #&gt; # A tibble: 1 × 3 #&gt; .metric .estimator .estimate #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 accuracy binary 0.901 We see how our accuracy (accuracy) has increased from 91.99% to 93.27%. This metric is highly recommended to improve the accuracy of our model, in addition to allowing us to more easily use it as a comparator between different models we can create. 12.9 Selection of the Most Optimal Model We have learned how to create some machine learning models. As we must have noticed, with caret we follow the same pattern for partitioning, training, and prediction. The variation lies in how to pre-process the data and the parameter tuning. We could thus create multiple models, but finally we have to verify one which will serve us to make our predictions. In this section, we are going to compare different predictive models accepting their default values and choose the best one using the tools presented in previous sections. To do this, we are going to use a new case. This time we are evaluating the behavior of our 5,000 clients, some of whom have unsubscribed from our services. We have 19 predictors, most of them numeric, in the mlc_churn dataset. To access the data we have to load the modeldata library. install.packages(&quot;modeldata&quot;) library(modeldata) data(mlc_churn) str(mlc_churn) #&gt; tibble [5,000 × 20] (S3: tbl_df/tbl/data.frame) #&gt; $ state : Factor w/ 51 levels &quot;AK&quot;,&quot;AL&quot;,&quot;AR&quot;,..: 17 36 32 36 37 2 20 25 19 50 ... #&gt; $ account_length : int [1:5000] 128 107 137 84 75 118 121 147 117 141 ... #&gt; $ area_code : Factor w/ 3 levels &quot;area_code_408&quot;,..: 2 2 2 1 2 3 3 2 1 2 ... #&gt; $ international_plan : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 2 2 2 1 2 1 2 ... #&gt; $ voice_mail_plan : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 2 2 1 1 1 1 2 1 1 2 ... #&gt; $ number_vmail_messages : int [1:5000] 25 26 0 0 0 0 24 0 0 37 ... #&gt; $ total_day_minutes : num [1:5000] 265 162 243 299 167 ... #&gt; $ total_day_calls : int [1:5000] 110 123 114 71 113 98 88 79 97 84 ... #&gt; $ total_day_charge : num [1:5000] 45.1 27.5 41.4 50.9 28.3 ... #&gt; $ total_eve_minutes : num [1:5000] 197.4 195.5 121.2 61.9 148.3 ... #&gt; $ total_eve_calls : int [1:5000] 99 103 110 88 122 101 108 94 80 111 ... #&gt; $ total_eve_charge : num [1:5000] 16.78 16.62 10.3 5.26 12.61 ... #&gt; $ total_night_minutes : num [1:5000] 245 254 163 197 187 ... #&gt; $ total_night_calls : int [1:5000] 91 103 104 89 121 118 118 96 90 97 ... #&gt; $ total_night_charge : num [1:5000] 11.01 11.45 7.32 8.86 8.41 ... #&gt; $ total_intl_minutes : num [1:5000] 10 13.7 12.2 6.6 10.1 6.3 7.5 7.1 8.7 11.2 ... #&gt; $ total_intl_calls : int [1:5000] 3 3 5 7 3 6 7 6 4 5 ... #&gt; $ total_intl_charge : num [1:5000] 2.7 3.7 3.29 1.78 2.73 1.7 2.03 1.92 2.35 3.02 ... #&gt; $ number_customer_service_calls: int [1:5000] 1 1 0 2 3 0 3 0 1 0 ... #&gt; $ churn : Factor w/ 2 levels &quot;yes&quot;,&quot;no&quot;: 2 2 2 2 2 2 2 2 2 2 ... # We translate outputs mlc_churn &lt;- mlc_churn |&gt; rename(churn_status = churn) |&gt; mutate(churn_status = ifelse(churn_status == &quot;yes&quot;, &quot;Sí&quot;, &quot;No&quot;)) |&gt; mutate(churn_status = as.factor(churn_status)) # Proportion of &quot;Yes&quot; and &quot;No&quot;s: prop.table(table(mlc_churn$churn_status)) #&gt; #&gt; No Sí #&gt; 0.8586 0.1414 We create now sample of training and test, 70% training. We create now sample of training and test, 70% training. set.seed(28) churn_split &lt;- initial_split(mlc_churn, prop = 0.7, strata = churn_status) churn_train &lt;- training(churn_split) churn_test &lt;- testing(churn_split) Up to here we have done exactly the same step as in previous models. However, previously we have specified the cross-validation method within our control parameters. Now we will create a shared validation set to compare all models fairly. We will create a list of 5 folds using the function vfold_cv() from rsample. set.seed(28) churn_folds &lt;- vfold_cv(churn_train, v = 5, strata = churn_status) churn_folds #&gt; # 5-fold cross-validation using stratification #&gt; # A tibble: 5 × 2 #&gt; splits id #&gt; &lt;list&gt; &lt;chr&gt; #&gt; 1 &lt;split [2799/700]&gt; Fold1 #&gt; 2 &lt;split [2799/700]&gt; Fold2 #&gt; 3 &lt;split [2799/700]&gt; Fold3 #&gt; 4 &lt;split [2799/700]&gt; Fold4 #&gt; 5 &lt;split [2800/699]&gt; Fold5 We will use the ROC metric for all models. In tidymodels, we define the metrics we want to calculate using a metric_set(). churn_metrics &lt;- metric_set(roc_auc, accuracy, sensitivity, specificity) The next step would be to choose the machine learning algorithms we want to use to create our models. parsnip provides a consistent interface for different models. We can check available engines for a model type, for example: show_engines(&quot;nearest_neighbor&quot;) #&gt; # A tibble: 2 × 2 #&gt; engine mode #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 kknn classification #&gt; 2 kknn regression We will create a series of models and compare them using ROC AUC. First, let’s define a common recipe for preprocessing. churn_recipe &lt;- recipe(churn_status ~ ., data = churn_train) |&gt; step_dummy(all_nominal_predictors(), -churn_status) |&gt; step_normalize(all_numeric_predictors()) 12.9.1 k-Nearest Neighbors Model Although it is a very simple model, it is also very useful. Let’s start with this model that we already learned to create during this chapter. # Spec knn_spec &lt;- nearest_neighbor(neighbors = tune()) |&gt; set_engine(&quot;kknn&quot;) |&gt; set_mode(&quot;classification&quot;) # Workflow knn_workflow &lt;- workflow() |&gt; add_recipe(churn_recipe) |&gt; add_model(knn_spec) # Tune knn_res &lt;- tune_grid( knn_workflow, resamples = churn_folds, grid = 10, metrics = churn_metrics ) show_best(knn_res, metric = &quot;roc_auc&quot;) #&gt; # A tibble: 5 × 7 #&gt; neighbors .metric .estimator mean n std_err .config #&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 15 roc_auc binary 0.684 5 0.0101 pre0_mod10_post0 #&gt; 2 13 roc_auc binary 0.679 5 0.00972 pre0_mod09_post0 #&gt; 3 11 roc_auc binary 0.676 5 0.0110 pre0_mod08_post0 #&gt; 4 10 roc_auc binary 0.674 5 0.0119 pre0_mod07_post0 #&gt; 5 8 roc_auc binary 0.665 5 0.0145 pre0_mod06_post0 12.9.2 Generalized Linear Model - GLM The generalized linear model (GLM) is a flexible generalization of ordinary linear regression. To do this we need to install the glmnet library before creating our model via tidymodels. install.packages(&quot;glmnet&quot;) # Spec glm_spec &lt;- logistic_reg(penalty = tune(), mixture = tune()) |&gt; set_engine(&quot;glmnet&quot;) |&gt; set_mode(&quot;classification&quot;) # Workflow glm_workflow &lt;- workflow() |&gt; add_recipe(churn_recipe) |&gt; add_model(glm_spec) # Tune glm_res &lt;- tune_grid( glm_workflow, resamples = churn_folds, grid = 10, metrics = churn_metrics ) show_best(glm_res, metric = &quot;roc_auc&quot;) #&gt; # A tibble: 5 × 8 #&gt; penalty mixture .metric .estimator mean n std_err .config #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 0.00599 1 roc_auc binary 0.819 5 0.00937 pre0_mod08_post0 #&gt; 2 0.0774 0.261 roc_auc binary 0.813 5 0.00862 pre0_mod09_post0 #&gt; 3 0.000464 0.578 roc_auc binary 0.808 5 0.0102 pre0_mod07_post0 #&gt; 4 0.00000278 0.894 roc_auc binary 0.807 5 0.0104 pre0_mod05_post0 #&gt; 5 0.00000000129 0.789 roc_auc binary 0.807 5 0.0104 pre0_mod02_post0 12.9.3 Random Forest Model Random Forest is a supervised machine learning technique based on decision trees. We will use the random forest model (RF). To do this we will first install the ranger library and then create the model via tidymodels. install.packages(&quot;ranger&quot;) # Spec rf_spec &lt;- rand_forest(mtry = tune(), trees = 1000, min_n = tune()) |&gt; set_engine(&quot;ranger&quot;) |&gt; set_mode(&quot;classification&quot;) # Workflow rf_workflow &lt;- workflow() |&gt; add_recipe(churn_recipe) |&gt; add_model(rf_spec) # Tune rf_res &lt;- tune_grid( rf_workflow, resamples = churn_folds, grid = 10, metrics = churn_metrics ) #&gt; i Creating pre-processing data to finalize 1 unknown parameter: &quot;mtry&quot; show_best(rf_res, metric = &quot;roc_auc&quot;) #&gt; # A tibble: 5 × 8 #&gt; mtry min_n .metric .estimator mean n std_err .config #&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 8 31 roc_auc binary 0.914 5 0.00998 pre0_mod02_post0 #&gt; 2 16 2 roc_auc binary 0.912 5 0.0104 pre0_mod03_post0 #&gt; 3 23 18 roc_auc binary 0.911 5 0.00975 pre0_mod04_post0 #&gt; 4 31 35 roc_auc binary 0.909 5 0.0101 pre0_mod05_post0 #&gt; 5 53 40 roc_auc binary 0.906 5 0.00890 pre0_mod08_post0 12.9.4 Support Vector Machine Model - SVM Support vector machines or support vector machines are a set of supervised learning algorithms. To create this model we will use the kernlab engine. install.packages(&quot;kernlab&quot;) # Spec svm_spec &lt;- svm_rbf(cost = tune(), rbf_sigma = tune()) |&gt; set_engine(&quot;kernlab&quot;) |&gt; set_mode(&quot;classification&quot;) # Workflow svm_workflow &lt;- workflow() |&gt; add_recipe(churn_recipe) |&gt; add_model(svm_spec) # Tune svm_res &lt;- tune_grid( svm_workflow, resamples = churn_folds, grid = 10, metrics = churn_metrics ) #&gt; maximum number of iterations reached 2.273272e-05 2.273273e-05maximum number of iterations reached 0.001260726 0.001226596maximum number of iterations reached 0.008990742 0.0089403maximum number of iterations reached 4.01911e-05 4.01911e-05maximum number of iterations reached 0.0004458751 0.0004390186maximum number of iterations reached 0.01426775 0.01386837maximum number of iterations reached 2.311619e-05 2.311619e-05maximum number of iterations reached 0.0004666225 0.0004600749maximum number of iterations reached 0.009561785 0.009488703maximum number of iterations reached 4.18965e-05 4.189651e-05maximum number of iterations reached 0.01467671 0.01418266maximum number of iterations reached 2.221129e-05 2.22113e-05maximum number of iterations reached 0.0009695224 0.0009465917maximum number of iterations reached 0.009269646 0.009208682maximum number of iterations reached 3.924845e-05 3.924845e-05maximum number of iterations reached 0.0002350936 0.0002328733maximum number of iterations reached 0.01304753 0.01272522maximum number of iterations reached 2.357812e-05 2.357812e-05maximum number of iterations reached 0.0003822001 0.000377737maximum number of iterations reached 0.009766638 0.009695014maximum number of iterations reached 4.284351e-05 4.284352e-05maximum number of iterations reached 0.01553881 0.01501075maximum number of iterations reached 2.310753e-05 2.310753e-05maximum number of iterations reached 0.0004798964 0.0004730654maximum number of iterations reached 0.009636723 0.009566509maximum number of iterations reached 4.147277e-05 4.147278e-05maximum number of iterations reached 0.0002249601 0.0002227933maximum number of iterations reached 0.01498293 0.01448009 show_best(svm_res, metric = &quot;roc_auc&quot;) #&gt; # A tibble: 5 × 8 #&gt; cost rbf_sigma .metric .estimator mean n std_err .config #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 0.00310 0.00599 roc_auc binary 0.864 5 0.00811 pre0_mod02_post0 #&gt; 2 32 0.000464 roc_auc binary 0.862 5 0.00900 pre0_mod10_post0 #&gt; 3 1 0.0000359 roc_auc binary 0.794 5 0.0103 pre0_mod07_post0 #&gt; 4 0.0312 0.00000278 roc_auc binary 0.793 5 0.0114 pre0_mod04_post0 #&gt; 5 0.000977 0.000000215 roc_auc binary 0.793 5 0.0114 pre0_mod01_post0 12.9.5 Naive Bayes Model Naïve Bayes (NB), Naive Bayes is one of the simplest, yet powerful, algorithms for classification based on Bayes’ Theorem. To use this model we will use the naivebayes library within tidymodels. install.packages(&quot;naivebayes&quot;) library(naivebayes) # Required for the engine # Spec nb_spec &lt;- naive_Bayes() |&gt; set_engine(&quot;naivebayes&quot;) |&gt; set_mode(&quot;classification&quot;) # Workflow nb_workflow &lt;- workflow() |&gt; add_recipe(churn_recipe) |&gt; add_model(nb_spec) # Tune nb_res &lt;- fit_resamples( nb_workflow, resamples = churn_folds, metrics = churn_metrics ) collect_metrics(nb_res) #&gt; # A tibble: 4 × 6 #&gt; .metric .estimator mean n std_err .config #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 accuracy binary 0.859 5 0.000246 pre0_mod0_post0 #&gt; 2 roc_auc binary 0.840 5 0.00983 pre0_mod0_post0 #&gt; 3 sensitivity binary 1 5 0 pre0_mod0_post0 #&gt; 4 specificity binary 0 5 0 pre0_mod0_post0 12.9.6 Model Comparison To compare the models, we can extract the metrics from each tuning result and visualize them. # Collect metrics knn_metrics &lt;- collect_metrics(knn_res) |&gt; mutate(model = &quot;kNN&quot;) glm_metrics &lt;- collect_metrics(glm_res) |&gt; mutate(model = &quot;GLM&quot;) rf_metrics &lt;- collect_metrics(rf_res) |&gt; mutate(model = &quot;RF&quot;) svm_metrics &lt;- collect_metrics(svm_res) |&gt; mutate(model = &quot;SVM&quot;) nb_metrics &lt;- collect_metrics(nb_res) |&gt; mutate(model = &quot;Naive Bayes&quot;) # Combine all_metrics &lt;- bind_rows(knn_metrics, glm_metrics, rf_metrics, svm_metrics, nb_metrics) # Visualize ROC AUC all_metrics |&gt; filter(.metric == &quot;roc_auc&quot;) |&gt; ggplot(aes(x = model, y = mean, fill = model)) + geom_col() + labs(y = &quot;ROC AUC&quot;, title = &quot;Model Comparison&quot;) + theme(legend.position = &quot;none&quot;) For this case the random forest model (RF) seems to be the best. This is not surprising given that this algorithm is related to its ability to cope with different input types and require little preprocessing. We can make our models better by pre-processing data and changing the ad-hoc parameters of each model. 12.9.7 Predicting using the best model Now that we have our best model (Random Forest), we proceed to perform the prediction on the test set. We need to finalize the workflow with the best hyperparameters from the tuning step first. # Select best parameters for RF best_rf &lt;- select_best(rf_res, metric = &quot;roc_auc&quot;) # Finalize workflow final_rf_workflow &lt;- finalize_workflow(rf_workflow, best_rf) # Fit on training data optimal_model &lt;- fit(final_rf_workflow, data = churn_train) # Predict on test data churn_predictions &lt;- augment(optimal_model, new_data = churn_test) # Evaluate results churn_predictions |&gt; conf_mat(truth = churn_status, estimate = .pred_class) #&gt; Truth #&gt; Prediction No Sí #&gt; No 1285 66 #&gt; Sí 3 147 churn_predictions |&gt; accuracy(truth = churn_status, estimate = .pred_class) #&gt; # A tibble: 1 × 3 #&gt; .metric .estimator .estimate #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 accuracy binary 0.954 Thus, we have found how to create a customer churn prediction model given 19 prediction variables with an accuracy of 96%. 12.10 Exercises The attrition data frame from the modeldata library shows data from a list of almost 1,500 employees of a company. Create a copy of this data frame and store it in the trabajadores object. Then, build an RF model with this data to predict the Attrition field (job desertion). Where the class “Yes” means they resigned and “No” means they still work. Solution data(attrition) str(attrition) workers &lt;- attrition workers &lt;- workers |&gt; rename(attrition_status = Attrition) # 70% for the training data set.seed(28) workers_split &lt;- initial_split(workers, prop = 0.7, strata = attrition_status) workers_train &lt;- training(workers_split) workers_test &lt;- testing(workers_split) # Recipe workers_recipe &lt;- recipe(attrition_status ~ ., data = workers_train) |&gt; step_dummy(all_nominal_predictors(), -attrition_status) |&gt; step_normalize(all_numeric_predictors()) # We create CV folds workers_folds &lt;- vfold_cv(workers_train, v = 5, strata = attrition_status) workers_metrics &lt;- metric_set(roc_auc, accuracy) # We create the model rf_spec &lt;- rand_forest(trees = 1000) |&gt; set_engine(&quot;ranger&quot;) |&gt; set_mode(&quot;classification&quot;) rf_wf &lt;- workflow() |&gt; add_recipe(workers_recipe) |&gt; add_model(rf_spec) workers_rf_res &lt;- fit_resamples( rf_wf, resamples = workers_folds, metrics = workers_metrics ) collect_metrics(workers_rf_res) Using the training data from the previous exercise, build the GLM model using tidymodels. Solution # Spec glm_spec &lt;- logistic_reg(penalty = tune(), mixture = tune()) |&gt; set_engine(&quot;glmnet&quot;) |&gt; set_mode(&quot;classification&quot;) # Workflow glm_wf &lt;- workflow() |&gt; add_recipe(workers_recipe) |&gt; add_model(glm_spec) # Tune workers_glm_res &lt;- tune_grid( glm_wf, resamples = workers_folds, grid = 10, metrics = workers_metrics ) show_best(workers_glm_res, metric = &quot;roc_auc&quot;) Using the training data, build the SVM model. Solution # Spec svm_spec &lt;- svm_rbf(cost = tune(), rbf_sigma = tune()) |&gt; set_engine(&quot;kernlab&quot;) |&gt; set_mode(&quot;classification&quot;) # Workflow svm_wf &lt;- workflow() |&gt; add_recipe(workers_recipe) |&gt; add_model(svm_spec) # Tune workers_svm_res &lt;- tune_grid( svm_wf, resamples = workers_folds, grid = 10, metrics = workers_metrics ) show_best(workers_svm_res, metric = &quot;roc_auc&quot;) From the created models, which is the most optimal? Solution # We generate model list lista_de_modelos &lt;- list(rf = trabajadores_modelo_rf, glmmet = workers_model_glm, svm = workers_model_svm) # We compare the models comparacion_modelos &lt;- resamples(lista_de_modelos) # We visualize the comparison bwplot(comparacion_modelos, metric = &quot;ROC&quot;) # We obtain the summary of the comparison summary(comparacion_modelos, metric = &quot;ROC&quot;) We see how the results overlap, so we could opt for the two that have the highest mean ROC and among them choose the one that gives us a smaller range of values. Create the confusion matrices for the three models created. Solution prediccion_rf &lt;- predict(trabajadores_modelo_rf, trabajadores_test) confusionMatrix(prediccion_rf, trabajadores_test$renuncia) prediccion_glm &lt;- predict(trabajadores_modelo_glm, trabajadores_test) confusionMatrix(prediccion_glm, trabajadores_test$renuncia) prediccion_svm &lt;- predict(trabajadores_modelo_svm, trabajadores_test) confusionMatrix(prediccion_svm, trabajadores_test$renuncia) Keep in mind that the model with the highest ROC value will not necessarily have the highest accuracy. Therefore the choice of the model was performed in a previous step. The ROC better balances sensitivity with the false positive rate. 12.11 Ethics: Bias in Algorithmic Decision Making In the previous exercise, we built models to predict employee attrition using variables like Gender, Age, and MaritalStatus. While mathematically sound, obtaining a high accuracy score does not mean the model is “good” or “fair” to use in the real world. 12.11.1 The Risk of Proxy Variables Even if we remove explicit sensitive attributes (like Gender or Ethnicity), other variables can act as proxies. * Zip Code: Often correlates with race or socioeconomic status. * Years of Experience: Strongly correlated with Age. 12.11.2 Feedback Loops If a company uses an algorithm to decide who to hire or fire based on historical data, they may perpetuate historical biases. * Scenario: If a company historically didn’t hire women for leadership roles, the training data will show that women are “less likely to succeed” in those roles. * Result: The model creates a feedback loop, rejecting qualified female candidates because they don’t match the historical pattern of “success”. 12.11.3 What can we do? Audit your Data: checking for representation balance (e.g., is one group significantly smaller?). Model Explainability: Use tools like DALEX or vip (variable importance) to understand why the model is making a decision. If MaritalStatus is the top predictor for firing someone, is that ethical? Human in the Loop: These models should support human decision-making, not replace it entirely. As Data Scientists, our responsibility extends beyond the AUC score. We must ensure our models do not harm individuals or groups. References "],["unsupervised-learning.html", "Chapter 13 Unsupervised Learning 13.1 K-Means Clustering 13.2 Hierarchical Clustering 13.3 Dimensionality Reduction 13.4 Exercises", " Chapter 13 Unsupervised Learning Now that we know how to create supervised learning algorithms, understanding unsupervised learning becomes an intuitive exercise. While in supervised learning we have a set of variables that we use to predict a certain output class (up/down, resign/not resign), in unsupervised learning we do not have expected output classes. In supervised learning we had training data and testing data that allowed us to validate the effectiveness of the model by its closeness to the known class. In unsupervised learning we do not have a default output. This in turn generates a great challenge because it is very difficult to know if we have already finished the work or if we can still generate another model with which we feel more satisfied. The simplest example to understand this type of learning is when we have our customer base and we want to segment them for the first time. In that case, we look for customers who behave in the same way, but being the first time, we don’t know how many segments we can have. The challenge lies in determining the cut-off: how many segments do we seek to create? The main applications of unsupervised learning are related to data clustering. Here, the goal is to find homogeneous subgroups within the data. These algorithms are based on the distance between observations. The customer segmentation example would be an example of clustering. The most commonly used clustering algorithms are: k-means clustering and hierarchical clustering. 13.1 K-Means Clustering To understand this method we will use examples first with a minimal amount of variables and then little by little we will create a more generic model. 13.1.1 Clustering with k = 2 Suppose we have a list of players on a soccer field and we take a photo from above to have their coordinates (variable 1 would be the x-axis and variable 2 would be the y-axis). We cannot see which team each player belongs to so we will paint everyone as black dots. players &lt;- tibble(x = c(-1, -2, 8, 7, -12, -15, -13, 15, 21, 12, -25, 26), y = c(1, -3, 6, -8, 8, 0, -10, 16, 2, -15, 1, 0) ) players |&gt; ggplot() + aes(x, y) + geom_point(size = 5) This method allows us to group based on the definition of centroids. We will define as many centroids as groups we want to obtain. Since for this case we know that there must be two teams, we will use 2 centroids (k = 2). The k-means algorithm then places these 2 points (centroids) randomly on the plane in a first iteration. Then, it calculates the distance between each center and the other data points. If it is closer to a centroid then it assigns it to centroid 1, otherwise to centroid 2. A first grouping has already been performed. Now each centroid within each group is located at the mean of the other points in its group and another iteration occurs to reassign all points. This iteration is done over and over again until the centroids are fixed. To create this model in R we will use the function kmeans(data, centers = k). kmeans_model &lt;- kmeans(players, centers = 2) # We print the coordinates of the centers kmeans_model$centers #&gt; x y #&gt; 1 -11.33333 -0.5000000 #&gt; 2 14.83333 0.1666667 This means that for these two centers the average distance to the other points is the minimum, therefore the algorithm assigns them to one group or another. Let’s see approximately where these centers are located if we marked them with an x. Thus, once the model is created we can obtain the clustering results, team 1 or team 2. team &lt;- kmeans_model$cluster We can add this team assignment as one more column of our players data set to be able to visualize them in R. # We add the cluster column players_agrupados &lt;- players |&gt; mutate(cluster = team) # We visualize the players according to the grouping players_agrupados |&gt; ggplot() + aes(x, y, fill = factor(cluster)) + geom_point(size = 5, pch = 21) + scale_fill_manual(values=c(&quot;#EE220D&quot;, &quot;#01A2FF&quot;)) + theme(legend.position = &quot;none&quot;) We have found two centroids until minimizing the sum of the squared differences between each centroid and the other points in the cluster. We can access and see how much this value is, given that it is part of the model results. # Sum of squares within each cluster kmeans_model$withinss #&gt; [1] 570.8333 863.6667 # Total kmeans_model$tot.withinss #&gt; [1] 1434.5 Tot.withinss comes from Total within-cluster sum of squares. 13.1.2 Clustering with k &gt;= 3 When we have 3 or more centers the idea is the same, we only change the centers parameter. kmeans_model &lt;- kmeans(players, centers = 3) team &lt;- kmeans_model$cluster players_agrupados &lt;- players |&gt; mutate(cluster = team) players_agrupados |&gt; ggplot() + aes(x, y, color = factor(cluster)) + geom_point(size = 5) + theme(legend.position = &quot;none&quot;) kmeans_model$tot.withinss #&gt; [1] 881.25 In this case we have found that the sum of squares within the clusters is smaller, so we could indicate that this grouping is more optimal than the grouping into two groups. However, the sum of squares is not necessarily the best indicator for choosing how many clusters to create. 13.1.3 Determination of Optimal Clusters We can mainly use two methods to determine how many clusters we should build, k. The sum of squares method (wss) and the average silhouette method (silhouette). To avoid having to calculate models for different values of k we will use the factoextra library, which was created especially to perform easy multivariate data analysis and elegant visualization, very useful for clustering. install.packages(&quot;factoextra&quot;) library(factoextra) 13.1.3.1 Sum of Squares Method To find the optimal “k” under this method, we will use the elbow plot, where we first calculate the total within-cluster sum of squares for different values of “k”. Then, visually we will identify a point where there seems to be a very strong drop followed by a more gradual drop in the slope. To do this, we will use the function fviz_nbclust(data, type, method) and enter our data, the type of algorithm that will be used to group and the measurement method. fviz_nbclust(players, FUN = kmeans, method = &quot;wss&quot;) In this case the “elbow” is found at the value k = 2, from there the sum of squares reduces but at a slower rate. 13.1.3.2 Average Silhouette Method The method described above is a visual aid that makes recognition difficult when the data points are closer. Therefore, it is much more frequent to perform a silhouette analysis (Rousseeuw 1987). This approach measures the quality of a clustering. That is, it determines how well each object lies within its group. A high average silhouette width indicates a good clustering. The average silhouette method calculates the average silhouette of observations for different values of “k”. The optimal number of groups “k” is the one that maximizes the average silhouette over a range of possible values for “k”. To do this, we change the method parameter in the function and obtain the silhouette analysis. fviz_nbclust(players, FUN = kmeans, method = &quot;silhouette&quot;) Here it is clearly seen that for a value of k=2 we have the best average, making this our optimal number of groups. 13.1.4 k-means for more than 2 variables The method we have learned can be easily extended to more variables. Only in this case it would no longer be possible to visualize it like the soccer team and we would only visualize the results of the grouping and the learned metrics. To do this, we will use the following customer dataset, where we will find a dataset of customers of a wholesale distributor. It includes the annual spending in monetary units on various product categories. url &lt;- &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/00292/Wholesale%20customers%20data.csv&quot; customers &lt;- read_csv(url) #&gt; Rows: 440 Columns: 8 #&gt; ── Column specification ───────────────────────────── #&gt; Delimiter: &quot;,&quot; #&gt; dbl (8): Channel, Region, Fresh, Milk, Grocery, Frozen, Detergents_Paper, De... #&gt; #&gt; ℹ Use `spec()` to retrieve the full column specification for this data. #&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. We are going to perform a grouping only considering the spending made on frozen foods, groceries and dairy products. customers_filtrado &lt;- customers |&gt; select(Milk, Grocery, Frozen) Once we have our data we would create a silhouette analysis to determine the best value of “k”. fviz_nbclust(customers_filtrado, FUN = kmeans, method = &quot;silhouette&quot;) Again, we get that the recommended number of clusters is 2. Let’s create the model for k = 2 and store the resulting cluster. modelo &lt;- kmeans(customers_filtrado, centers = 2) customers_agrupados &lt;- customers_filtrado |&gt; mutate(cluster = modelo$cluster) customers_agrupados #&gt; # A tibble: 440 × 4 #&gt; Milk Grocery Frozen cluster #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 9656 7561 214 1 #&gt; 2 9810 9568 1762 1 #&gt; 3 8808 7684 2405 1 #&gt; 4 1196 4221 6404 1 #&gt; 5 5410 7198 3915 1 #&gt; 6 8259 5126 666 1 #&gt; 7 3199 6975 480 1 #&gt; 8 4956 9426 1669 1 #&gt; 9 3648 6192 425 1 #&gt; 10 11093 18881 1159 2 #&gt; # ℹ 430 more rows Once we have grouped our data we can calculate the amount of data in each cluster and the mean of the values for each group and thus identify differences between these two potential customer segments. customers_agrupados |&gt; group_by(cluster) |&gt; summarise(total = n(), media_Milk = mean(Milk), media_Grocery = mean(Grocery), media_Frozen = mean(Frozen)) #&gt; # A tibble: 2 × 5 #&gt; cluster total media_Milk media_Grocery media_Frozen #&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 388 3955. 5386. 2902. #&gt; 2 2 52 19534. 27094. 4341. Thus, we have learned to segment customers using machine learning. 13.2 Hierarchical Clustering Hierarchical clustering is another method for grouping data. The word hierarchical comes from the hierarchies that this algorithm creates to determine the clusters. Unlike k-means, we do not start by indicating how many clusters we want to create, but rather the algorithm shows us a list of possible combinations according to the hierarchy of distances between points. Let’s see it with an example. 13.2.1 Clustering with two variables To do this we will use the same soccer team example that we used previously. With the difference that this time we number each player to make visualization easier. num &lt;- 1:12 players &lt;- tibble(x = c(-1, -2, 8, 7, -12, -15, -13, 15, 21, 12, -25, 26), y = c(1, -3, 6, -8, 8, 0, -10, 16, 2, -15, 1, 0)) players |&gt; ggplot() + aes(x, y, label = num) + geom_point(size = 5) + geom_text(nudge_x = 1.3, nudge_y = 1.3) This algorithm searches for the two points with the shortest distance, the closest ones, and groups them. Then it searches for another two points with the smallest distance and asks: is the distance between these two new points less than the distance of these points to the previously created group? If the answer is yes, it groups them, otherwise it groups the closest point to the first created group. Let’s understand the algorithm graphically. Points 1 and 2 have the lowest hierarchy since they have the shortest distance. Then the algorithm searches for the next two closest points (point 9 and 12) and when comparing with the midpoint of 1 and 2 it opts to create a new group with a slightly higher hierarchy and so on. However, now that we have point 7 and 11 and we calculate the distance, it turns out that that distance is not the smallest compared to the distances with the other existing groups. For example, 7 is closer to the midpoint of 1 and 2, and 11 is closer to the midpoint of 5 and 6. Thus, the algorithm creates a higher hierarchy for this grouping. The algorithm continues until it finally creates a group that includes everyone as the highest hierarchy. In the following graph we can not only appreciate this but also on the y-axis the distance between each point or group of points. Up to here we haven’t done more than generate hierarchies from the distances which will serve us later to determine how many clusters to generate. Let’s create in R what has been advanced so far. The first thing we will do is calculate the distances between all points. To do this we will use the dist() function. player_distances &lt;- dist(players) With the calculated distances we can create the hierarchical model using the hclust(distance_matrix) function. hierarchical_model &lt;- hclust(player_distances) Once our model is created we can visualize it using the dendextend library. install.packages(&quot;dendextend&quot;) library(dendextend) The visualization we saw is called a dendrogram. To do this we just have to convert our model to dendrogram format. dend_model &lt;- as.dendrogram(hierarchical_model) plot(dend_model) So far we have only seen the hierarchy, but what interests us is the grouping. The grouping is done by the calculated distance (h parameter). Let’s try with a distance of 60. We will use the color_branches and color_labels functions to make the changes visible. cut_height &lt;- 60 dend_model |&gt; color_branches(h = cut_height) |&gt; color_labels(h = cut_height) |&gt; plot() #&gt; Loading required namespace: colorspace Since the highest hierarchy distance is approximately 50, then in this case it groups everyone into one large cluster. Let’s try with a lower number, for example 40. cut_height &lt;- 40 dend_model |&gt; color_branches(h = cut_height) |&gt; color_labels(h = cut_height) |&gt; plot() |&gt; abline(h = cut_height, lty = 2) By making a cut at 40 we now have two clusters, in this case the red color and the green color. Let’s try with a lower number, 28. cut_height &lt;- 28 dend_model |&gt; color_branches(h = cut_height) |&gt; color_labels(h = cut_height) |&gt; plot() |&gt; abline(h = cut_height, lty = 2) Now we have three clusters and so we could continue until obtaining the clusters we need. We must have noticed how impractical it is to use the distances of the hierarchical model because they vary according to the data we have. This model allows us to make cuts not only by distances but also by indicating how many clusters we want, parameter k. desired_clusters &lt;- 3 dend_model |&gt; color_branches(k = desired_clusters) |&gt; color_labels(k = desired_clusters) |&gt; plot() We see that it gives us the same grouping whether we use distances or number of desired clusters. 13.2.2 Determination of Optimal Clusters To calculate how many clusters are optimal to create we will use the silhouette analysis again, but this time with the argument FUN = hcut to determine that it be evaluated based on a hierarchical model. fviz_nbclust(players, FUN = hcut, method = &quot;silhouette&quot;) It is not surprising that the value of k is also 2, which coincides with the number obtained in the k-means model. 13.2.3 Obtain the grouping Now that we have validated that the recommended number of clusters is 2, we calculate the grouping from the previously created model. players_agrupados &lt;- players |&gt; mutate(cluster = cutree(hierarchical_model, k = 2) ) players_agrupados #&gt; # A tibble: 12 × 3 #&gt; x y cluster #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 -1 1 1 #&gt; 2 -2 -3 1 #&gt; 3 8 6 2 #&gt; 4 7 -8 2 #&gt; 5 -12 8 1 #&gt; 6 -15 0 1 #&gt; 7 -13 -10 1 #&gt; 8 15 16 2 #&gt; 9 21 2 2 #&gt; 10 12 -15 2 #&gt; 11 -25 1 1 #&gt; 12 26 0 2 Finally, let’s visualize the grouping performed with this method. players_agrupados |&gt; ggplot() + aes(x, y, color = factor(cluster)) + geom_point(size = 5) + theme(legend.position = &quot;none&quot;) We see that the grouping is the same as with the previous method, basically because we are talking about two variables and two clusters. Both methods learned are very flexible, so the creation of models for more variables follows the same logic learned in these sections. 13.3 Dimensionality Reduction We have created clusters with a controlled number of variables. However, we are going to encounter in many cases many more variables that make interpretation difficult and it is important to identify if two variables have the same behavior to be able to take only one of them. For this case we are going to take as an example a credit card customer dataset, adaptation of the public dataset in Kaggle, from the following route. url &lt;- &quot;https://dparedesi.github.io/DS-con-R/cards_df_de_credito.csv&quot; cards_df &lt;- read_csv(url) We have more than 8 thousand customers with 13 attributes. We will analyze if there are strongly correlated variables. To do this we will use the corrplot library. library(corrplot) Next, we will enter the dataset to visualize correlations between the variables, corrplot(cor(cards_df), type=&quot;upper&quot;, method=&quot;ellipse&quot;, tl.cex=0.9) There is a strong correlation between the total purchases variable and the purchases made in the first 3 months. We can visualize these two variables to validate. cards_df |&gt; ggplot() + aes(x=purchases, y=purchases_primeros_tres_meses) + geom_point() + labs(title=&quot;Customer Attributes&quot;, subtitle=&quot;Relationship between total purchases and first 3 months&quot;) Given this, we could include within our analysis only one of these two variables. We could also validate the distribution of these variables. # We remove the purchases first 3 months variable cards_df &lt;- cards_df[, !names(cards_df) == &quot;purchases_primeros_tres_meses&quot;] cards_df |&gt; pivot_longer(cols = everything(), names_to = &quot;atributos&quot;, values_to = &quot;valores&quot;) |&gt; ggplot() + aes(x=valores, fill=atributos) + geom_histogram(colour=&quot;black&quot;, show.legend=FALSE) + facet_wrap(~atributos, scales=&quot;free_x&quot;) + labs(x=&quot;Values&quot;, y=&quot;Frequency&quot;, title=&quot;Customer Attributes - Histogram&quot;) We see data concentrations in some variables such as tenure (time our customer has been with us). We can validate it by zooming into that variable. boxplot(cards_df$tenure) prop.table(table(cards_df$tenure)) 85% of our data are from customers who have been with us for 12 months. We could choose to filter the data to analyze customers who have 1 year and thus remove this variable from the grouping. cards_df &lt;- cards_df |&gt; filter(tenure == 12) cards_df &lt;- cards_df[, !names(cards_df) == &quot;tenure&quot;] # We will do the same with the balance_freq variable prop.table(table(cards_df$balance_freq)) cards_df &lt;- cards_df |&gt; filter(balance_freq == 1) cards_df &lt;- cards_df[, !names(cards_df) == &quot;balance_freq&quot;] If we also analyze the distributions of each variable we find the following: summary(cards_df) We see that there are variables that have maximums of 1, as there are others that have a maximum of 30 thousand or 50 thousand. We had already seen previously the importance of normalizing data. Here we will also do it with the scale() function. cards_df_norm &lt;- as.data.frame(scale(cards_df)) We can verify that the distribution does not change, only the scale. cards_df_norm |&gt; pivot_longer(cols = everything(), names_to = &quot;atributos&quot;, values_to = &quot;valores&quot;) |&gt; ggplot() + aes(x=valores, fill=atributos) + geom_histogram(colour=&quot;black&quot;, show.legend=FALSE) + facet_wrap(~atributos, scales=&quot;free_x&quot;) + labs(x=&quot;Values&quot;, y=&quot;Frequency&quot;, title=&quot;Customer Attributes - Histogram&quot;) Data preparation and variable reduction is a necessary step when we create machine learning models. However, we must do it carefully, given that in this exercise when preparing the data, although we have fewer variables (10), we also have fewer rows. str(cards_df_norm) More advanced techniques such as Principal Component Analysis (PCA) and Singular Value Decomposition (SVD) are used to perform dimensionality reduction more rigorously so as not to lose so much data in our analysis. However, their interpretation is still under development due to the high mathematical complexity required for their understanding. 13.4 Exercises In the following exercises we will work on post data from 10 fashion companies that have their pages on Facebook and the reactions of their followers. To do this, we will work with the data in the following repository: url &lt;- &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/00488/Live.csv&quot; posts &lt;- read_csv(url) # We remove columns not relevant to the analysis columnas_no_relevantes &lt;- c(&quot;status_type&quot;,&quot;status_id&quot;, &quot;status_published&quot;, &quot;Column1&quot;, &quot;Column2&quot;, &quot;Column3&quot;, &quot;Column4&quot;) data_posts &lt;- posts[, !names(posts) %in% columnas_no_relevantes] With the data_posts object normalized (use scale() function) and create the data_posts_norm object. Build a silhouette plot to determine how many cluster groups are recommended using the k-means algorithm. Solution data_posts_norm &lt;- as.data.frame(scale(data_posts)) fviz_nbclust(data_posts_norm, FUN = kmeans, method = &quot;silhouette&quot;) With the data_posts object build a silhouette plot to determine how many cluster groups are recommended using the hierarchical algorithm. Solution fviz_nbclust(data_posts, FUN = hcut, method = &quot;silhouette&quot;) If you had to remove a variable from the analysis, which variable would it be? Solution # We perform a visualization of the correlation of variables corrplot(cor(data_posts), type=&quot;upper&quot;, method=&quot;ellipse&quot;, tl.cex=0.9) # We can check it by plotting these two variables: data_posts |&gt; ggplot() + aes(x=num_reactions, y=num_likes) + geom_point() Remove the num_reactions variable from the data_posts_norm object and the data_posts object and perform a silhouette analysis again using data_posts_norm. Does the number of clusters change? Solution data_posts &lt;- data_posts[, -1] data_posts_norm &lt;- data_posts_norm[, -1] fviz_nbclust(data_posts_norm, FUN = kmeans, method = &quot;silhouette&quot;) The number of clusters does not change because there exists another variable with the same behavior as this one. Create the k-means model to group using the recommended number of clusters found. Use the data_posts_norm object for the creation of the model. Create the data_posts_agrupados object where the original data of data_posts is with the additional column cluster_medias indicating the cluster result of this model. Solution modelo_kmedias &lt;- kmeans(data_posts_norm, centers = 2) data_posts_agrupados &lt;- data_posts |&gt; mutate(cluster_kmedias = modelo_kmedias$cluster) Create the hierarchical model to group using the recommended number of clusters found. Use the data_posts_norm object for the creation of the model. Add to the data_posts_agrupados object the column cluster_jer to store the result of the grouping. Solution distancias &lt;- dist(data_posts_norm) modelo_jerarquico &lt;- hclust(distancias) data_posts_agrupados &lt;- data_posts_agrupados |&gt; mutate(cluster_jer = cutree(modelo_jerarquico, k = 2) ) Calculate the average of each value of the variables for each group of the k-means model. Solution data_posts_agrupados |&gt; select(-cluster_jer) |&gt; group_by(cluster_kmedias) |&gt; summarise_all(list(mean)) Calculate the average of each value of the variables for each group of the hierarchical model. Solution data_posts_agrupados |&gt; select(-cluster_kmedias) |&gt; group_by(cluster_jer) |&gt; summarise_all(list(mean)) References "],["introduction-3.html", "Introduction", " Introduction Below are some real cases presented based on what has been learned in this book. For the following cases, not only the concepts learned so far will be applied, but some complementary libraries that speed up data processing are also included. "],["case-study-real-estate-market-analysis.html", "Chapter 14 Case Study: Real Estate Market Analysis 14.1 Objectives 14.2 Loading Libraries 14.3 Exploring the Data 14.4 Data Cleaning 14.5 Exploratory Analysis 14.6 Creating Indicators 14.7 Conclusions", " Chapter 14 Case Study: Real Estate Market Analysis In this case study, we will apply our data transformation and visualization skills to analyzing the real estate market. We will use the txhousing dataset provided by the ggplot2 package, which contains information about housing sales in Texas. This dataset allows us to explore similar concepts to financial invoice factoring (tracking value over time, comparing categories, and analyzing volume) but with a much more stable and standardized data source. 14.1 Objectives Data Cleaning: Handle missing values and format dates. Transformation: Aggregate data by city and year to find trends. Visualization: Create time-series plots to analyze market health. Analysis: Identify cities with the highest growth and stability. 14.2 Loading Libraries We will use the core tidyverse libraries. library(tidyverse) library(lubridate) library(scales) library(ggthemes) 14.3 Exploring the Data First, let’s load and inspect the data. data(&quot;txhousing&quot;) glimpse(txhousing) #&gt; Rows: 8,602 #&gt; Columns: 9 #&gt; $ city &lt;chr&gt; &quot;Abilene&quot;, &quot;Abilene&quot;, &quot;Abilene&quot;, &quot;Abilene&quot;, &quot;Abilene&quot;, &quot;Abil… #&gt; $ year &lt;int&gt; 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, … #&gt; $ month &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, … #&gt; $ sales &lt;dbl&gt; 72, 98, 130, 98, 141, 156, 152, 131, 104, 101, 100, 92, 75, … #&gt; $ volume &lt;dbl&gt; 5380000, 6505000, 9285000, 9730000, 10590000, 13910000, 1263… #&gt; $ median &lt;dbl&gt; 71400, 58700, 58100, 68600, 67300, 66900, 73500, 75000, 6450… #&gt; $ listings &lt;dbl&gt; 701, 746, 784, 785, 794, 780, 742, 765, 771, 764, 721, 658, … #&gt; $ inventory &lt;dbl&gt; 6.3, 6.6, 6.8, 6.9, 6.8, 6.6, 6.2, 6.4, 6.5, 6.6, 6.2, 5.7, … #&gt; $ date &lt;dbl&gt; 2000.000, 2000.083, 2000.167, 2000.250, 2000.333, 2000.417, … The dataset contains: - city: Name of the city. - year, month: Date components. - sales: Number of sales. - volume: Total value of sales. - median: Median sale price. - listings: Total active listings. - inventory: “Months inventory”: amount of time it would take to sell all current listings at current sales pace. - date: Date in decimal format (e.g., 2000.08). 14.4 Data Cleaning Real-world data often has missing values (NA). Let’s check how many missing values we have in the sales column. sum(is.na(txhousing$sales)) #&gt; [1] 568 We see there are records with no sales data. For our analysis of market volume, we should remove these incomplete records. We will creating a clean dataset housing_clean. We will also create a proper date column using lubridate::make_date(), which is easier to work with than the decimal date. housing_clean &lt;- txhousing |&gt; filter(!is.na(sales)) |&gt; mutate(date_proper = make_date(year, month, 1)) |&gt; select(-date) # Remove the decimal date 14.5 Exploratory Analysis 14.5.1 Market Volume Over Time Let’s look at the total sales volume across all of Texas over time. This gives us a “macro” view of the market, similar to how we might look at total portfolio value in a financial context. # Aggregate by date total_market &lt;- housing_clean |&gt; group_by(date_proper) |&gt; summarise( total_sales = sum(sales), total_volume = sum(volume, na.rm = TRUE) ) # Plot Volume total_market |&gt; ggplot(aes(x = date_proper, y = total_volume)) + geom_line(color = &quot;steelblue&quot;) + scale_y_continuous(labels = label_dollar(scale = 1e-9, suffix = &quot;B&quot;)) + theme_minimal() + labs( title = &quot;Total Texas Housing Market Volume&quot;, subtitle = &quot;Monthly Total Sales Volume (Billions)&quot;, x = &quot;Year&quot;, y = &quot;Volume ($)&quot; ) We can clearly see the seasonality (peaks in summer) and the impact of the 2008 financial crisis (dip around 2008-2010), followed by a strong recovery. 14.5.2 Comparing Cities Just as we might compare different companies or portfolios, let’s compare the median housing prices in the major cities. We’ll focus on the “Big 4” Texas cities: Austin, Dallas, Houston, and San Antonio. major_cities &lt;- c(&quot;Austin&quot;, &quot;Dallas&quot;, &quot;Houston&quot;, &quot;San Antonio&quot;) city_trends &lt;- housing_clean |&gt; filter(city %in% major_cities) city_trends |&gt; ggplot(aes(x = date_proper, y = median, color = city)) + geom_line(alpha = 0.7) + theme_minimal() + scale_y_continuous(labels = label_dollar()) + labs( title = &quot;Median Housing Prices in Major Cities&quot;, x = &quot;Year&quot;, y = &quot;Median Price&quot;, color = &quot;City&quot; ) Austin (green) clearly shows the steepest growth curve, especially post-2012. 14.6 Creating Indicators In financial analysis, we often create ratios. Here, let’s look at Inventory, which is a measure of supply vs. demand. - High Inventory: Buyer’s market (prices might drop). - Low Inventory: Seller’s market (prices might rise). Let’s look at the average inventory per year for these cities. city_inventory &lt;- city_trends |&gt; group_by(city, year) |&gt; summarise(avg_inventory = mean(inventory, na.rm = TRUE), .groups = &quot;drop&quot;) city_inventory |&gt; ggplot(aes(x = year, y = avg_inventory, color = city)) + geom_line(size = 1) + theme_fivethirtyeight() + labs( title = &quot;Market Health: Months of Inventory&quot;, subtitle = &quot;Lower means simpler to sell (Seller&#39;s Market)&quot;, color = &quot;City&quot; ) #&gt; Warning: Using `size` aesthetic for lines was deprecated in #&gt; ggplot2 3.4.0. #&gt; ℹ Please use `linewidth` instead. #&gt; This warning is displayed once every 8 hours. #&gt; Call `lifecycle::last_lifecycle_warnings()` to see #&gt; where this warning was generated. We see a convergence around 2014-2015 where inventory became very tight across all major cities. 14.7 Conclusions Through this case study, we accessed a standardized dataset (txhousing) and performed key Data Science tasks: 1. Cleaning: Handling NAs and formatting dates. 2. Aggregation: Summarizing millions of dollars of volume into a clear trend line. 3. Comparison: Benchmarking different categories (Cities) against each other. This workflow is identical to what is required for analyzing invoice portfolios, stock tickers, or customer churn cohorts, but uses a reliable data source that ensures our code is reproducible. "],["google-analytics-from-r.html", "Chapter 15 Google Analytics from R 15.1 Problem 15.2 Access to data 15.3 Visualization 15.4 Conclusion", " Chapter 15 Google Analytics from R Understanding the audience that enters our website helps us make better decisions, whether these are commercial or content release decisions. We can, thus, insert a visit counter or use Google Analytics to start collecting much more than the total visits. 15.1 Problem We have a website to which we already placed the Google Analytics code to understand visit statistics to my website, but I want reports that today the web does not provide us. We need to access the raw data to represent our own reports and access them even without having to enter the Google Analytics website. 15.2 Access to data We are going to assume for this case that we already have a google analytics account and we are already tracking data from our website through some view. For this case I am going to use the statistics to the website that you are currently reading. To access the Google Analytics data we will use the googleAnalyticsR library. In addition, to quickly manipulate dates from or to we will use the lubridate library. install.packages(&quot;googleAnalyticsR&quot;) library(googleAnalyticsR) library(lubridate) Then, we have to authenticate. To do this we will use the ga_auth() function, which will open a web page to log in with the account in which we have access to Google Analytics. ga_auth() Now that we are authenticated we can bring all our accounts using the ga_account_list() function. account_list &lt;- ga_account_list() From here we will search for the row of the website that interests us and from there we will obtain the viewID column. The viewID in Google Analytics for this website is the following: ga_id &lt;- 218744945 Finally, we need two variables of the date from when to when we want the data. from_date &lt;- &quot;2020-01-01&quot; to_date &lt;- &quot;2020-03-31&quot; Or if we wish we can only calculate the information of the last two months, or two days, etc. # Two months ago until now from_date &lt;- seq(now(), length = 2, by = &quot;-2 months&quot;)[2] |&gt; as_date() |&gt; as.character() to_date &lt;- now() |&gt; as_date() |&gt; as.character() from_date to_date Thus, we can already make a call to obtain the data we need using the google_analytics() function. history &lt;- google_analytics(ga_id, date_range = c(from_date, to_date), metrics = &quot;users&quot;, dimensions = &quot;date&quot;) With this data frame we could filter it or visualize it, depending on what we need. 15.3 Visualization Now with access to the data we can use the multiple metrics and dimensions available. For this case we are going to exemplify visualizing the city from where they visit this website in the last 90 days, which is related to the information in the paragraph of the main page, the preface, of this website (however, for that other calculation it is performed for another period of time). # Last 90 days to date from_date &lt;- seq(now(), length = 2, by = &quot;-90 days&quot;)[2] |&gt; as_date() |&gt; as.character() to_date &lt;- now() |&gt; as_date() |&gt; as.character() # We add the city as a dimension history &lt;- google_analytics(ga_id, date_range = c(from_date, to_date), metrics = &quot;users&quot;, dimensions = c(&quot;cityID&quot;, &quot;city&quot;)) As we see, the dimension also allows a vector as input. We will create a histogram with the top 5 cities that visited this website in the last 90 days. history |&gt; filter(city != &quot;(not set)&quot;) |&gt; group_by(city) |&gt; summarise(total = sum(users)) |&gt; mutate(proportion = total / sum(total)) |&gt; top_n(5, wt = proportion) |&gt; mutate(city = reorder(city, proportion, sum)) |&gt; ggplot() + aes(proportion, city) + geom_col() + labs( x = &quot;Proportion of visits&quot;, title = &quot;Proportion of visits by city&quot;, y = &quot;&quot; ) Keep in mind that in this case there is an issue of recognition of IPs coming from Lima, Peru, and that is why they do not appear as the first visitor. At the time of performing this analysis they all appeared as “(not set)”. However, if the same analysis is done by country and not by city, Peru is recognized and appears as one of the top visiting the web. 15.4 Conclusion We see that it is very easy to access Google Analytics data. We can create our own reports directly from R to analyze as many dimensions as we need without having to use the reporting provided by Google Analytics. "],["genai-intro.html", "Chapter 16 Data Science in the Age of AI 16.1 What is a Large Language Model? 16.2 Coding with AI: The “Pair Programmer” 16.3 Ethics &amp; Risks in the AI Era", " Chapter 16 Data Science in the Age of AI The field of Data Science is evolving rapidly. Just as the transition from base R to the tidyverse transformed how we write code, the emergence of Large Language Models (LLMs) and Generative AI is transforming how we solve problems. In this chapter, we will explore how to integrate these powerful tools into your R workflow—not to replace your skills, but to amplify them. 16.1 What is a Large Language Model? At its core, a Large Language Model (like GPT-4, Claude, or Llama) is a probabilistic engine. It has been trained on vast amounts of text to predict the “next most likely token” (piece of a word) in a sequence. While they can seem intelligent, it is crucial to remember: * They do not “know” facts: They generate plausible-sounding text based on patterns. * They can hallucinate: They can confidently state things that are completely false (especially about R packages that don’t exist!). * They are non-deterministic: Asking the same question twice might yield different answers. [!WARNING] Trust, but Verify. Never run code generated by an AI without understanding what it does. Always test it on a small sample of your data first. 16.2 Coding with AI: The “Pair Programmer” The most immediate application of GenAI for a Data Scientist is as a Pair Programmer. Tools like GitHub Copilot or simply chatting with ChatGPT/Claude can significantly speed up your coding. 16.2.1 1. Explaining Complex Code Have you ever inherited a project with a complex chunk of code you don’t understand? Paste it into an LLM and ask: “Explain this R code step-by-step.” Example Prompt: &gt; “I have this R code using purrr::map. Can you explain what it does in simple terms and suggest if there is a more modern way to write it?” 16.2.2 2. Generating Boilerplate Writing the skeleton for a Shiny app or a complex ggplot theme can be tedious. Example Prompt: &gt; “Create an R script that sets up a basic Shiny dashboard with a sidebar layout. It should generate a histogram of the palmerpenguins dataset.” 16.2.3 3. Regex: The Ultimate Use Case Regular Expressions (Regex) are powerful but famously difficult to remember. This is one of the best use cases for AI. Scenario: You have a dataset with messy phone numbers like (51) 999-999-999, 51 999 999 999, and +51999999999. You want to extract just the digits. Bad Way: Spending an hour reading StackOverflow. AI Way: Prompt: “I have a column in R with Peruvian phone numbers in inconsistent formats (e.g., (51) 987-654-321). Write a regular expression to extract only the 9 digits of the mobile number, ignoring the country code +51. Show me how to use it with stringr.” Potential Output: library(stringr) phones &lt;- c(&quot;(51) 987-654-321&quot;, &quot;+51 987654321&quot;, &quot;987 654 321&quot;) digits &lt;- str_extract(phones, &quot;(?&lt;=51\\\\D{0,2})\\\\d{9}|\\\\d{9}&quot;) (Note: Always test the regex provided! AI often struggles with lookbehinds) 16.3 Ethics &amp; Risks in the AI Era While these tools are powerful, they come with significant risks that every Data Scientist must manage. 16.3.1 1. Hallucinations &amp; Fabrication LLMs are designed to generate plausible text, not truth. * The “Package” Problem: An LLM might invent an R package like shiny.dashboard.plus.ultra because it sounds real. Always check CRAN. * False Confidence: It will explain a concept incorrectly with 100% confidence. 16.3.2 2. The Reproducibility Crisis Data Science relies on reproducibility. If you ask ChatGPT to write code for you: * Will it write the same code tomorrow? (No). * Can you cite “ChatGPT” as an author in a scientific paper? (Generally no). * Best Practice: Treat AI-generated code as a first draft. You must review, test, and own the final code. 16.3.3 3. Data Privacy &amp; IP When using free external tools (like ChatGPT), remember: If the service is free, you (and your data) might be the product. Never paste PII (Personally Identifiable Information) like client names, IDs, or private financial data into a public LLM. Corporate Policy: Many companies ban public LLMs. Check if you have an internal instance (e.g., Enterprise Copilot). Anonymize: If you must use a public tool, rename columns (Client_A, Revenue_X) and inject fake values before prompting. In the next section, we will go a step further: interacting with LLMs programmatically using R. "],["genai-api.html", "Chapter 17 LLMs as an Analysis Engine 17.1 The Evolution of NLP 17.2 Interacting with APIs from R 17.3 Zero-Shot Classification 17.4 Text Cleaning with LLMs 17.5 Summary", " Chapter 17 LLMs as an Analysis Engine In the previous section, we used AI as a coding assistant. Now, we will use it as a data processing engine. 17.1 The Evolution of NLP We saw in Chapter 5 ((ref?)(text-mining)) how to perform Sentiment Analysis using the syuzhet package and the NRC Lexicon. This approach is “Bag of Words”—it looks up each word in a dictionary. Lexicon Approach: “This movie was not bad.” -&gt; “Bad” is negative -&gt; Negative Sentiment (fails to catch negation). LLM Approach: “This movie was not bad.” -&gt; Understands nuance -&gt; Positive/Neutral Sentiment. 17.2 Interacting with APIs from R To use an LLM (like GPT-4o or Claude 3.5 Sonnet) from R, we typically use an API (Application Programming Interface). While there are packages like elmer or openai being developed, knowing how to do it with httr2 is a fundamental skill. 17.2.1 Prerequisite: API Keys You need an API key from a provider (OpenAI, Anthropic, etc.). Never hardcode your key in a script. Store it in your .Renviron file: # In your .Renviron file OPENAI_API_KEY=&quot;sk-...&quot; 17.2.2 Making a Request Here is a function to analyze sentiment using an LLM. library(httr2) library(jsonlite) analyze_sentiment_llm &lt;- function(text_input) { api_key &lt;- Sys.getenv(&quot;OPENAI_API_KEY&quot;) # 1. Create the request req &lt;- request(&quot;https://api.openai.com/v1/chat/completions&quot;) |&gt; req_headers(Authorization = paste(&quot;Bearer&quot;, api_key)) |&gt; req_body_json(list( model = &quot;gpt-4o-mini&quot;, messages = list( list(role = &quot;system&quot;, content = &quot;You are a sentiment analysis bot. Respond ONLY with one word: &#39;Positive&#39;, &#39;Negative&#39;, or &#39;Neutral&#39;.&quot;), list(role = &quot;user&quot;, content = text_input) ), temperature = 0 )) # 2. Perform the request response &lt;- req_perform(req) # 3. Parse the result result &lt;- response |&gt; resp_body_json() return(result$choices[[1]]$message$content) } # Testing it analyze_sentiment_llm(&quot;I absolutely hated the popcorn, but the movie was a masterpiece.&quot;) # LLM Result: &quot;Positive&quot; (likely, as it understands the core subject is the movie) 17.3 Zero-Shot Classification One of the most powerful abilities of LLMs is Zero-Shot Classification. You don’t need to train a model; you just describe the categories. Imagine you have customer feedback and want to categorize it into: “Pricing”, “Usability”, or “Feature Request”. Prompt: &gt; “Classify the following text into one of these categories: [Pricing, Usability, Feature Request]. Text: ‘I can’t find the export button.’” R Implementation: You would wrap the above prompt in a function similar to analyze_sentiment_llm. This replaces complex supervised learning pipelines for many simple tasks. 17.4 Text Cleaning with LLMs Another major use case is data cleaning. Scenario: You have a column Job_Title with values like “Sr. Data Scientist”, “Senior Data Scientist”, “Data Scientist II”, “Data Science Lead”. You want to standardize them. Instead of writing 50 regex rules, you can send the unique values to an LLM: “Map the following list of job titles to a standard taxonomy: [Data Scientist, Data Analyst, Data Engineer]. Return a CSV format.” This approach turns unstructured text into structured data with minimal code. 17.5 Summary LLMs capture context that lexicons miss. httr2 is your friend for connecting R to the AI ecosystem. Always handle API keys securely. AI is not free—monitor your usage costs. "],["ethics-checklist.html", "Chapter 18 Appendix A: Responsible AI Checklist 18.1 1. Data Quality &amp; lineage 18.2 2. Fairness &amp; Bias 18.3 3. Transparency &amp; Explainability 18.4 4. GenAI Specifics", " Chapter 18 Appendix A: Responsible AI Checklist As we conclude this book, use this checklist before deploying any model or analysis to production. 18.1 1. Data Quality &amp; lineage Provenance: Do I know exactly where this data came from? Consent: Was the data collected with consent? Does it contain PII? Representation: Does the training data match the real-world population it will be applied to? 18.2 2. Fairness &amp; Bias Protected Classes: Have I checked performance across different groups (Gender, Age, Ethnicity)? Proxy Variables: Are there variables (like Zip Code) acting as proxies for protected classes? Impact: Who could be harmed if this model makes a mistake? (e.g., Denying a loan vs. Recommending a bad movie). 18.3 3. Transparency &amp; Explainability Documentation: Is the model card created? (Inputs, Outputs, Limitations). Explainability: Can I explain to a non-technical stakeholder why the model made a specific prediction? Feedback Loop: Is there a mechanism for users to report errors or contest decisions? 18.4 4. GenAI Specifics Fact-Checking: Have I verified AI-generated code/facts against reliable sources? Security: Have I ensured no sensitive data is being sent to public APIs? Attribution: Am I transparent about which parts of the work were AI-generated? “With great power comes great responsibility.” — Stan Lee (and every Data Scientist) "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
