# (PART) Generative AI {-}

# Data Science in the Age of AI {#genai-intro}

The field of Data Science is in a state of constant evolution. We started by learning how to handle vectors and lists in Base R, we moved to the elegance of the **tidyverse** for data manipulation, and we explored the robustness of **tidymodels** for machine learning. Now, we are facing a new paradigm shift: **Generative AI**.

Just as the calculator did not replace the mathematician, Large Language Models (LLMs) will not replace the Data Scientist. However, a Data Scientist using AI will likely replace one who does not.

In this part of the book, we will demystify these "magic black boxes". We will learn what they are, how to control them programmatically from R, and how to use them to unlock unstructured data that was previously inaccessible.

## What is a Large Language Model?

To work effectively with LLMs, we must stop treating them as "people" and start treating them as **probabilistic engines**.

### It's all about Probability

At its most fundamental level, an LLM like GPT-4, Claude, or Llama is a "next token prediction machine". It has been trained on a massive corpus of text (books, websites, code repositories) to answer a simple statistical question:

> *Given the sequence of text "The capital of France is...", what is the most likely next piece of text?*

The model does not "know" geography. It knows that, statistically, the token "Paris" appears more frequently after that sequence than "London" or "Potato".

### Tokens vs. Words

We often think models read words, but they actually process **tokens**, which can be whole words, fragments, or even spaces. For instance, "apple" might be a single token, while a complex word like "antidisestablishmentarianism" could be split into four or five. A useful rule of thumb is that **1,000 tokens are roughly equivalent to 750 words**. This distinction is critical for two reasons: **Cost**, as you are billed by the token for both input and output; and **Context Window**, which serves as the model's short-term memory. A model with a 128k context window can effectively "remember" about 96,000 words of conversation before it begins to lose track of the beginning.

### Temperature: Controlling Creativity

One of the most important parameters you can control is **Temperature**, which dictates the randomness of the output. A temperature of **0** makes the model deterministic, always selecting the most probable next tokenâ€”ideal for tasks requiring precision like data extraction, coding, or math. Conversely, raising the temperature to **1** or higher encourages the model to take risks and choose less likely tokens, making it suitable for creative writing, brainstorming, and poetry.

> [!TIP]
> **For Data Science, start at 0.** When writing code or extracting data, we want reliability, not creativity.

## Setting Up Your AI Environment

Before we write code, we must secure our environment. Accessing high-quality models usually requires an API Key (from OpenAI, Anthropic, Google, etc.).

> [!DANGER]
> **NEVER** paste your API key directly into your R script. If you push that script to GitHub, bots will steal your key in seconds and drain your bank account.
*   **Anonymize:** If you must use a public tool, rename columns (`Client_A`, `Revenue_X`) and inject fake values before prompting.

### The Solution: Local LLMs

For sensitive data, the best solution is running a **Local LLM** on your own machine using tools like **Ollama** or **LM Studio**. This approach ensures 100% privacy and offline access, though it does come with trade-offs: it requires a capable computer (such as a Mac M-series or NVIDIA GPU), and local models are typically smaller and less capable than massive cloud models like GPT-4.

### The `.Renviron` File

The standard way to handle secrets in R is the `.Renviron` file. This file lives in your project's root or your home directory and is not tracked by Git (ensure it is in your `.gitignore`).

1.  Open or create the file using R:
    ```r
    usethis::edit_r_environ()
    ```
2.  Add your keys in the following format:
    ```bash
    OPENAI_API_KEY="sk-proj-12345..."
    ANTHROPIC_API_KEY="sk-ant-12345..."
    GITHUB_PAT="ghp_12345..."
    ```
3.  Restart your R session.
4.  Access them in R:
    ```r
    Sys.getenv("OPENAI_API_KEY")
    ```

## AI as the "Pair Programmer"

The most immediate value of AI is not replacing your analysis, but accelerating the code you write to perform it.

### The Great Refactorer

We all have old code: nested `for` loops, variable names like `x1`, `x2`, and manual indexing. AI excels at modernizing legacy code.

**Scenario:** You have this Base R code to filter and clean data:

```r
# Old Code
data <- read.csv("sales.csv")
clean_data <- data[data$amount > 100, ]
clean_data$date <- as.Date(clean_data$date)
final <- clean_data[order(clean_data$date), ]
```

**Prompt to AI:**
> "Refactor this R code to use the `tidyverse` and the pipe (`|>`) operator. Ensure variable names are snake_case."

**AI Output:**
```r
library(tidyverse)

sales_data <- read_csv("sales.csv") |> 
  filter(amount > 100) |> 
  mutate(date = as.Date(date)) |> 
  arrange(date)
```

### The Translator

One of the hardest parts of learning R is knowing *which* package does what you want. You can describe your intent in plain English (or Spanish!) and get the function.

**Example Prompt:**
> "I have this R code using `purrr::map`. Can you explain what it does in simple terms and suggest if there is a more modern way to write it?"

### Pro Tip: Prompt Engineering 101


Getting good code from an LLM isn't magic; it's engineering. A high-quality prompt typically combines four key components. First, establish a **Role** ("You are an expert R programmer...") to frame the model's perspective. Second, clearly define the **Task** ("Write a function to..."). Third, set explicit **Constraints** ("Use `dplyr`, not base R; do not assume clean data"). Finally, specify the desired **Format** ("Return the code in a single block with comments") to ensure the output matches your needs.

> [!TIP]
> **Iterate.** Your first prompt uses vague terms. Your second prompt clarifies them. Your third prompt gets the perfect answer.
> "I have a date column '2023-12-25'. I want to extract the week number of the year. Which `lubridate` function should I use?"

**AI Output:**
> "You should use `lubridate::isoweek()` or `lubridate::week()`."

### The Regex Master

Regular Expressions (Regex) are powerful but notoriously difficult to write. This is arguably the *best* use case for LLMs.

**Scenario:** You have a column with messy Peru phone numbers: `(51) 999-999-999`, `+51 999 999 999`.

**Prompt:**
> "I have inconsistent phone numbers. Write a regex compatible with `stringr` to extract only the 9 digits of the mobile number, ignoring country code."

**AI Output:**

```{r regex-demo, warning=FALSE, message=FALSE}
library(stringr)
phones <- c("(51) 987-654-321", "+51 987654321", "987 654 321")
# The Pattern: Simple extraction of 9 consecutive digits
clean_phones <- str_extract(phones, "\\d{9}") 
print(clean_phones)
```


### The Error Decoder

R error messages can be cryptic.
*   *"Error in result[[1]] : subscript out of bounds"*
*   *"Error: aesthetics must be either length 1 or the same as the data"*

Instead of staring at the screen, paste the error **and** the code chunk into the AI. It will usually pinpoint the exact mismatch in list lengths or ggplot layers.

## The Risks: Hallucinations

We cannot finish this introduction without a warning. LLMs are **people pleasers**. They want to give you an answer, even if they have to invent it.

### The "Package" Hallucination
It is common for an LLM to invent an R function that *should* exist but doesn't.

> **User:** "How do I calculate the Gini coefficient in `dplyr`?"
> **AI:** "Just use `summarize(gini = gini_coeff(income))`!"

There is no `gini_coeff` function in `dplyr` default exports. It sounded plausible, but running it will crash your script. **Always verify functions in the Help tab (`?function_name`).**

In the next chapter, we will stop chatting and start coding. We will build an engine to send data to the AI and get structured insights back.
